<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: pyspark&spark | EdmondFrank's 时光足迹]]></title>
  <link href="https://edmondfrank.github.io/blog/categories/pyspark-and-spark/atom.xml" rel="self"/>
  <link href="https://edmondfrank.github.io/"/>
  <updated>2018-01-18T19:57:16+08:00</updated>
  <id>https://edmondfrank.github.io/</id>
  <author>
    <name><![CDATA[EdmondFrank]]></name>
    <email><![CDATA[EdmomdFrank@yahoo.co.jp]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Python在Spark上的机器学习之机器学习实战(下)]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/11/27/pythonzai-sparkshang-de-ji-qi-xue-xi-zhi-ji-qi-xue-xi-shi-zhan-xia/"/>
    <updated>2017-11-27T12:45:01+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/11/27/pythonzai-sparkshang-de-ji-qi-xue-xi-zhi-ji-qi-xue-xi-shi-zhan-xia</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="python在spark上的机器学习之机器学习实战下">Python在Spark上的机器学习之机器学习实战(下)</h1></p>

<h2 id="mllib-的使用续">MLlib 的使用（续）</h2>




<p>我们在上篇讲到了：数据相关性分析和特征选取，但是我们在上篇中所提及的方法基本都是针对标准的数值型的数据特征；那么，我们下篇就继续将分类变量的统计检验分析，以及最后的建模过程讲述完整。</p>




<h3 id="统计校验">　统计校验</h3>




<p>在通过特征变量的相关系数选择特征时，对于一般的分类变量而言，我们无法计算它们之间的相关系数，但是我们可以通过对它们进行卡方校验来检测它们的分布之间是否存在较大的差异。</p>




<p><strong>卡方检验</strong>：是用途非常广的一种假设检验方法，它在分类资料统计推断中的应用，包括：两个样本率或两个构成比比较的卡方检验；多个样本率或多个构成比比较的卡方检验以及分类资料的相关分析等。</p>




<p><strong>卡方检验</strong>就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为0，表明理论值完全符合。</p>




<p>而在PySpark中你可以用 <strong>.chiSqTest()</strong> 方法来轻松实现卡方检验。</p>


<pre><code class="python">import pyspark.mllib.linalg as ln
for cat in categorical_cols[1:]:
    agg = births_transformed \
        .groupby('INFANT_ALIVE_AT_REPORT') \
        .pivot(cat) \
        .count()
    agg_rdd = agg \
        .rdd \
        .map(lambda row: (row[1:])) \
        .flatMap(lambda row:
                [0 if e == None else e for e in row]) \
        .collect()
    row_length = len(agg.collect()[0]) - 1
    agg = ln.Matrices.dense(row_length, 2, agg_rdd)
    test = st.Statistics.chiSqTest(agg)
    print(cat, round(test.pValue, 4))
</code></pre>

<p>我们遍历所有的分类变量并以 <strong>infant_alive_ at_report</strong>进行分类统计。下一步，我们需要将其转化成RDD，所以我们要先利用pyspark.mllib.linalg模将它们转换成一个矩阵。 <br>
当我们成功将其转换成矩阵的形式之后，我们就可以用<strong>.chiSqTest()</strong>来校验我们的结果。</p>




<p>最后结果显示如下：</p>




<p><img src="https://i.loli.net/2017/11/16/5a0daece4fae9.png" alt="chisqtest.png" title=""></p>




<p>从结果我们可以看出，所有分类变量对理论值的预测都是有意义的，因此，我们在构建最后的预测模型的时候都要考虑上这些分类型特征变量。</p>




<h3 id="创建最后的待训练数据集">创建最后的待训练数据集</h3>




<p>经过一轮的数据分析和特征变量筛选之后，最终到了我们最终的建模阶段了。首先我们将筛选出来以DataFrame数据结构模型表达的数据转换成以LabeledPoints形式表示的RDD。</p>




<p>LabeledPoint 是 MLlib 中的一种数据结构，它包含了两个属性值：label（标识），features（特征）一般用作机器学习模型的训练。</p>




<p>其中，label就是我们目标的分类的标识而features就是我们用于分类的特征， <br>
通常是一个Numpy 数组，列表，psyspark.mllib.linalg.SparseVector,pyspark.mllib,linalg.DenseVector或者是scipy.sparse的形式。</p>


<pre><code class="python">import pyspark.mllib.feature as ft
import pyspark.mllib.regression as reg
hashing = ft.HashingTF(7)
births_hashed = births_transformed \
    .rdd \
    .map(lambda row: [
        list(hashing.transform(row[1]).toArray())
            if col == 'BIRTH_PLACE'
            else row[i]
        for i, col
        in enumerate(features_to_keep)]) \
    .map(lambda row: [[e] if type(e) == int else e
            for e in row]) \
    .map(lambda row: [item for sublist in row
            for item in sublist]) \
    .map(lambda row: reg.LabeledPoint(
        row[0],
        ln.Vectors.dense(row[1:]))
        )
</code></pre>

<h3 id="划分训练集和测试集">划分训练集和测试集</h3>




<p>形如sklearn.model_selection.train_test_split随机划分训练集和测试集的模块一般，在PySpark中RDDs也有一个便利的<strong>.randomSplit(…)</strong>方法用于随机划分训练集和测试集。</p>




<p>在本例中可以这样使用</p>




<pre class="prettyprint"><code class="language-python hljs ">births_train, births_test = births_hashed.randomSplit([<span class="hljs-number">0.6</span>, <span class="hljs-number">0.4</span>])</code></pre>




<p>没错，仅仅需要上面这样一行的代码，我们就可以将我们的待训练数据按照随机60%，40%来划分好我们的训练集和测试集了。</p>




<h3 id="开始建模">开始建模</h3>




<p>在一切准备就绪之后，我们就可以开始通过我们上面的训练数据集来建模了。在这里我们来尝试建立两个模型：一个线性的Logistic回归模型，一个非线性的随机森林模型。然后，在初次建模的时候，我们先采用筛选出来的全部特征来建模，然后我们再通过<strong>ChiSqSelector（…）</strong>方法来归纳出最能代表全部整体的四个主成分。</p>




<h4 id="logistic-回归模型">Logistic 回归模型</h4>


<pre><code class="python">from pyspark.mllib.classification \
import LogisticRegressionWithLBFGS
LR_Model = LogisticRegressionWithLBFGS \
.train(births_train, iterations=10)

LR_results = (
births_test.map(lambda row: row.label) \
.zip(LR_Model \
.predict(births_test\
.map(lambda row: row.features)))
).map(lambda row: (row[0], row[1] * 1.0))

import pyspark.mllib.evaluation as ev
LR_evaluation = ev.BinaryClassificationMetrics(LR_results)
print('Area under PR: {0:.2f}' \
.format(LR_evaluation.areaUnderPR))
print('Area under ROC: {0:.2f}' \
.format(LR_evaluation.areaUnderROC))
LR_evaluation.unpersist()
</code></pre>

<p>从上面的建模过程可以看出，使用PySpark训练一个模型也是非常简单的。我们只需要调用<strong>.train(…)</strong>方法，并传入之前处理好的LabeledPoints数据即可。不过需要注意的一点是我们要提前指定一个较小训练的迭代次数以免训练时间过长。</p>




<p>同时，在上面的代码中，我们在训练完一个模型之后使用MLlib中为我们提供的评估分类和回归准确度的<strong>.BinaryClassificationMetrics（…）</strong>方法来分析我们最后预测的结果。</p>




<p>最后，结果图示如下：</p>




<p><img src="https://i.loli.net/2017/11/17/5a0dc13a5ea1a.png" alt="logistic_roc.png" title=""></p>




<p>通过PR，ROC的结果，我们可以看出，这个模型还是可接受的。</p>




<h3 id="选取出最具代表性的分类特征">选取出最具代表性的分类特征</h3>




<p>通常来说，一个采取更少的特征的简单模型，往往会比一个复杂的模型，在分类问题上更具有代表性和可解释性。而在MLlib中，则可以通过<strong>.Chi-Square selector</strong>来提取出模型中最具代表性的一些分类特征变量来简化我们的模型。</p>


<pre><code class="python">selector = ft.ChiSqSelector(4).fit(births_train)
topFeatures_train = (
    births_train.map(lambda row: row.label) \
    .zip(selector \
        .transform(births_train \
            .map(lambda row: row.features)))
    ).map(lambda row: reg.LabeledPoint(row[0], row[1]))
topFeatures_test = (
    births_test.map(lambda row: row.label) \
    .zip(selector \
        .transform(births_test \
            .map(lambda row: row.features)))
).map(lambda row: reg.LabeledPoint(row[0], row[1]))
</code></pre>

<h3 id="随机森林模型">随机森林模型</h3>


<pre><code class="python">from pyspark.mllib.tree import RandomForest
RF_model = RandomForest \
.trainClassifier(data=topFeatures_train,
numClasses=2,
categoricalFeaturesInfo={},
numTrees=6,
featureSubsetStrategy='all',
seed=666)

RF_results = (
topFeatures_test.map(lambda row: row.label) \
.zip(RF_model \
.predict(topFeatures_test \
.map(lambda row: row.features)))
)
RF_evaluation = ev.BinaryClassificationMetrics(RF_results)
print('Area under PR: {0:.2f}' \
.format(RF_evaluation.areaUnderPR))

print('Area under ROC: {0:.2f}' \
.format(RF_evaluation.areaUnderROC))
model_evaluation.unpersist()
</code></pre>

<p>随机森林模型（Random forest 后面简称RF）在训练上总体与Logistic类似，不同的参数是RF在训练前需要指定类别总数：numClasses，树的棵数：numTrees（这两个参数的意义大家可以参照下随机森林模型的<a href="https://baike.baidu.com/item/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/1974765?fr=aladdin">百科介绍</a>）</p>




<p><strong>注：在随机森林模型的创建中，我们采用的是上面提取出来的最具代表性的有效特征，这就意味着模型用到的特征是比之前的Logistic要少的。</strong></p>




<p>最后，结果图示如下：</p>




<p><img src="https://i.loli.net/2017/11/17/5a0dc8621d985.png" alt="rf_roc.png" title=""></p>




<p>通过结果我们可以看出，随机森林模型，在采用比之前更少的特征下的建模的最终预测效果是由于之前的Logistic回归模型的。</p>




<p>下面我们同样使用代表性特征来重建一次Logistic回归模型</p>


<pre><code class="python">LR_Model_2 = LogisticRegressionWithLBFGS \
.train(topFeatures_train, iterations=10)
LR_results_2 = (
topFeatures_test.map(lambda row: row.label) \
.zip(LR_Model_2 \
.predict(topFeatures_test \
.map(lambda row: row.features)))
).map(lambda row: (row[0], row[1] * 1.0))
LR_evaluation_2 = ev.BinaryClassificationMetrics(LR_results_2)
print('Area under PR: {0:.2f}' \
.format(LR_evaluation_2.areaUnderPR))
print('Area under ROC: {0:.2f}' \
.format(LR_evaluation_2.areaUnderROC))
LR_evaluation_2.unpersist()
</code></pre>

<p>最终结果： <br>
<img src="https://i.loli.net/2017/11/17/5a0dc9d2aa75c.png" alt="logistic_lbfgs.png" title=""></p>




<p>通过结果，我们可以看出，虽然没有达到RF模型的准确度，但是与采用了全特征的Logistic回归模型处于同一水平。所以，我们在可选的情况下，通常采用更少的特征来构建更为简化和有效的模型。</p>




<h2 id="小结">小结</h2>




<p>到这里，Python在Spark上的机器学习的实战案例也结束了，欢迎大家继续关注我的博客。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python在Spark上的机器学习之机器学习实战(上)]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/11/17/pythonzai-sparkshang-de-ji-qi-xue-xi-zhi-ji-qi-xue-xi-shi-zhan-shang/"/>
    <updated>2017-11-17T09:16:10+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/11/17/pythonzai-sparkshang-de-ji-qi-xue-xi-zhi-ji-qi-xue-xi-shi-zhan-shang</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="python在spark上的机器学习之机器学习实战上">Python在Spark上的机器学习之机器学习实战(上)</h1></p>

<h2 id="mllib-的使用">MLlib 的使用</h2>




<p>在上面的章节之中，我们已经讲过了如何利用PySpark进行数据操作和分析了。那么在这篇文章中，我们就真正利用PySpark结合MLlib来建立一个分类模型。</p>




<p><strong>MLlib</strong>：即Machine Learning Library，MLlib 是Spark对常用的机器学习算法的实现库，同时包括相关的测试和数据生成器。MLlib 目前支持四种常见的机器学习问题：二元分类，回归，聚类以及协同过滤，同时也包括一个底层的梯度下降优化基础算法。</p>




<h3 id="载入和转化数据">载入和转化数据</h3>




<p>首先，我们在建立一个DataFrame之前，我们先针对性的指定下DataFrame中数据类型，方便我们数据后期的分析与计算。</p>


<pre><code class="python">import pyspark.sql.types as typ
labels = [
('INFANT_ALIVE_AT_REPORT', typ.StringType()),
('BIRTH_YEAR', typ.IntegerType()),
('BIRTH_MONTH', typ.IntegerType()),
('BIRTH_PLACE', typ.StringType()),
('MOTHER_AGE_YEARS', typ.IntegerType()),
('MOTHER_RACE_6CODE', typ.StringType()),
('MOTHER_EDUCATION', typ.StringType()),
('FATHER_COMBINED_AGE', typ.IntegerType()),
('FATHER_EDUCATION', typ.StringType()),
('MONTH_PRECARE_RECODE', typ.StringType()),
...
('INFANT_BREASTFED', typ.StringType())
]
schema = typ.StructType([
typ.StructField(e[0], e[1], False) for e in labels
])
</code></pre>

<p>下一步，我们通过 <strong>.read.csv()</strong> 方法来载入数据，这个方法除了能够载入原数据之外还可以载入GZipped压缩后的csv数据。其实header参数设为 True 代表数据文件的第一行是数据的元信息（即为列表的说明字段）。</p>




<pre class="prettyprint"><code class="language-python hljs ">births = spark.read.csv(<span class="hljs-string">'births_train.csv.gz'</span>,
header=<span class="hljs-keyword">True</span>,
schema=schema)</code></pre>




<p>由于在我们的数据集中有大量的分类变量都是字符串，所以我们首先要想办法将这一类变量转换成数字的形式。</p>




<pre class="prettyprint"><code class=" hljs ruleslanguage"><span class="hljs-array"># </span>转换<span class="hljs-string">'INFANT_ALIVE_AT_REPORT'</span>
recode_dictionary = {
    <span class="hljs-string">'YNU'</span>: {
        <span class="hljs-string">'Y'</span>: <span class="hljs-number">1</span>,
        <span class="hljs-string">'N'</span>: <span class="hljs-number">0</span>,
        <span class="hljs-string">'U'</span>: <span class="hljs-number">0</span>
            }
}</code></pre>




<p>在这里总的来说，我们的目的就是一个二分类问题，即预测婴儿的存活情况，也就是“存活 1 ”或“死亡 0 ”。因为，要做到一种未雨绸缪的效果，我们要先去除所有与婴儿有关的特征信息，仅仅是通过婴儿父母的基本信息以及婴儿的出生地来预测一下婴儿出生后存活的概率。</p>


<pre><code class="python">selected_features = [
'INFANT_ALIVE_AT_REPORT',
'BIRTH_PLACE',
'MOTHER_AGE_YEARS',
'FATHER_COMBINED_AGE',
'CIG_BEFORE',
'CIG_1_TRI',
'CIG_2_TRI',
'CIG_3_TRI',
'MOTHER_HEIGHT_IN',
'MOTHER_PRE_WEIGHT',
'MOTHER_DELIVERY_WEIGHT',
'MOTHER_WEIGHT_GAIN',
'DIABETES_PRE',
'DIABETES_GEST',
'HYP_TENS_PRE',
'HYP_TENS_GEST',
'PREV_BIRTH_PRETERM'
]
births_trimmed = births.select(selected_features)
</code></pre>

<p>在这个数据集中，大量的变量特征值都是Yes/No/Unknown值，我们将Yes编码成1，另外的其他值编码成0。</p>




<p>而在代表怀孕妈妈的吸烟数量的这个特征值的编码上，我们采用这样的规则。0：代表妈妈在怀孕期间没有抽过烟；而1-97：代表妈妈在怀孕期间真实的抽烟次数，而98：则代表孕期抽烟次数高达98次及以上；但99：意味着妈妈的孕期抽烟情况未知。</p>


<pre><code class="python">import pyspark.sql.functions as func
def recode(col, key):
    return recode_dictionary[key][col]
def correct_cig(feat):
    return func \
        .when(func.col(feat) != 99,
        func.col(feat))\
        .otherwise(0)
    rec_integer = func.udf(recode,typ.IntegerType())
</code></pre>

<p>由于Spark的机制问题，我们无法直接将DataFrame来用recode函数进行处理，所以我们首先要先它转换成Spark能够理解的UDF。</p>


<pre><code class="python">births_transformed = births_trimmed \
.withColumn('CIG_BEFORE', correct_cig('CIG_BEFORE'))\
.withColumn('CIG_1_TRI', correct_cig('CIG_1_TRI'))\
.withColumn('CIG_2_TRI', correct_cig('CIG_2_TRI'))\
.withColumn('CIG_3_TRI', correct_cig('CIG_3_TRI'))
cols = [(col.name, col.dataType) for col in births_trimmed.schema]
YNU_cols = []

for i, s in enumerate(cols):
if s[1] == typ.StringType():
dis = births.select(s[0]) \
.distinct() \
.rdd \
.map(lambda row: row[0]) \
    .collect()
if 'Y' in dis:
    YNU_cols.append(s[0])
</code></pre>

<p>最后，为了一次性转换所有的 YNU_cols 数据，我们用以下的方法：</p>




<pre class="prettyprint"><code class="language-python hljs ">exprs_YNU = [
    rec_integer(x,
    func.lit(<span class="hljs-string">'YNU'</span>)).alias(x)
    <span class="hljs-keyword">if</span> x <span class="hljs-keyword">in</span> YNU_cols
    <span class="hljs-keyword">else</span> x
    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> births_transformed.columns
]
births_transformed = births_transformed.select(exprs_YNU)</code></pre>




<p>让我们来检查一下转换的结果吧：</p>




<blockquote>
  <p>births_transformed.select(YNU_cols[-5:]).show(5)</p>
</blockquote>




<p><img src="https://ooo.0o0.ooo/2017/11/01/59f9cda7add15.png" alt="translate_res.png" title=""></p>




<h3 id="数据预分析">数据预分析</h3>




<p>为了建立一个良好的统计模型，我们首先需要了解清楚数据的组成分布以及背后的含义。</p>




<p>下面我们可以通过Spark提供的一些函数来对数据进行描述性分析。</p>


<pre><code class="python">import pyspark.mllib.stat as st
import numpy as np
numeric_cols = ['MOTHER_AGE_YEARS','FATHER_COMBINED_AGE',
'CIG_BEFORE','CIG_1_TRI','CIG_2_TRI','CIG_3_TRI',
'MOTHER_HEIGHT_IN','MOTHER_PRE_WEIGHT',
'MOTHER_DELIVERY_WEIGHT','MOTHER_WEIGHT_GAIN'
]
numeric_rdd = births_transformed\
    .select(numeric_cols)\
    .rdd \
    .map(lambda row: [e for e in row])
mllib_stats = st.Statistics.colStats(numeric_rdd)
for col, m, v in zip(numeric_cols,
    mllib_stats.mean(),
    mllib_stats.variance()):
    print('{0}: \t{1:.2f} \t {2:.2f}'.format(col, m, np.sqrt(v)))
</code></pre>

<p><img src="https://ooo.0o0.ooo/2017/11/01/59f9cf95819ec.png" alt="statics.png" title=""></p>




<p>根据输出的统计结果我们可以看出：在婴儿父母的年龄对比上，妈妈是明显比爸爸年轻的。妈妈的平均年龄在28岁左右，而爸爸的平均年龄确是44岁。</p>




<p>对于大部分的分类变量，我们也可以一一的来统计下他们的各个数值出现的频数：</p>


<pre><code class="python">categorical_cols = [e for e in births_transformed.columns
if e not in numeric_cols]
categorical_rdd = births_transformed\
    .select(categorical_cols)\
    .rdd \
    .map(lambda row: [e for e in row])
for i, col in enumerate(categorical_cols):
    agg = categorical_rdd \
        .groupBy(lambda row: row[i]) \
        .map(lambda row: (row[0], len(row[1])))
    print(col, sorted(agg.collect(),
key=lambda el: el[1],
reverse=True))
</code></pre>

<p><img src="https://ooo.0o0.ooo/2017/11/01/59f9d1109a62e.png" alt="frequence.png" title=""></p>




<p>根据这次的结果，我们又可以看出大部分的婴儿都是在医院出现的（医院的出生地代号BIRTH_PLACE=1）</p>




<h3 id="相关系数">相关系数</h3>




<p>相关性的分析有利于我们发现特征变量中的多重共线性的情况，而多重共线性则是影响我们模型的鲁棒性的关键因素之一。</p>


<pre><code class="python">corrs = st.Statistics.corr(numeric_rdd)
for i, el in enumerate(corrs &gt; 0.5):
    correlated = [
        (numeric_cols[j], corrs[i][j])
        for j, e in enumerate(el)
        if e == 1.0 and j != i]
    if len(correlated) &gt; 0:
        for e in correlated:
            print('{0}-to-{1}: {2:.2f}' \
            .format(numeric_cols[i], e[0], e[1]))
</code></pre>

<p>上面的代码会替我们计算特征变量之间的相关系数矩阵，并输出相关系数高于0.5的特征。</p>




<p><img src="https://ooo.0o0.ooo/2017/11/01/59f9d3831bd0f.png" alt="cor_matrix.png" title=""></p>




<p>根据上图输出的结果，我们又可以看出 CIG_XXX 系列的特征都有些非常高的相关性，所以在这个系列的特征之中保留一个即可。 在这里我只保留 <strong>CIG_1_TRI</strong>这个特征。同理在WEIGHT系列中我只保留<strong>MOTHER_PRE_WEIGHT</strong>这个特征。</p>


<pre><code class="python">features_to_keep = [
'INFANT_ALIVE_AT_REPORT',
'BIRTH_PLACE',
'MOTHER_AGE_YEARS',
'FATHER_COMBINED_AGE',
'CIG_1_TRI',
'MOTHER_HEIGHT_IN',
'MOTHER_PRE_WEIGHT',
'DIABETES_PRE',
'DIABETES_GEST',
'HYP_TENS_PRE',
'HYP_TENS_GEST',
'PREV_BIRTH_PRETERM'
]
births_transformed = births_transformed.select([e for e in features_
to_keep])
</code></pre>

<h3 id="小结">小结</h3>




<p>在这一篇实战的文章中，我讲解了：</p>




<ul>
<li>数据的载入和转换</li>
<li>数据的描述性分析</li>
<li>数据相关性分析</li>
</ul>




<p>限于时间和篇幅，我打算将有关分类变量的统计检验分析，以及最后的特征选取和建模放在下一篇文章之中，欢迎大家继续阅读我的下一篇文件。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深入理解Spark 编程模型]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/11/10/shen-ru-li-jie-spark-bian-cheng-mo-xing/"/>
    <updated>2017-11-10T13:14:53+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/11/10/shen-ru-li-jie-spark-bian-cheng-mo-xing</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="深入理解spark-编程模型">深入理解Spark 编程模型</h1></p>

<h2 id="spark的编程模型">Spark的编程模型</h2>




<p>Spark的应用程序主要由两部分组成：</p>




<ul>
<li>Driver</li>
<li>Executor</li>
</ul>




<p>除此之外，在Spark的编程模型的构成还包括许多其他的成分，如：<strong>SparkContext</strong>，这是Spark的应用程序的入口，负责调度各个运算资源，协调各个Worker节点上的Executor；</p>




<p>而<strong>Dirver program</strong>则负责运行Spark应用的main()函数并创建SparkContext，通常情况下，我们用SparkContext来指代Driver program。</p>




<p><strong>Executor</strong>：这是Spark应用中运行在Work Node上的一个进程，该进程负责运行Task，并且负责将数据存在内存和磁盘上，每个应用都会申请自己的Executors来负责调度和处理。</p>




<p>其次，在Spark编程模型中还有以下重要的概念，需要了解：</p>




<ul>
<li><strong>Application</strong>：Spark的应用程序，包含一个Driver program 和 若干个Executor</li>
<li><strong>Cluster Manager</strong>：在集群上获取资源的外部服务</li>
<li><strong>Work Node</strong>：集群中任何可以运行Application代码的节点，其中运行着一个或多个Executor进程。</li>
<li><strong>Job</strong>：可以被拆分成Task的并行计算的工作单元，一般由Spark Action触发的一次执行作业</li>
<li><strong>Stage</strong>：每个Job会被拆分成很多个Task，而每组任务就被称作Stage（相当于一个TaskSet）</li>
<li><strong>Task</strong>：运行在Executor上的工作单元</li>
<li><strong>RDD</strong>：弹性分布式数据集的简称，是Spark的最核心的模块和类之一</li>
</ul>




<h2 id="hadoop数据集">Hadoop数据集</h2>




<p>Spark可以将任何Hadoop所支持的存储资源转化成RDD，例如：本地文件，HDFS，Cassandra，HBase等。同时，Spark不仅支持文本文件和SequenceFiles还兼容任何Hadoop InputFormat的格式。</p>




<h3 id="textfile方法">textFile()方法</h3>




<p>使用textFile()可以将本地文件或HDFS文件转化成RDD</p>




<p><strong>读取整个文件目录</strong></p>




<blockquote>
  <p>textFile(“file:///hfds/directory”)</p>
</blockquote>




<p><strong>读取文本或压缩文件（可以自动执行解压缩并加载文件数据）</strong></p>




<blockquote>
  <p>textFile(“file:///hfds/directory/data.gz”)</p>
</blockquote>




<p><strong>使用通配符进行读取</strong></p>




<blockquote>
  <p>textFile(“file:///hfds/data/*.csv”)</p>
</blockquote>




<p>对于其他格式数据的读取有以下的方法：</p>




<ul>
<li><strong>wholeTextFiles()</strong>:读取目录里的小文件，返回由（用户名，内容）结构构成的键值对</li>
<li><strong>sequenceFile<a href="">K,V</a></strong>:可以将SequenceFile转换成RDD</li>
<li><strong>SparkContext.hadoopRDD</strong>:可以将其他任何Hadoop输入类型转换成RDD使用</li>
</ul>




<h2 id="rdd">RDD</h2>




<p><strong>RDD</strong>，弹性分布数据集，是Spark最核心的东西，他表示已被分区，不可变的并能够被并行操作的数据集合，不同的数据集格式对应着不同的RDD的实现。RDD的前提是其必须是可序列化的，同时RDD可以cache到内存之中</p>




<h3 id="特点">特点</h3>




<ol>
<li>只能通过转换操作（如map/filter/groupBy/join等）来从规定数据源（稳定存储的数据或其他RDD）中创建RDD</li>
<li>状态不可变，即不能修改</li>
<li>容错性强，由于RDD中的元素会根据key来分区，并保存在多个节点上，还原时只会重新计算丢失的分区的数据，不会影响整个系统的使用</li>
<li>RDD中会保存他的继承信息，即关于它是如何从其他RDD中生成的信息</li>
<li>被重用的RDD会缓存在内存中，或溢出至磁盘作持久化存储</li>
<li>Spark会延迟计算RDD，这样RDD就能够转换管道化（pipeline）</li>
<li>有丰富的动作（action）如：count/reduce/collect/save等支持</li>
<li>惰性求值，即执行了多少次transformation操作，RDD都不会真正执行运算，而只有action操作执行时，运算才会触发</li>
</ol>




<h2 id="rdd的元数据">RDD的元数据</h2>




<p>每个RDD都包含了5部分的信息，他们包括数据分区的集合，能根据本地性快速访问数据的偏好位置（最佳位置），依赖关系，计算方法（函数），分区策略。</p>




<p>示例：</p>




<p><img src="https://i.loli.net/2017/11/10/5a04866bbb6c5.png" alt="rdd_meta.png" title=""></p>




<h2 id="rdd的操作">RDD的操作</h2>




<p>RDD中的操作主要分为两大类：</p>




<ul>
<li><strong>转换(transformation):</strong>现有的RDD通过转换来生成一个新的RDD，转换是延迟执行（惰性求值）的。</li>
<li><strong>动作(actions):</strong>在RDD上执行动作后，就会运行计算，然后返回结果给驱动程序或者写入文件系统，从而触发Job。</li>
</ul>




<p>常用transformation：</p>




<p><img src="https://i.loli.net/2017/11/10/5a0487f0527d5.png" alt="rdd_transformation.png" title=""></p>




<p>常用actions：</p>




<p><img src="https://i.loli.net/2017/11/10/5a0487f047dab.png" alt="rdd_actions.png" title=""></p>




<h3 id="持久化">持久化</h3>




<p>缓存的操作 <br>
使用<strong>persist</strong>和<strong>cache</strong>方法可以将任意RDD缓存在内存或磁盘文件中，缓存不仅可以加速RDD的读取速度同时兼备了容错性，可以通过构建他的transformation自动重构。</p>




<p><strong>缓存</strong>是Spark最重要的一个功能，就是在不同操作间，持久化（或缓存）一个数据集在内存中。当你持久化一个RDD，每一个结点都将把它的计算分块结果保存在内存中，并在对此数据集（或者衍生出的数据集）进行的其它动作中重用。这将使得后续的动作(actions)变得更加迅速（通常快10倍）。所以缓存是用Spark构建迭代算法的关键。</p>




<p>如果你需要删除被持久化的RDD，可以用unpersistRDD()来完成该工作。</p>




<p>此外，每一个RDD都可以用不同的保存级别进行保存，从而允许你持久化数据集在硬盘，或者在内存作为序列化的Java对象（节省空间），甚至于跨结点复制。这些等级选择，是通过将一个<strong>org.apache.spark.storage.StorageLevel</strong>对象传递给persist()方法进行确定。</p>




<p>cache()方法是使用默认存储级别的快捷方法，也就是StorageLevel.MEMORY_ONLY(将反序列化的对象存入内存）。</p>




<p>StorageLevel有五个属性，分别是：</p>




<ul>
<li>useDisk_是否使用磁盘</li>
<li>useMemory_是否使用内存</li>
<li>useOffHeap_是否使用堆外内存如：Tachyon</li>
<li>deserialized_是否进行反序列化</li>
<li>replication_备份数目。</li>
</ul>




<p><strong>存储级别的选择</strong> <br>
Spark的不同存储级别，旨在满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择：</p>




<ul>
<li>如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快。</li>
<li>如果不行，试着使用MEMORY_ONLY_SER并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。</li>
<li>尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤 <br>
了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。</li>
</ul>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python在Spark上的机器学习(四)之可视化工具的介绍与PySpark的结合使用示例]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/11/04/pythonzai-sparkshang-de-ji-qi-xue-xi-si-zhi-ke-shi-hua-gong-ju-de-jie-shao-yu-pysparkde-jie-he-shi-yong-shi-li/"/>
    <updated>2017-11-04T23:20:17+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/11/04/pythonzai-sparkshang-de-ji-qi-xue-xi-si-zhi-ke-shi-hua-gong-ju-de-jie-shao-yu-pysparkde-jie-he-shi-yong-shi-li</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="python在spark上的机器学习四之可视化工具的介绍与pyspark的结合使用示例">Python在Spark上的机器学习(四)之可视化工具的介绍与PySpark的结合使用示例</h1></p>

<h2 id="前言">前言</h2>




<p>在Python和Java的生态圈中，有许多可用的可视化库，但是在这篇文章中，我们主要来介绍一下matplotlib 和 Bokeh的使用。</p>




<p>首先，这两个库都是<a href="https://www.anaconda.com/">Anaconda</a>预装的。如果你是通过Anaconda来搭建的Python的科学计算环境的话，直接就可以通过import导入来使用这两个库了。</p>




<p>但是如果还没安装和配置好环境的朋友，可以自行参考<a href="http://matplotlib.org/index.html">Matplotlib</a>和<a href="https://bokeh.pydata.org/en/latest/">Bokeh</a>的官方站点的教程来下载配置环境。</p>




<blockquote>
  <p>注：这一类对各系统平台支持良好的库，一般安装流程也就无非两条pip命令，如： <br>
  python -mpip install -U pip <br>
  python -mpip install -U matplotlib <br>
  pip install bokeh <br>
  或 <br>
  conda install bokeh <br>
  所以各位读者也没有必要担心配置麻烦。</p>
</blockquote>




<h2 id="有关matplotlib和bokeh的介绍">有关matplotlib和bokeh的介绍</h2>




<h3 id="matplotlib">Matplotlib</h3>




<p><strong>Matplotlib</strong>是一个Python 2D绘图库，可以跨平台生成各种通用格式和适用于交互式环境的高质量图表。 Matplotlib可直接用于Python脚本，IPython shell，Jupyter以及Web应用程序服务器之中。 <br>
<strong>Matplotlib</strong>简化了许多繁琐的绘图操作，使得原本简单的图表在绘制上更加简单，而复杂的图表绘制也更容易上手。只需几行代码即可生成许多好看的图表。如，直方图、功率谱、条形图、错误图，散点图等。</p>




<p>官方绘图预览：</p>




<p><img src="http://matplotlib.org/_images/sphx_glr_simple_plot_0011.png" alt="enter image description here" title=""> <br>
<img src="http://matplotlib.org/_images/sphx_glr_histogram_features_0011.png" alt="enter image description here" title=""></p>




<p><img src="http://matplotlib.org/_images/sphx_glr_barchart_demo_0011.png" alt="enter image description here" title=""> <br>
<img src="http://matplotlib.org/_images/sphx_glr_pie_features_0011.png" alt="enter image description here" title=""></p>




<h3 id="bokeh">Bokeh</h3>




<p><strong>Bokeh</strong> (Bokeh.js) 是一个 Python 交互式可视化库，支持现代化 Web 浏览器，提供非常完美的展示功能。Bokeh 的目标是使用 D3.js 样式提供优雅，简洁新颖的图形化风格，同时提供大型数据集的高性能交互功能。Boken 可以快速的创建交互式的绘图，仪表盘和数据应用。</p>




<p>鉴于Bokeh强调的更多是一种交互式的绘图体验，在这里我就不貼静态图了，不过下面我会附上一些官方demo的例子，让大家感受下Bokeh的强大之处。</p>




<p><a href="https://bokeh.pydata.org/en/latest/docs/gallery/stocks.html">趋势走向图</a></p>




<p><a href="https://bokeh.pydata.org/en/latest/docs/gallery/iris.html">散点图</a></p>




<p><a href="https://bokeh.pydata.org/en/latest/docs/gallery/texas.html">地域分布图</a></p>




<p><a href="https://bokeh.pydata.org/en/latest/docs/gallery/boxplot.html">箱型图</a></p>




<h2 id="结合pyspark进行可视化分析">结合PySpark进行可视化分析</h2>




<h3 id="模块加载">模块加载</h3>




<p>以下实验均在Jupyter环境下进行 <br>
<strong>matplotlib</strong></p>




<pre class="prettyprint"><code class="language-python hljs ">%matplotlib inline
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
plt.style.use(<span class="hljs-string">'ggplot'</span>)</code></pre>




<p><strong>bokeh</strong></p>




<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-keyword">import</span> bokeh.charts <span class="hljs-keyword">as</span> chrt
<span class="hljs-keyword">from</span> bokeh.io <span class="hljs-keyword">import</span> output_notebook
output_notebook()</code></pre>




<h3 id="频率分布分析">频率分布分析</h3>




<p>频率分布图是最为简单有效的观察数据的分布情况的方法之一。</p>




<h4 id="读取数据">读取数据</h4>




<p>本文用到的数据文件依旧是上文所提及的信用欺诈检测的数据集，具体下载地址：<a href="http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz">这里</a></p>


<pre><code class="python">import pyspark.sql.types as typ
fraud = sc.textFile('/home/ef/Desktop/learningPySpark-master/ccFraud.csv')
header = fraud.first()
fraud = fraud \
.filter(lambda row: row != header) \
.map(lambda row: [int(elem) for elem in row.split(',')])
fields = [
*[
typ.StructField(h[1:-1], typ.IntegerType(), True)
for h in header.split(',')
]
]
schema = typ.StructType(fields)
fraud_df = spark.createDataFrame(fraud, schema)
hists = fraud_df.select('balance').rdd.flatMap(
lambda row: row
).histogram(20)

fraud_df.printSchema()
</code></pre>

<blockquote>
  <p>输出： <br>
  root <br>
   |– custID: integer (nullable = true) <br>
   |– gender: integer (nullable = true) <br>
   |– state: integer (nullable = true) <br>
   |– cardholder: integer (nullable = true) <br>
   |– balance: integer (nullable = true) <br>
   |– numTrans: integer (nullable = true) <br>
   |– numIntlTrans: integer (nullable = true) <br>
   |– creditLine: integer (nullable = true) <br>
   |– fraudRisk: integer (nullable = true)</p>
</blockquote>




<h4 id="绘制频率分布直方图">绘制频率分布直方图</h4>




<p><strong>matplotlib</strong></p>




<pre class="prettyprint"><code class="language-python hljs ">data = {
<span class="hljs-string">'bins'</span>: hists[<span class="hljs-number">0</span>][:-<span class="hljs-number">1</span>],
<span class="hljs-string">'freq'</span>: hists[<span class="hljs-number">1</span>]
}
plt.bar(data[<span class="hljs-string">'bins'</span>], data[<span class="hljs-string">'freq'</span>], width=<span class="hljs-number">2000</span>)
plt.title(<span class="hljs-string">'Histogram of \'balance\''</span>)
plt.show()</code></pre>




<blockquote>
  <p>输出: <br>
  <img src="https://ooo.0o0.ooo/2017/10/27/59f2fbe5dc612.png" alt="mat_hist.png" title=""></p>
</blockquote>




<p><strong>bokeh</strong></p>




<pre class="prettyprint"><code class="language-python hljs ">data = {
<span class="hljs-string">'bins'</span>: hists[<span class="hljs-number">0</span>][:-<span class="hljs-number">1</span>],
<span class="hljs-string">'freq'</span>: hists[<span class="hljs-number">1</span>]
}
b_hist = chrt.Bar(
data,
values=<span class="hljs-string">'freq'</span>, label=<span class="hljs-string">'bins'</span>,
title=<span class="hljs-string">'Histogram of \'balance\''</span>)
chrt.show(b_hist)</code></pre>




<blockquote>
  <p>输出: <br>
  <img src="https://ooo.0o0.ooo/2017/10/27/59f2fbe5e1f34.png" alt="bokeh_hist.png" title=""></p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python在Spark上的机器学习(三)之统计分析]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/10/27/pythonzai-sparkshang-de-ji-qi-xue-xi-san-zhi-tong-ji-fen-xi/"/>
    <updated>2017-10-27T18:39:42+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/10/27/pythonzai-sparkshang-de-ji-qi-xue-xi-san-zhi-tong-ji-fen-xi</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="python在spark上的机器学习三之统计分析">Python在Spark上的机器学习(三)之统计分析</h1></p>

<h2 id="背景">背景</h2>




<p>通常来说，一个完整使用机器学习建模解决问题的过程包含一下步骤：</p>




<ul>
<li>数据获取</li>
<li>数据预处理</li>
<li>数据统计分析</li>
<li>算法建模</li>
<li>训练</li>
<li>预测/分类</li>
</ul>




<p>这就意味着，在我们进行一般的数学建模或者挑选机器学习训练算法之前，应该先对数据进行清洗以及简单的统计分析，以便了解数据中显著的特征或者规律（虽然现在的机器学习方法，很多情况下根本不需要了解数据的意义，仅仅是通过堆叠特征就能获得一个可行的结果，但这显然离一个优秀的结果还是有一段距离的）。为了得到更加鲁棒的模型，以及了解数据背后的含义，我们这篇文章就来讲讲如何在PySpark上进行简单的统计分析。</p>




<h3 id="概念介绍">概念介绍</h3>




<p><strong>描述性统计分析</strong> <br>
这是一个统计学的概念，描述性统计是以揭示数据分布特性的方式汇总并表达定量数据的方法。主要包括数据的频数分析、数据的集中趋势分析、数据离散程度分析、数据的分布、以及一些基本的统计图形。特征括并表示定量数据，揭示数据分布的特征。 <br>
描述性统计是一类统计方法的汇总，作用是提供了一种概括和表征数据的有效且相对简便的方法。通常用图示法来表述，易于看懂，能发现质量特性值（总体）的分布状况、趋势走向的一些规律，便于采取措施。用于汇总和表征数据，通常是对数据进一步定量分析的基础，或是对推断性统计方法的有效补充。</p>




<h3 id="数据读取">数据读取</h3>




<p>本文使用的是一个信用欺诈检测的一个数据集，具体下载地址：<a href="http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz">这里</a></p>


<pre><code class="python">import PySpark.sql.types as typ
fraud = sc.textFile('data/ccFraud.csv')
header = fraud.first()
fraud = fraud.filter(lambda row:row!=header)\
.map(lambda row:[int(elem) for elem in row.split(',')])
fields = [
    *[
        typ.StructField(h[1:-1],typ.IntegerType(),True)
        for h in header.split(',')
    ]
]
schema = typ.StructType(fields)
fraud_df = spark.createDataFrame(fraud,schema=schema)
fraud_df.printSchema()
fraud_df.head()
</code></pre>

<blockquote>
  <p>输出结果 <br>
  root <br>
   |– custID: integer (nullable = true) <br>
   |– gender: integer (nullable = true) <br>
   |– state: integer (nullable = true) <br>
   |– cardholder: integer (nullable = true) <br>
   |– balance: integer (nullable = true) <br>
   |– numTrans: integer (nullable = true) <br>
   |– numIntlTrans: integer (nullable = true) <br>
   |– creditLine: integer (nullable = true) <br>
   |– fraudRisk: integer (nullable = true) <br>
  Out: <br>
  Row(custID=1, gender=1, state=35, cardholder=1, balance=3000, numTrans=4, numIntlTrans=14, creditLine=2, fraudRisk=0)</p>
</blockquote>




<p>在上面的代码中，我们读入了我们的数据集，以及创建了一个DataFrame，下面我们再进行一些简单的分析操作。</p>




<h3 id="简单统计分析">简单统计分析</h3>


<pre><code class="python">fraud_df.groupby('gender').count().show() #按照性别分类汇总


numerical = ['balance', 'numTrans', 'numIntlTrans']
desc = fraud_df.describe(numerical) 
##对balance，numTrans，numIntTrans进行描述性分析
desc.show()


# 计算balance值分布的偏度
fraud_df.agg({'balance': 'skewness'}).show()
</code></pre>

<blockquote>
  <p>输出结果 <br>
  +——+——-+ <br>
  |gender|  count| <br>
  +——+——-+ <br>
  |     1|6178231| <br>
  |     2|3821769| <br>
  +——+——-+ <br>
  +——-+—————–+——————+—————–+ <br>
  |summary|          balance|          numTrans|     numIntlTrans| <br>
  +——-+—————–+——————+—————–+ <br>
  |  count|         10000000|          10000000|         10000000| <br>
  |   mean|     4109.9199193|        28.9351871|        4.0471899| <br>
  | stddev|3996.847309737258|26.553781024523122|8.602970115863904| <br>
  |    min|                0|                 0|                0| <br>
  |    max|            41485|               100|               60| <br>
  +——-+—————–+——————+—————–+ <br>
  +——————+ <br>
  | skewness(balance)| <br>
  +——————+ <br>
  |1.1818315552993839| <br>
  +——————+</p>
</blockquote>




<p>上面我们使用了一些常用的统计分析函数，以及简单地了解了一下PySpark 聚合函数agg()的使用。通常地，常用的聚合函数还有avg() , count(), countDistinct() , max() 等。</p>




<h3 id="相关分析">相关分析</h3>




<p><strong>相关分析</strong>（correlation analysis），相关分析是研究现象之间是否存在某种依存关系，并对具体有依存关系的现象探讨其相关方向以及相关程度，是研究随机变量之间的相关关系的一种统计方法。通常两个变量之间存在的相关关系有：正相关、完全正相关、负相关、完全负相关、无相关。</p>




<p><strong>相关系数</strong>是最早由统计学家卡尔·皮尔逊设计的统计指标，是研究变量之间线性相关程度的量，一般用字母 r 表示。由于研究对象的不同，相关系数有多种定义方式，较为常用的是皮尔逊相关系数。简单来说，相关系数是衡量两个变量间相关关系的指标。</p>




<p>在PySpark中计算两个变量之间的相关系数是非常简单的，往往只需要一条代码：</p>




<blockquote>
  <p>fraud_df.corr(‘balance’, ‘numTrans’)#计算balance和numTrans的相关系数</p>
</blockquote>




<p>输出</p>




<blockquote>
  <p>Out：0.0004452314017265385</p>
</blockquote>




<p>我们还可以通过下面的方法来创建一个相关矩阵。</p>


<pre><code class="python">n_numerical = len(numerical)
corr = []
for i in range(0, n_numerical):
    temp = [None] * i
    for j in range(i, n_numerical):
        temp.append(fraud_df.corr(numerical[i], numerical[j]))
    corr.append(temp)
</code></pre>

<blockquote>
  <p>输出结果; <br>
  [[1.0, 0.0004452314017265387, 0.0002713991339817875], [None, 1.0, -0.00028057128198165555], [None, None, 1.0]]</p>
</blockquote>




<p>正如输出结果所示，这个信用欺诈检测的数据集中的特征之间不存在过大的相关关系，基本全部都为相互独立关系。因此，在选择特征代入机器学习算法训练时，可以采用全部的变量。正也是统计分析的用处，可以帮助我们了解变量之间的线性相关关系，有利于帮住我们选取训练的特征变量。</p>

]]></content>
  </entry>
  
</feed>
