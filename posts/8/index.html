
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="ja"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>EdmondFrank's 时光足迹</title>
  <meta name="author" content="EdmondFrank">

  
  <meta name="description" content="﻿K-means聚类简介 前面介绍了不少监督学习的各类算法，那么这次我们就来认识和了解一下无监督学习。首先，K-means聚类算法就是一种无监督学习的算法。这就意味着他可以通过没有分类标记的数据学习出训练数据本身的分组信息。（其实他的本质就是自动的将属性/特征类似的数据归到统一类别） &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="https://edmondfrank.github.io/posts/8/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="EdmondFrank's 时光足迹" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.cat.net/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
   <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_SVG"></script>
  
  

</head>

<body >
  <div id="container">
    <header role="banner"><hgroup>
  <h1><a href="/">EdmondFrank's 时光足迹</a></h1>
  
    <h2>この先は暗い夜道だけかもしれない　それでも信じて進むんだ。星がその道を少しでも照らしてくれるのを。<br>或许前路永夜，即便如此我也要前进，因为星光即使微弱也会我为照亮前途。<br>——《四月は君の嘘》</h2>
  
</hgroup>

</header>
    <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="site:edmondfrank.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/archives">Archives</a></li>
</ul>

</nav>
    <div id="main">
      <div id="content">
        <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/10/06/k-meansju-lei-jian-jie/">K-means聚类简介</a></h1>
    
    
      <div class="post-meta">
        <p class="meta">
          <span class="timestamp">- 








  



<time datetime="2017-10-06T14:28:25+08:00" pubdate data-updated="true"></time> -</span>
          
        </p>
      </div>
    
  </header>


  <div class="entry-content"><p>﻿<h1 id="k-means聚类简介">K-means聚类简介</h1></p>

<p>前面介绍了不少监督学习的各类算法，那么这次我们就来认识和了解一下无监督学习。首先，K-means聚类算法就是一种无监督学习的算法。这就意味着他可以通过没有分类标记的数据学习出训练数据本身的分组信息。（其实他的本质就是自动的将属性/特征类似的数据归到统一类别）而其最后分成的组数则由参数K来决定。</p>




<h2 id="算法基本步骤">算法基本步骤</h2>




<ol>
<li>随机初始化K个簇心。用作标识各组数据。</li>
<li>根据各个数据的特征/属性，将他们聚类到距离最接近的一个簇，并打上分类标识。</li>
<li>根据加入的数据重新计算每个簇的质心（均值，Means）</li>
<li>重复2，3步，直至最后所有数据被划分成k个簇。</li>
</ol>




<p>从以后的步骤我们也可以充分地看出，再聚类完毕之前数据是没有分组和标识的，而是通过聚类来查找和分析被划分在一起的数据。</p>




<p>同时，每个簇的质心是定义组的特征的代表，我们往往可以通过一个簇的质心看出该分组的特征以及属性。从而判断出这个簇代表了一个怎么样的组。</p>




<h2 id="应用领域">应用领域</h2>




<ul>
<li>网站，App的用户群体聚类</li>
<li>信用卡诈骗监测（这个是利用了聚类算法的反原理，寻找离群值）</li>
<li>文章自动归类</li>
<li>图像归类</li>
<li>人群健康状态监测（同信用卡诈骗检测同原理）</li>
</ul>




<p>此外，我们还可以通过监控数据点根据时间的变化而造成的组间切换现象找到状态变化的方法。</p>




<h2 id="算法主要原理">算法主要原理</h2>




<p>（1）在数据划分方面，我们主要依赖的思路是：<strong>欧氏距离</strong>。即，每个簇的质心定义了一种聚类，然后每个点根据其到每个簇的质心的欧几里德距离的平方的大小，将其分配到最近的簇中。我们假设<script type="math/tex" id="MathJax-Element-1">c_i</script>是簇集合C中质心i，那么我们可以将其聚类的过程表达成下列形式：</p>




<p><script type="math/tex" id="MathJax-Element-2">argmin_{c_i \in C} dist(c_i,x)^2</script></p>




<p>（2）<strong>质心的更新</strong>，在这一步骤中，各个簇的质心将要被重新计算，通过加入新的数据再重新计算包括i新数据在内的的整个簇的均值以此作为新的质心。该过程可以表达为一下形式：</p>




<p><script type="math/tex" id="MathJax-Element-3">c_i=\frac{1}{|S_i|}\sum_{x_i \in S_i }x_i</script></p>




<p>在多次迭代之后，该算法可以保证收敛于一个结果，虽然有可能只是一个局部的最优解而已。为了使得最后结果更加稳定和可靠，我们也可以通过多次运行算法然后再在多个结果中取得一个均衡（为什么说多次计算可以得到更好的结果呢？ 因为每次随机初始化的质心都是不同的，这样使得每次的运行结果都不尽相同，为了得到一个更加稳定的结果和更好的鲁棒性，多次运行往往是一个简单但却有效的方法。）</p>




<h2 id="有关k值的选择">有关K值的选择</h2>




<p>在K-means聚类算法中，K值的选择往往是一个较为难以抉择的问题。最后的情况是，在你应用这个算法之前，已经知道数据应该分为多少个类别合适，或者将这种方法应用在一个半监督学习的场合，使得最后的分类结果有据可依。</p>




<p>那么，在一般我们不太清楚分类的情况之下，我们能做的就是通过为K值制定一个范围，然后在这个范围之上，我们尝试运行我们的分类器，然后根据分类效果来评判我们的k的取值。</p>




<p>另外一方面，聚类算法通常会不断降低各个样本点到质心之间的距离，随着k值的增加，“距离”更是一直下降。但是，当k值增大到一定程度时，虽然“距离”仍在下降，但下降的速率已经变得十分缓慢了。我们常常称这个点的k值为<strong>肘点</strong>。往往到达肘点的k值已经能够很好地将各个类别之间的差异信息给描述出来，所以我们可以在不清楚数据具体分类信息的情况下，选择肘点值来作为我们的k值。</p>




<p><img src="https://www.datascience.com/hs-fs/hubfs/Blog/introduction-to-k-means-clustering-elbow-point-example.png?t=1507257936532&amp;width=760&amp;height=411&amp;name=introduction-to-k-means-clustering-elbow-point-example.png" alt="enter image description here" title=""></p>




<h2 id="example">Example</h2>




<p>在这里我们通过Python的Sklearn库来看一下K-means的应用实例</p>




<p><a href="https://raw.githubusercontent.com/datascienceinc/learn-data-science/master/Introduction-to-K-means-Clustering/Data/data_1024.csv">Delivery Fleet Data 数据下载</a></p>




<h3 id="数据读取">数据读取</h3>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class='line'><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&quot;data_1024.csv&quot;</span><span class="p">,</span><span class="n">sep</span><span class="o">=</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;Distance_Feature&#39;</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;Speeding_Feature&#39;</span><span class="p">])</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;Distance_Feature&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;Speeding_Feature&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;Delivery Fleet Data&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span class='line'><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>


<blockquote>
  <p>输出结果 <br>
    Driver_ID   Distance_Feature    Speeding_Feature <br>
  0   3423311935  71.24   28.0 <br>
  1   3423313212  52.53   25.0 <br>
  2   3423313724  64.54   27.0 <br>
  3   3423311373  55.69   22.0 <br>
  4   3423310999  54.58   25.0 <br>
  <img src="https://i.loli.net/2017/10/06/59d71bde43c28.png" alt="kmeans.png" title=""></p>
</blockquote>




<h3 id="算法使用">算法使用</h3>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
</span><span class='line'><span class="n">X_train</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">&#39;Distance_Feature&#39;</span><span class="p">,</span><span class="s">&#39;Speeding_Feature&#39;</span><span class="p">]]</span>
</span><span class='line'><span class="n">kmeas</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">item</span><span class="o">/</span><span class="mf">255.</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">kmeas</span><span class="o">.</span><span class="n">labels_</span><span class="p">]</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;Distance_Feature&#39;</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;Speeding_Feature&#39;</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;Distance_Feature&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;Speeding_Feature&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;Delivery Fleet Data&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span class='line'><span class="n">kmeas</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">item</span><span class="o">/</span><span class="mf">255.</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">kmeas</span><span class="o">.</span><span class="n">labels_</span><span class="p">]</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;Distance_Feature&#39;</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;Speeding_Feature&#39;</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;Distance_Feature&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;Speeding_Feature&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;Delivery Fleet Data&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>




<blockquote>
  <p>输出结果 <br>
  <img src="https://i.loli.net/2017/10/06/59d720b726068.png" alt="sk-kmeans.png" title=""></p>
</blockquote>




<p>上面的结果展示了，K分别等于2和4时对数据的聚类的结果，由于这个算法使用还算简单，在这里我也不做赘述了。那么有关K-means的简介到这里也就结束了，谢谢大家的阅读。</p>

</div>
  
  


      <footer>
      
      - <a href="/blog/2017/10/06/k-meansju-lei-jian-jie/">K-means聚类简介</a>
      <time datetime="2017-10-06T14:28:25+08:00" pubdate><span class='month'>Oct</span> <span class='day'>06</span> <span class='year'>2017</span></time>
      
      <span class="categories">posted in <a class='category' href='/blog/categories/ji-qi-xue-xi/'>机器学习</a></span>
      
      </footer>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/09/30/ji-qi-xue-xi-zhi-ti-du-xia-jiang/">机器学习之梯度下降</a></h1>
    
    
      <div class="post-meta">
        <p class="meta">
          <span class="timestamp">- 








  



<time datetime="2017-09-30T20:19:04+08:00" pubdate data-updated="true"></time> -</span>
          
        </p>
      </div>
    
  </header>


  <div class="entry-content"><h1 id="机器学习之梯度下降">机器学习之梯度下降</h1>




<h2 id="梯度下降gradient-descent">梯度下降(gradient descent)</h2>




<p>是线性回归的一种(Linear Regression)，在<a href="https://www.bilibili.com/video/av9912938/?from=search&amp;seid=3888228796805461518">Andrew Ng的machine learning</a>前期课程中有着详细的讲解，首先给出一个关于课程中经典的例子:预测房屋的价格。</p>




<table>
<thead>
<tr>
  <th>面积(feet2)</th>
  <th>房间个数</th>
  <th>价格（1000$）</th>
</tr>
</thead>
<tbody><tr>
  <td>2104</td>
  <td>3</td>
  <td>400</td>
</tr>
<tr>
  <td>1600</td>
  <td>3</td>
  <td>330</td>
</tr>
<tr>
  <td>2400</td>
  <td>3</td>
  <td>369</td>
</tr>
<tr>
  <td>1416</td>
  <td>2</td>
  <td>232</td>
</tr>
<tr>
  <td>3000</td>
  <td>4</td>
  <td>540</td>
</tr>
</tbody></table>




<p>在上面的表格中，房间的个数以及房屋的面积都是输入变量而，价格就是我们要预测输出的值。其中，面积和房屋个数分别表示一个特征，我们在这将他们一起用X来表示。价格用Y来表示。同时表格的每一行表示成一个样本，具体表示为：<script type="math/tex" id="MathJax-Element-1">f: X_1=[24104,3] \to Y_1=400</script>。</p>




<p>那么我们具体的任务就可以概括成给定一个训练集合，学习出一个函数<script type="math/tex" id="MathJax-Element-2">f</script>，使得<script type="math/tex" id="MathJax-Element-3">f(x)</script>能够符合结果Y，或者说能够使得<script type="math/tex" id="MathJax-Element-4">f(x)</script>与Y的之间的误差最小。</p>




<p>我们可以用以下的式子来表达上述的关系：</p>




<p><script type="math/tex" id="MathJax-Element-5">f(x) = \theta_1 x_1 + \theta_2 x_2 + \beta</script></p>




<p>其中，<script type="math/tex" id="MathJax-Element-6">\theta 表示X映射成Y的权重，而x表示为各组特征。</script> <br>
我们再分别用<script type="math/tex" id="MathJax-Element-7">x^{j},y^{j}来表示第J个样本</script>。这样我们就可以写出代价函数：</p>




<p><script type="math/tex" id="MathJax-Element-8">J(\theta) = \frac {1}{2}\sum ^m_{i=0}(f_\theta(x^{i})-y^{(i)})^2</script></p>




<p>然后我们要使得计算的结果无限接近真实值y的话，我们就要令得我们的代价函数（俗称的误差）最小化。要使得<script type="math/tex" id="MathJax-Element-9">J(\theta)</script>最小，即对<script type="math/tex" id="MathJax-Element-10">J(\theta)</script>进行求导操作，并使得其结果为0。</p>




<p><script type="math/tex" id="MathJax-Element-11">\theta_j = \theta_j - \frac{\partial J(\theta)}{\partial \theta_j}</script></p>




<p>当只有单个特征时，上式中的j表示系数（权重）的编号，右边的值赋给左边的变量后完成一次迭代过程。</p>




<p>而多个特征时，其迭代过程如下： <br>
<img src="https://i.loli.net/2017/09/23/59c5f7bb6ccee.png" alt="gd.png" title=""></p>




<p>上式就是<strong>批梯度下降算法(batch gradient descent)</strong>，当上式收敛时则退出迭代，何为收敛，即前后两次迭代的值不再发生变化了。一般情况下，会设置一个具体的参数，当前后两次迭代差值小于该参数时候结束迭代。注意以下几点：</p>




<p>(1) <strong>a 即学习率（learning rate</strong>），决定的下降步伐，如果太小，则找到函数最小值的速度就很慢，如果太大，则可能会出现无法逼近最小值的现象；</p>




<p>(2) 初始点不同，获得的最小值也不同，因此梯度下降求得的只是局部最小值；</p>




<p>(3) 越接近最小值时，下降速度越慢；</p>




<p>(4) 计算批梯度下降算法时候，计算每一个θ值都需要遍历计算所有样本，当数据量的时候这是比较费时的计算。</p>




<p>批梯度下降算法的步骤可以归纳为以下几步：</p>




<p>(1)先确定向下一步的步伐大小，我们称为Learning rate ；</p>




<p>(2)任意给定一个初始值：θ向量，一般为0向量</p>




<p>(3)确定一个向下的方向，并向下走预先规定的步伐，并更新θ向量</p>




<p>(4)当下降的高度小于某个定义的值，则停止下降；</p>




<h2 id="随机梯度下降算法">随机梯度下降算法</h2>




<p>因为每次计算梯度都需要遍历所有的样本点。这是因为梯度是J(θ)的导数，而J(θ)是需要考虑所有样本的误差和 ，这个方法问题就是，扩展性问题，当样本点很大的时候，基本就没法算了。</p>




<p>所以接下来又提出了<strong>随机梯度下降算法(stochastic gradient descent )</strong>。随机梯度下降算法，每次迭代只是考虑让该样本点的J(θ)趋向最小，而不管其他的样本点，这样算法会很快，但是收敛的过程会比较曲折，整体效果上，大多数时候它只能接近局部最优解，而无法真正达到局部最优解。所以适合用于较大训练集的例子。</p>




<p>其具体流程用公式表达如下：</p>




<p><img src="https://i.loli.net/2017/09/23/59c5f947573e4.png" alt="sgd.png" title=""></p>




<h2 id="代码实现">代码实现</h2>




<h3 id="批梯度下降算法">批梯度下降算法</h3>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c">#Training data set</span>
</span><span class='line'><span class="c">#each element in x represents (x0,x1,x2)</span>
</span><span class='line'><span class="n">x</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mf">4.</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span>
</span><span class='line'><span class="c">#y[i] is the output of y = theta0 * x[0] + theta1 * x[1] +theta2 * x[2]</span>
</span><span class='line'><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">95.364</span><span class="p">,</span><span class="mf">97.217205</span><span class="p">,</span><span class="mf">75.195834</span><span class="p">,</span><span class="mf">60.105519</span><span class="p">,</span><span class="mf">49.342380</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.000001</span>
</span><span class='line'><span class="c">#learning rate</span>
</span><span class='line'><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>
</span><span class='line'><span class="n">diff</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
</span><span class='line'><span class="n">error1</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'><span class="n">error0</span> <span class="o">=</span><span class="mi">0</span>
</span><span class='line'><span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c">#init the parameters to zero</span>
</span><span class='line'><span class="n">theta0</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'><span class="n">theta1</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'><span class="n">theta2</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'><span class="n">sum0</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'><span class="n">sum1</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'><span class="n">sum2</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'><span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class='line'>
</span><span class='line'>    <span class="c">#calculate the parameters</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class='line'>        <span class="c">#begin batch gradient descent</span>
</span><span class='line'>        <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="p">(</span> <span class="n">theta0</span> <span class="o">+</span> <span class="n">theta1</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta2</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="p">)</span>
</span><span class='line'>        <span class="n">sum0</span> <span class="o">=</span> <span class="n">sum0</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span class='line'>        <span class="n">sum1</span> <span class="o">=</span> <span class="n">sum1</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span><span class='line'>        <span class="n">sum2</span> <span class="o">=</span> <span class="n">sum2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>
</span><span class='line'>        <span class="c">#end  batch gradient descent</span>
</span><span class='line'>    <span class="n">theta0</span> <span class="o">=</span> <span class="n">sum0</span><span class="p">;</span>
</span><span class='line'>    <span class="n">theta1</span> <span class="o">=</span> <span class="n">sum1</span><span class="p">;</span>
</span><span class='line'>    <span class="n">theta2</span> <span class="o">=</span> <span class="n">sum2</span><span class="p">;</span>
</span><span class='line'>    <span class="c">#calculate the cost function</span>
</span><span class='line'>    <span class="n">error1</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">lp</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
</span><span class='line'>        <span class="n">error1</span> <span class="o">+=</span> <span class="p">(</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="p">(</span> <span class="n">theta0</span> <span class="o">+</span> <span class="n">theta1</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta2</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="p">)</span> <span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">error1</span><span class="o">-</span><span class="n">error0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
</span><span class='line'>        <span class="k">break</span>
</span><span class='line'>    <span class="k">else</span><span class="p">:</span>
</span><span class='line'>        <span class="n">error0</span> <span class="o">=</span> <span class="n">error1</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">print</span><span class="p">(</span><span class="s">&#39; theta0 : </span><span class="si">%f</span><span class="s">, theta1 : </span><span class="si">%f</span><span class="s">, theta2 : </span><span class="si">%f</span><span class="s">, error1 : </span><span class="si">%f</span><span class="s">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span><span class="n">theta1</span><span class="p">,</span><span class="n">theta2</span><span class="p">,</span><span class="n">error1</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="k">print</span> <span class="p">(</span><span class="s">&#39;Done: theta0 : </span><span class="si">%f</span><span class="s">, theta1 : </span><span class="si">%f</span><span class="s">, theta2 : </span><span class="si">%f</span><span class="s">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span><span class="n">theta1</span><span class="p">,</span><span class="n">theta2</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>




<blockquote>
  <p>输出结果： <br>
   theta0 : 0.377225, theta1 : 0.625295, theta2 : 1.120912, error1 : 4405.869965 <br>
   theta0 : 0.729497, theta1 : 1.193311, theta2 : 2.164098, error1 : 3094.652486 <br>
   theta0 : 1.058680, theta1 : 1.708424, theta2 : 3.135362, error1 : 2089.261446 <br>
   theta0 : 1.366497, theta1 : 2.174683, theta2 : 4.040070, error1 : 1335.974025 <br>
   theta0 : 1.654541, theta1 : 2.595831, theta2 : 4.883186, error1 : 789.589683 <br>
   theta0 : 1.924288, theta1 : 2.975327, theta2 : 5.669299, error1 : 412.137681 <br>
   theta0 : 2.177098, theta1 : 3.316371, theta2 : 6.402654, error1 : 171.776368 <br>
   theta0 : 2.414234, theta1 : 3.621921, theta2 : 7.087177, error1 : 41.856096 <br>
   theta0 : 2.636861, theta1 : 3.894714, theta2 : 7.726499, error1 : 0.121739 <br>
   theta0 : 2.846057, theta1 : 4.137277, theta2 : 8.323976, error1 : 28.034282 <br>
   theta0 : 3.042819, theta1 : 4.351950, theta2 : 8.882714, error1 : 110.193963 <br>
   …… <br>
    theta0 : 98.101057, theta1 : -13.028753, theta2 : 1.133773, error1 : 3.473679 <br>
   theta0 : 98.101058, theta1 : -13.028753, theta2 : 1.133773, error1 : 3.473678 <br>
   theta0 : 98.101058, theta1 : -13.028753, theta2 : 1.133773, error1 : 3.473677 <br>
   theta0 : 98.101059, theta1 : -13.028753, theta2 : 1.133773, error1 : 3.473676 <br>
   theta0 : 98.101059, theta1 : -13.028753, theta2 : 1.133773, error1 : 3.473675 <br>
   theta0 : 98.101060, theta1 : -13.028753, theta2 : 1.133772, error1 : 3.473674 <br>
   theta0 : 98.101061, theta1 : -13.028753, theta2 : 1.133772, error1 : 3.473673 <br>
  Done: theta0 : 98.101061, theta1 : -13.028753, theta2 : 1.133772</p>
</blockquote>




<p>由上面的输出结果可以知道，梯度下降算法会在迭代一定次数后收敛于一个较少的损失值（即error）然而这并不是最优解而只是一个局部最小值（因为我们也可以从结果看到 theta0 : 2.636861, theta1 : 3.894714, theta2 : 7.726499, error1 : 0.121739，这项数据的最终error反而优于我们的最终解）。</p>




<h3 id="随机梯度下降算法-1">随机梯度下降算法</h3>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c">#Training data set</span>
</span><span class='line'><span class="c">#each element in x represents (x0,x1,x2)</span>
</span><span class='line'><span class="n">x</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mf">4.</span><span class="p">,</span><span class="mi">4</span><span class="p">)]</span>
</span><span class='line'><span class="c">#y[i] is the output of y = theta0 * x[0] + theta1 * x[1] +theta2 * x[2]</span>
</span><span class='line'><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">95.364</span><span class="p">,</span><span class="mf">97.217205</span><span class="p">,</span><span class="mf">75.195834</span><span class="p">,</span><span class="mf">60.105519</span><span class="p">,</span><span class="mf">49.342380</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.0001</span>
</span><span class='line'><span class="c">#learning rate</span>
</span><span class='line'><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>
</span><span class='line'><span class="n">diff</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
</span><span class='line'><span class="n">error1</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'><span class="n">error0</span> <span class="o">=</span><span class="mi">0</span>
</span><span class='line'><span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="c">#init the parameters to zero</span>
</span><span class='line'><span class="n">theta0</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'><span class="n">theta1</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'><span class="n">theta2</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'>
</span><span class='line'><span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class='line'>
</span><span class='line'>    <span class="c">#calculate the parameters</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="p">(</span> <span class="n">theta0</span> <span class="o">+</span> <span class="n">theta1</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta2</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="p">)</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">theta0</span> <span class="o">=</span> <span class="n">theta0</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span class='line'>        <span class="n">theta1</span> <span class="o">=</span> <span class="n">theta1</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span><span class='line'>        <span class="n">theta2</span> <span class="o">=</span> <span class="n">theta2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'>    <span class="c">#calculate the cost function</span>
</span><span class='line'>    <span class="n">error1</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">lp</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
</span><span class='line'>        <span class="n">error1</span> <span class="o">+=</span> <span class="p">(</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="p">(</span> <span class="n">theta0</span> <span class="o">+</span> <span class="n">theta1</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta2</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="p">)</span> <span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">error1</span><span class="o">-</span><span class="n">error0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
</span><span class='line'>        <span class="k">break</span>
</span><span class='line'>    <span class="k">else</span><span class="p">:</span>
</span><span class='line'>        <span class="n">error0</span> <span class="o">=</span> <span class="n">error1</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">print</span> <span class="p">(</span><span class="s">&#39; theta0 : </span><span class="si">%f</span><span class="s">, theta1 : </span><span class="si">%f</span><span class="s">, theta2 : </span><span class="si">%f</span><span class="s">, error1 : </span><span class="si">%f</span><span class="s">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span><span class="n">theta1</span><span class="p">,</span><span class="n">theta2</span><span class="p">,</span><span class="n">error1</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="k">print</span> <span class="p">(</span><span class="s">&#39;Done: theta0 : </span><span class="si">%f</span><span class="s">, theta1 : </span><span class="si">%f</span><span class="s">, theta2 : </span><span class="si">%f</span><span class="s">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span><span class="n">theta1</span><span class="p">,</span><span class="n">theta2</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>




<blockquote>
  <p>输出结果： <br>
   theta0 : 2.782632, theta1 : 3.207850, theta2 : 7.998823, error1 : 7.508687 <br>
   theta0 : 4.254302, theta1 : 3.809652, theta2 : 11.972218, error1 : 813.550287 <br>
   theta0 : 5.154766, theta1 : 3.351648, theta2 : 14.188535, error1 : 1686.507256 <br>
   theta0 : 5.800348, theta1 : 2.489862, theta2 : 15.617995, error1 : 2086.492788 <br>
   theta0 : 6.326710, theta1 : 1.500854, theta2 : 16.676947, error1 : 2204.562407 <br>
   theta0 : 6.792409, theta1 : 0.499552, theta2 : 17.545335, error1 : 2194.779569 <br>
   theta0 : 7.223066, theta1 : -0.467855, theta2 : 18.302105, error1 : 2134.182794 <br>
   theta0 : 7.630213, theta1 : -1.384304, theta2 : 18.982980, error1 : 2056.719790 <br>
   …… <br>
    theta0 : 97.986505, theta1 : -13.221170, theta2 : 1.257223, error1 : 1.553680 <br>
   theta0 : 97.986620, theta1 : -13.221169, theta2 : 1.257186, error1 : 1.553579 <br>
   theta0 : 97.986735, theta1 : -13.221167, theta2 : 1.257150, error1 : 1.553479 <br>
   theta0 : 97.986849, theta1 : -13.221166, theta2 : 1.257113, error1 : 1.553379 <br>
   theta0 : 97.986963, theta1 : -13.221165, theta2 : 1.257077, error1 : 1.553278 <br>
  Done: theta0 : 97.987078, theta1 : -13.221163, theta2 : 1.257041</p>
</blockquote>




<p>通过上述批梯度下降和随机梯度下降算法代码的对比，不难发现两者的区别： <br>
随机梯度下降算法在迭代的时候，每迭代一个新的样本，就会更新一次所有的theta参数。 <br>
因此当样本数量很大时候，批梯度得做完所有样本的计算才能更新一次theta，从而花费的时间远大于随机梯度下降。但是随机梯度下降过早的结束了迭代，使得它获取的值只是接近局部最优解，而并非像批梯度下降算法那样就是局部最优解。</p>

</div>
  
  


      <footer>
      
      - <a href="/blog/2017/09/30/ji-qi-xue-xi-zhi-ti-du-xia-jiang/">机器学习之梯度下降</a>
      <time datetime="2017-09-30T20:19:04+08:00" pubdate><span class='month'>Sep</span> <span class='day'>30</span> <span class='year'>2017</span></time>
      
      <span class="categories">posted in <a class='category' href='/blog/categories/ji-qi-xue-xi/'>机器学习</a></span>
      
      </footer>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/09/24/python-shi-xian-tui-jian-xi-tong/">Python 实现推荐系统</a></h1>
    
    
      <div class="post-meta">
        <p class="meta">
          <span class="timestamp">- 








  



<time datetime="2017-09-24T10:51:37+08:00" pubdate data-updated="true"></time> -</span>
          
        </p>
      </div>
    
  </header>


  <div class="entry-content"><h1 id="python-实现推荐系统">Python 实现推荐系统</h1>




<h2 id="引言">引言</h2>




<p>最早的推荐系统应该是亚马逊为了提升长尾货物的用户抵达率而发明的。已经有数据证明，长尾商品的销售额以及利润总和与热门商品是基本持平的。亚马逊网站上在线销售的商品何止百万，但首页能够展示的商品数量又极其有限，给用户推荐他们可能喜欢的商品就成了一件非常重要的事情。当然，商品搜索也是一块大蛋糕，亚马逊的商品搜索早已经开始侵蚀谷歌的核心业务了。</p>




<p>从这些例子之中，我们可以看到我们能够使用许多不同的方式来搜集兴趣偏好。有时候，这些数据可能来自人们购买的商品，以及这些商品关联的评价信息。我们可以利用一组算法从中挖掘，建立几个有意思的推荐系统。</p>




<h2 id="推荐系统的简介">推荐系统的简介</h2>




<p>两种最普遍的推荐系统的类型是基于内容的推荐系统和协同过滤推荐系统（CF）。协同过滤基于用户对产品的态度产生推荐，基于内容的推荐系统基于物品属性的相似性进行推荐。CF可以分为基于内存的协同过滤和基于模型的协同过滤。</p>




<h3 id="基于内容推荐">基于内容推荐</h3>




<p><strong>基于内容的推荐（Content-based Recommendation）</strong>，它是建立在项目的内容信息上作出推荐的，而不需要依据用户对项目的评价意见，更多地需要用机器学习的方法从关于内容的特征描述的事例中得到用户的兴趣资料。在基于内容的推荐系统中，项目或对象是通过相关的特征的属性来定义，系统基于用户评价对象的特征，学习用户的兴趣，考察用户资料与待预测项目的相匹配程度。用户的资料模型取决于所用学习方法，常用的有决策树、神经网络和基于向量的表示方法等。</p>




<h4 id="基于内容推荐方法的优点是">基于内容推荐方法的优点是：</h4>




<ul>
<li>不需要其它用户的数据，没有冷开始问题和稀疏问题。</li>
<li>能为具有特殊兴趣爱好的用户进行推荐。</li>
<li>能推荐新的或不是很流行的项目，没有新项目问题。</li>
<li>通过列出推荐项目的内容特征，可以解释为什么推荐那些项目。</li>
<li>已有比较好的技术，如关于分类学习方面的技术已相当成熟。</li>
</ul>




<h4 id="缺点是">缺点是:</h4>




<p>要求内容能容易抽取成有意义的特征，要求特征内容有良好的结构性，并且用户的口味必须能够用内容特征形式来表达，不能显式地得到其它用户的判断情况。</p>




<h3 id="协同过滤推荐">协同过滤推荐</h3>




<p><strong>协同过滤推荐（Collaborative Filtering Recommendation）</strong>，是推荐系统中应用最早和最为成功的技术之一。它一般采用最近邻技术，利用用户的历史喜好信息计算用户之间的距离，然后利用目标用户的最近邻居用户对商品评价的加权评价值来预测目标用户对特定商品的喜好程度，系统从而根据这一喜好程度来对目标用户进行推荐。协同过滤最大优点是对推荐对象没有特殊的要求，能处理非结构化的复杂对象，如音乐、电影。</p>




<p>协同过滤是基于这样的假设：为一用户找到他真正感兴趣的内容的好方法是首先找到与此用户有相似兴趣的其他用户，然后将他们感兴趣的内容推荐给此用 户。其基本思想非常易于理解，在日常生活中，我们往往会利用好朋友的推荐来进行一些选择。协同过滤正是把这一思想运用到电子商务推荐系统中来，基于其他用 户对某一内容的评价来向目标用户进行推荐。</p>




<p>基于协同过滤的推荐系统可以说是从用户的角度来进行相应推荐的，而且是自动的即用户获得的推荐是系统从购买模式或浏览行为等隐式获得的，不需要用户努力地找到适合自己兴趣的推荐信息，如填写一些调查表格等。</p>




<p>和基于内容的过滤方法相比，</p>




<h4 id="协同过滤具有如下的优点">协同过滤具有如下的优点：</h4>




<ul>
<li>能够过滤难以进行机器自动内容分析的信息，如艺术品，音乐等。</li>
<li>共享其他人的经验，避免了内容分析的不完全和不精确，并且能够基于一些复杂的，难以表述的概念（如信息质量、个人品味）进行过滤。</li>
<li>有推荐新信息的能力。可以发现内容上完全不相似的信息，用户对推荐信息的内容事先是预料不到的。这也是协同过滤和基于内容的过滤一个较大的差别，基于内容的过滤推荐很多都是用户本来就熟悉的内容，而协同过滤可以发现用户潜在的但自己尚未发现的兴趣偏好。</li>
<li>能够有效的使用其他相似用户的反馈信息，较少用户的反馈量，加快个性化学习的速度。</li>
</ul>




<h4 id="缺点是-1">缺点是：</h4>




<p>最典型的问题有， <br>
稀疏问题（Sparsity） <br>
可扩展问题（Scalability） <br>
冷启动问题</p>




<h2 id="实例应用">实例应用</h2>




<p>我们下面通过使用MovieLens数据集（这是一个有关电影评分的经典数据集， 其中Movielens-100k和movielens-1M有用户对电影的打分，电影的title、genre、IMDB链接、用户的gender、age、occupation、zip code。movielens-10M中还有用户对电影使用的tag信息。）</p>




<p>数据集的<a href="http://files.grouplens.org/datasets/movielens/ml-100k.zip">下载地址</a></p>




<h3 id="数据读取">数据读取</h3>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'><span class="n">header</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;user_id&#39;</span><span class="p">,</span><span class="s">&#39;item_id&#39;</span><span class="p">,</span><span class="s">&#39;rating&#39;</span><span class="p">,</span><span class="s">&#39;timestamp&#39;</span><span class="p">]</span>
</span><span class='line'><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;/home/ef/Desktop/ml-100k/u.data&#39;</span><span class="p">,</span>
</span><span class='line'>                <span class="n">sep</span><span class="o">=</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">,</span><span class="n">names</span><span class="o">=</span><span class="n">header</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</span><span class='line'><span class="n">n_users</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">user_id</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class='line'><span class="n">n_items</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">item_id</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="s">&#39;Number of users = &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_users</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#39; | Number of movies = &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_items</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>




<blockquote>
  <p>输出结果： <br>
     user_id  item_id  rating  timestamp <br>
  0      196      242       3  881250949 <br>
  1      186      302       3  891717742 <br>
  2       22      377       1  878887116 <br>
  3      244       51       2  880606923 <br>
  4      166      346       1  886397596 <br>
  Number of users = 943 | Number of movies = 1682</p>
</blockquote>




<h3 id="数据划分">数据划分</h3>




<p>使用scikit-learn库来划分数据集</p>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cross_validation</span> <span class="k">as</span> <span class="n">cv</span>
</span><span class='line'><span class="n">X_train</span><span class="p">,</span><span class="n">x_test</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>




<h3 id="基于内存的协同过滤">基于内存的协同过滤</h3>




<p>基于内存的协同过滤方法可以分为两个部分：</p>




<ul>
<li>用户－产品协同过滤，即：基于用户的协同过滤推荐</li>
<li>产品－产品协同过滤，即：基于物品的协同过滤推荐</li>
</ul>




<p>基于用户的协同过滤推荐，将选取一个特定的用户，基于打分的相似性发现类似于该用户的用户，并推荐那些相似用户喜欢的产品。基于物品的协同过滤推荐会选取一个产品，发现喜欢该产品的用户，并找到这些相似用户还喜欢的其它产品。</p>




<p>基于用户的协同过滤推荐：“喜欢这东西的人也喜欢……” <br>
基于物品的协同过滤推荐：“像你一样的人也喜欢（买个该商品的还买了）……”</p>




<p>在这两种情况下，从整个数据集构建一个用户产品矩阵。</p>




<h4 id="用户产品矩阵的例子">用户产品矩阵的例子：</h4>




<p>计算相似性，并创建一个相似性矩阵。 <br>
通常用于推荐系统中的距离矩阵是余弦相似性，其中，打分被看成n维空间中的向量，而相似性是基于这些向量之间的角度进行计算的。用户a和b的余弦相似性可以用下面的公式进行计算：</p>




<p><script type="math/tex" id="MathJax-Element-1">Similar_u^{cos}(U_a,U_b)=\frac{U_a*U_b}{||U_a|| * ||U_b||} = \frac {\sum x_{a,m}x_{b,m}}{\sqrt {\sum x^2_{a,m}\sum x^2_{b,m} }}</script></p>




<p>同时，物品a和b的相似读也可以写成以上形式。</p>




<p>创建用户产品矩阵，针对测试数据和训练数据，创建两个矩阵：</p>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cross_validation</span> <span class="k">as</span> <span class="n">cv</span>
</span><span class='line'><span class="n">X_train</span><span class="p">,</span><span class="n">x_test</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">train_data_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_users</span><span class="p">,</span><span class="n">n_items</span><span class="p">))</span>
</span><span class='line'><span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">X_train</span><span class="o">.</span><span class="n">itertuples</span><span class="p">():</span>
</span><span class='line'>    <span class="n">train_data_matrix</span><span class="p">[</span><span class="n">line</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">line</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">line</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</span><span class='line'><span class="n">test_data_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_users</span><span class="p">,</span> <span class="n">n_items</span><span class="p">))</span>
</span><span class='line'><span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">x_test</span><span class="o">.</span><span class="n">itertuples</span><span class="p">():</span>
</span><span class='line'>    <span class="n">test_data_matrix</span><span class="p">[</span><span class="n">line</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">line</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">line</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'><span class="c">## 通过余弦相似度计算用户和物品的相似度</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">pairwise_distances</span>
</span><span class='line'><span class="n">user_similarity</span> <span class="o">=</span> <span class="n">pairwise_distances</span><span class="p">(</span><span class="n">train_data_matrix</span><span class="p">,</span> <span class="n">metric</span> <span class="o">=</span> <span class="s">&quot;cosine&quot;</span><span class="p">)</span>
</span><span class='line'><span class="n">item_similarity</span> <span class="o">=</span> <span class="n">pairwise_distances</span><span class="p">(</span><span class="n">train_data_matrix</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">metric</span> <span class="o">=</span> <span class="s">&quot;cosine&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>




<h4 id="评估">评估</h4>




<p>这里采用均方根误差(RMSE)来度量预测评分的准确性,可以使用sklearn的mean_square_error(MSE)函数，其中RMSE仅仅是MSE的平方根。</p>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sqrt</span>
</span><span class='line'><span class="k">def</span> <span class="nf">rmse</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">):</span>
</span><span class='line'>    <span class="n">prediction</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="n">ground_truth</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span><span class='line'>    <span class="n">ground_truth</span> <span class="o">=</span> <span class="n">ground_truth</span><span class="p">[</span><span class="n">ground_truth</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="k">print</span> <span class="p">(</span><span class="s">&#39;User based CF RMSE: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rmse</span><span class="p">(</span><span class="n">user_prediction</span><span class="p">,</span> <span class="n">test_data_matrix</span><span class="p">)))</span>
</span><span class='line'><span class="k">print</span> <span class="p">(</span><span class="s">&#39;Item based CF RMSe: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rmse</span><span class="p">(</span><span class="n">item_prediction</span><span class="p">,</span> <span class="n">test_data_matrix</span><span class="p">)))</span>
</span><span class='line'><span class="k">print</span> <span class="p">(</span><span class="s">&#39;User based CF RMSE: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rmse</span><span class="p">(</span><span class="n">user_prediction</span><span class="p">,</span> <span class="n">test_data_matrix</span><span class="p">)))</span>
</span><span class='line'><span class="k">print</span> <span class="p">(</span><span class="s">&#39;Item based CF RMSe: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rmse</span><span class="p">(</span><span class="n">item_prediction</span><span class="p">,</span> <span class="n">test_data_matrix</span><span class="p">)))</span>
</span></code></pre></td></tr></table></div></figure>




<blockquote>
  <p>输出结果： <br>
  User based CF RMSE: 3.087357412872858 <br>
  Item based CF RMSe: 3.437038163412728</p>
</blockquote>




<p>但从结果来看，算法的推荐效果还算理想。</p>




<h3 id="基于模型的协同过滤">基于模型的协同过滤</h3>




<p>基于模型的协同过滤是基于<strong>矩阵分解(MF)的</strong>，矩阵分解广泛应用于推荐系统中，它比基于内存的CF有更好的扩展性和稀疏性。MF的目标是从已知的评分中学习用户的潜在喜好和产品的潜在属性，随后通过用户和产品的潜在特征的点积来预测未知的评分。 <br>
这是一个典型的机器学习的问题，可以将已有的用户喜好信息作为训练样本，训练出一个预测用户喜好的模型，这样以后用户在进入系统，可以基于此模型计算推荐。这种方法的问题在于如何将用户实时或者近期的喜好信息反馈给训练好的模型，从而提高推荐的准确度。</p>




<h4 id="svd">SVD</h4>




<p>SVD即：<strong>奇异值分解（Singular value decomposition）</strong>奇异值分解是线性代数中一种重要的矩阵分解，在信号处理、统计学等领域有重要应用。奇异值分解在某些方面与对称矩阵或Hermite矩阵基于特征向量的对角化类似。然而这两种矩阵分解尽管有其相关性，但还是有明显的不同。对称阵特征向量分解的基础是谱分析，而奇异值分解则是谱分析理论在任意矩阵上的推广。</p>




<p>在矩阵M的奇异值分解中</p>




<p><script type="math/tex" id="MathJax-Element-2">M=UΣV^T</script> </p>




<ul>
<li><strong>U的列(columns)</strong>组成一套对M的正交”输入”或”分析”的基向量。这些向量是MM*的特征向量。</li>
<li><strong>V的列(columns)</strong>组成一套对M的正交”输出”的基向量。这些向量是M*M的特征向量。</li>
<li><strong>Σ对角线上的元素</strong>是奇异值，可视为是在输入与输出间进行的标量的”膨胀控制”。这些是M*M及MM*的奇异值，并与U和V的列向量相对应。</li>
</ul>




<p>那么矩阵M就可以分解成U，Σ，V。 <br>
U矩阵表示对应于隐藏特性空间中的用户的特性矩阵，而V矩阵表示对应于隐藏特性空间中的产品的特性矩阵。</p>




<p>现在，我们可以通过U，Σ和<script type="math/tex" id="MathJax-Element-3">V^T</script>的点积进行预测了。</p>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># 计算矩阵的稀疏度</span>
</span><span class='line'><span class="n">sparsity</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">n_users</span><span class="o">*</span><span class="n">n_items</span><span class="p">),</span><span class="mi">3</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="s">&#39;The sparsity level of MovieLen100K is &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sparsity</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#39;%&#39;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="kn">as</span> <span class="nn">sp</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">scipy.sparse.linalg</span> <span class="kn">import</span> <span class="n">svds</span>
</span><span class='line'><span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">vt</span> <span class="o">=</span> <span class="n">svds</span><span class="p">(</span><span class="n">train_data_matrix</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>
</span><span class='line'><span class="n">s_diag_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</span><span class='line'><span class="n">x_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">s_diag_matrix</span><span class="p">),</span><span class="n">vt</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="s">&#39;User-based CF MSE: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rmse</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">test_data_matrix</span><span class="p">)))</span>
</span></code></pre></td></tr></table></div></figure>




<blockquote>
  <p>输出结果： <br>
  The sparsity level of MovieLen100K is 93.7% <br>
  User-based CF MSE: 2.63901831155016</p>
</blockquote>




<p>最后，我们可以发现，采用SVD分解矩阵的基于模型的协同过滤方法在推荐的表现上更胜一筹。</p>

</div>
  
  


      <footer>
      
      - <a href="/blog/2017/09/24/python-shi-xian-tui-jian-xi-tong/">Python 实现推荐系统</a>
      <time datetime="2017-09-24T10:51:37+08:00" pubdate><span class='month'>Sep</span> <span class='day'>24</span> <span class='year'>2017</span></time>
      
      <span class="categories">posted in <a class='category' href='/blog/categories/ji-qi-xue-xi/'>机器学习</a></span>
      
      </footer>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/09/19/aprioriyu-guan-lian-fen-xi/">Apriori与关联分析</a></h1>
    
    
      <div class="post-meta">
        <p class="meta">
          <span class="timestamp">- 








  



<time datetime="2017-09-19T13:33:11+08:00" pubdate data-updated="true"></time> -</span>
          
        </p>
      </div>
    
  </header>


  <div class="entry-content"><h1 id="关联分析">关联分析</h1>




<h2 id="相关概念">相关概念</h2>




<p>作为经典的机器学习算法来说，关联分析并不算复杂。简单来说，从大规模数据集中寻找物品间的隐含关系被称作<strong>关联分析(association analysis)</strong>或者<strong>关联规则学习(association rule learning)</strong>。而其中关联分析的关键就是在众多的物品组成的<strong>项集(itemset)</strong>中找到那些经常一起出现的物品集合，也就是我们所谓的<strong>频繁项集</strong>。由于寻找物品的不同组合是一项非常耗时的任务，所需的计算代价也非常的高，这样显然用蛮力的搜索方法是不可取的。为了解决这样的问题，我们需要用到<strong>Apriori算法</strong>来解决。</p>




<h4 id="有关关联分析的一些应用">有关关联分析的一些应用</h4>




<ol>
<li>购物篮分析，这是关联分析最经典的一项应用。我们耳熟能详的<a href="http://www.jianshu.com/p/a22890153f93"><em>尿布与啤酒</em></a>就是来源于此（虽然这个故事的真实性有待考据，但这并不影响它成为一个营销界的小故事）。</li>
<li>公众热点发现</li>
<li>新闻及流行趋势挖掘</li>
<li>搜索引擎推荐</li>
<li>发现毒蘑菇的相似特征（该例子取自《机器学习实战》一书）</li>
</ol>




<h2 id="apriori算法">Apriori算法</h2>




<p>优点：易编码实现 <br>
缺点：在大数据集上计算效果较慢</p>




<p>既然关联分析是一种在大规模数据集中寻找有趣的关系的任务。这些关系可以有两种形式：一种就是我们上面所说的<strong>频繁项集(frequent itemsets)</strong>;另外一种则是<strong>关联规则(association rules)</strong>。关联规则，其暗示了两种物品之间可能存在很强的关系。</p>




<p>那么我们该如何定义有趣的关系呢？以及，频繁项集中的频繁有怎么划定呢？虽然，许多概念都可以说明这些问题，但是最重要及最通用的莫过于其<strong>支持度</strong>和<strong>可信度</strong>。</p>




<p><strong>支持度(support)：</strong>一个项集的支持度被定义为数据集包含该项集的记录比例。</p>




<blockquote>
  <p>举个例子: <br>
<img src="https://i.loli.net/2017/09/30/59ce75c2b9080.jpg" alt="apriori.jpg" title="apriori.jpg" /><br>
  如上图所示，因为5项记录中有4条包含了{豆奶}，所以其支持度为4/5;依此类推，{豆奶，尿布}的支持读为3/5。</p>
</blockquote>




<p><strong>可信度或置信度(confidence)：</strong>这个概念是针对一条{尿布} -&gt; {葡萄酒}这样的关联规则来定义的。</p>




<blockquote>
  <p><script type="math/tex" id="MathJax-Element-1">置信度({尿布} -> {葡萄酒} )= 支持度({尿布,葡萄酒}) / 支持度({尿布})</script></p>
</blockquote>




<p>支持度和可信度是用来量化关联分析是否成功的方法。</p>




<h4 id="apriori原理">Apriori原理</h4>




<p>如果计算一个集合中的频繁项集的支持度，首先需要遍历全部可能的项集，比如针对一个包含了4个产品的集合，那么购买该集合产品的可能集合数目为2^4-1=15，而针对包含N项的集合，就需要遍历2^N-1。显然，这样的计算量很大。为了降低所需计算时间，就需要我们所谓的Apriori原理。</p>




<p><strong>其基本思路：</strong>如果某个项集是频繁的，那么它的所有子集也是频繁的。该定理的逆反定理为：如果某一个项集是非频繁的，那么它的所有超集（包含该集合的集合）也是非频繁的。Apriori原理的出现，可以在得知某些项集是非频繁之后，不需要计算该集合的超集，有效地避免项集数目的指数增长，从而在合理时间内计算出频繁项集。</p>




<h4 id="算法基本流程">算法基本流程</h4>




<p>当集合中项的个数大于0时： <br>
    创建一个长度为k的候选项集列表 <br>
    检查数据以确认每个项集都是频繁的 <br>
    保留频繁项集，并构建k+1项组成的候选集列表</p>




<h4 id="代码实现">代码实现</h4>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">load_dataset</span><span class="p">():</span>
</span><span class='line'>    <span class="s">&quot;Load the sample dataset.&quot;</span>
</span><span class='line'>    <span class="k">return</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]</span>
</span><span class='line'><span class="k">def</span> <span class="nf">createC1</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
</span><span class='line'>    <span class="s">&quot;Create a list of candidate item sets of size one.&quot;</span>
</span><span class='line'>    <span class="n">c1</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">transaction</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
</span><span class='line'>        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">transaction</span><span class="p">:</span>
</span><span class='line'>            <span class="k">if</span> <span class="ow">not</span> <span class="p">[</span><span class="n">item</span><span class="p">]</span> <span class="ow">in</span> <span class="n">c1</span><span class="p">:</span>
</span><span class='line'>                <span class="n">c1</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">item</span><span class="p">])</span>
</span><span class='line'>    <span class="n">c1</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
</span><span class='line'>    <span class="c">#frozenset because it will be a ket of a dictionary.</span>
</span><span class='line'>    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">frozenset</span><span class="p">,</span> <span class="n">c1</span><span class="p">))</span>
</span><span class='line'><span class="k">def</span> <span class="nf">scanD</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">candidates</span><span class="p">,</span> <span class="n">min_support</span><span class="p">):</span>
</span><span class='line'>    <span class="s">&quot;Returns all candidates that meets a minimum support level&quot;</span>
</span><span class='line'>    <span class="n">sscnt</span> <span class="o">=</span> <span class="p">{}</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">tid</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
</span><span class='line'>        <span class="c">#print(tid)</span>
</span><span class='line'>        <span class="k">for</span> <span class="n">can</span> <span class="ow">in</span> <span class="n">candidates</span><span class="p">:</span>
</span><span class='line'>            <span class="k">if</span> <span class="n">can</span><span class="o">.</span><span class="n">issubset</span><span class="p">(</span><span class="n">tid</span><span class="p">):</span>
</span><span class='line'>                <span class="n">sscnt</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">can</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span class='line'>                <span class="n">sscnt</span><span class="p">[</span><span class="n">can</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">num_items</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
</span><span class='line'>    <span class="n">retlist</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'>    <span class="n">support_data</span> <span class="o">=</span> <span class="p">{}</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">sscnt</span><span class="p">:</span>
</span><span class='line'>        <span class="n">support</span> <span class="o">=</span> <span class="n">sscnt</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="n">num_items</span>
</span><span class='line'>        <span class="k">if</span> <span class="n">support</span> <span class="o">&gt;=</span> <span class="n">min_support</span><span class="p">:</span>
</span><span class='line'>            <span class="n">retlist</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
</span><span class='line'>        <span class="n">support_data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">support</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">retlist</span><span class="p">,</span> <span class="n">support_data</span>
</span><span class='line'><span class="k">def</span> <span class="nf">aprioriGen</span><span class="p">(</span><span class="n">freq_sets</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
</span><span class='line'>    <span class="s">&quot;Generate the joint transactions from candidate sets&quot;</span>
</span><span class='line'>    <span class="n">retList</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'>    <span class="n">lenLk</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">freq_sets</span><span class="p">)</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">lenLk</span><span class="p">):</span>
</span><span class='line'>        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">lenLk</span><span class="p">):</span>
</span><span class='line'>            <span class="n">L1</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">freq_sets</span><span class="p">[</span><span class="n">i</span><span class="p">])[:</span><span class="n">k</span> <span class="o">-</span> <span class="mi">2</span><span class="p">]</span>
</span><span class='line'>            <span class="n">L2</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">freq_sets</span><span class="p">[</span><span class="n">j</span><span class="p">])[:</span><span class="n">k</span> <span class="o">-</span> <span class="mi">2</span><span class="p">]</span>
</span><span class='line'>            <span class="n">L1</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
</span><span class='line'>            <span class="n">L2</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
</span><span class='line'>            <span class="k">if</span> <span class="n">L1</span> <span class="o">==</span> <span class="n">L2</span><span class="p">:</span>
</span><span class='line'>                <span class="n">retList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">freq_sets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">|</span> <span class="n">freq_sets</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">retList</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">apriori</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">minsupport</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
</span><span class='line'>    <span class="s">&quot;Generate a list of candidate item sets&quot;</span>
</span><span class='line'>    <span class="n">C1</span> <span class="o">=</span> <span class="n">createC1</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</span><span class='line'>    <span class="n">D</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">set</span><span class="p">,</span> <span class="n">dataset</span><span class="p">))</span>
</span><span class='line'>    <span class="n">L1</span><span class="p">,</span> <span class="n">support_data</span> <span class="o">=</span> <span class="n">scanD</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">C1</span><span class="p">,</span> <span class="n">minsupport</span><span class="p">)</span>
</span><span class='line'>    <span class="n">L</span> <span class="o">=</span> <span class="p">[</span><span class="n">L1</span><span class="p">]</span>
</span><span class='line'>    <span class="n">k</span> <span class="o">=</span> <span class="mi">2</span>
</span><span class='line'>    <span class="k">while</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="n">k</span> <span class="o">-</span> <span class="mi">2</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
</span><span class='line'>        <span class="n">Ck</span> <span class="o">=</span> <span class="n">aprioriGen</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="n">k</span> <span class="o">-</span> <span class="mi">2</span><span class="p">],</span> <span class="n">k</span><span class="p">)</span>
</span><span class='line'>        <span class="n">Lk</span><span class="p">,</span> <span class="n">supK</span> <span class="o">=</span> <span class="n">scanD</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">Ck</span><span class="p">,</span> <span class="n">minsupport</span><span class="p">)</span>
</span><span class='line'>        <span class="n">support_data</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">supK</span><span class="p">)</span>
</span><span class='line'>        <span class="n">L</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Lk</span><span class="p">)</span>
</span><span class='line'>        <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">L</span><span class="p">,</span> <span class="n">support_data</span>
</span></code></pre></td></tr></table></div></figure>


<h4 id="挖掘关联规则">挖掘关联规则</h4>




<p>上面，我们实现了用apriori算法找出了频繁项集。现在我们来尝试找出关联规则。那么什么是关联规则呢？我们可以举这样一个例子：假设我们有一个频繁项集是：{豆奶，莴苣}。那么则意味着一个人如果他已经购买了豆奶，那么他也有很大的几率会购买莴苣;但反过来却不一定成立：则如果一个人购买了莴苣，但还是不能说明他有很大几率会接着购买豆奶。</p>




<p>既然，在上面的例子中我们已经用支持度作为一种参考单位去量化一个集合的频繁关系，在这里我们同理也可以通过另一个概念来度量关联规则。没错，它就是我们已经在上面提过的<strong>可信度</strong>。根据可信度的计算法则和定义可知：</p>




<blockquote>
  <p><script type="math/tex" id="MathJax-Element-2">置信度({尿布} -> {葡萄酒} )= 支持度({尿布,葡萄酒}) / 支持度({尿布})</script></p>
</blockquote>




<p>这也是我们上面已经提及过的问题。加上，在上面的例子之中我们已经得到了所有的频繁项集的支持度，那么为了求出各关联规则的可信度也就是剩下一个除法的问题而已。</p>




<h4 id="如何从频繁项集中生成关联规则">如何从频繁项集中生成关联规则</h4>




<p>一般对于一个频繁项集而言，如果我们直接生成关联规则的话，往往会有很多条。例如：我们对一个频繁项集{0，1，2，3}直接生成关联规则的话结果就如下图所示一样： <br>
<img src="https://i.loli.net/2017/09/30/59ce75dc70dd2.png" alt="assrule.png" title="assrule.png" /><br>
从上图我们就可以看出仅仅含4项的频繁项集就会生成出如此多的关联规则，那么如果频繁项集中含有10，20，甚至100个元素那么生成的候选关联规则的数据是相当庞大。</p>




<p>在此，我们可以参考apriori算法中挖掘频繁项集的思路。在发现频繁项集时我们通过最低支持度的方法来忽略掉一些项集支持度的计算;同理，在这里我们将这个思想应用在关联规则的生成之上，即：如果一条规则的不满足最低可信度的要求，那么其子集自然也不能满足最低可信度的要求。对于这部分规则的可信度计算我们可以直接忽略。如上图中的阴影部分。我们假设规则{0，1，2} -&gt; 3不能满足最低可信度，那么，他的子集{1，2}-&gt;{0，3}，{0，2}-&gt;{1，3}等将不能满足最低可信度。</p>




<h4 id="代码实现-1">代码实现</h4>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">generateRules</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">support_data</span><span class="p">,</span> <span class="n">min_confidence</span><span class="o">=</span><span class="mf">0.7</span><span class="p">):</span>
</span><span class='line'>    <span class="sd">&quot;&quot;&quot;Create the association rules</span>
</span><span class='line'><span class="sd">    L: list of frequent item sets</span>
</span><span class='line'><span class="sd">    support_data: support data for those itemsets</span>
</span><span class='line'><span class="sd">    min_confidence: minimum confidence threshold</span>
</span><span class='line'><span class="sd">    &quot;&quot;&quot;</span>
</span><span class='line'>    <span class="n">rules</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">L</span><span class="p">)):</span>
</span><span class='line'>        <span class="k">for</span> <span class="n">freqSet</span> <span class="ow">in</span> <span class="n">L</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
</span><span class='line'>            <span class="n">H1</span> <span class="o">=</span> <span class="p">[</span><span class="nb">frozenset</span><span class="p">([</span><span class="n">item</span><span class="p">])</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">freqSet</span><span class="p">]</span>
</span><span class='line'>            <span class="k">print</span><span class="p">(</span><span class="s">&quot;freqSet&quot;</span><span class="p">,</span> <span class="n">freqSet</span><span class="p">,</span> <span class="s">&#39;H1&#39;</span><span class="p">,</span> <span class="n">H1</span><span class="p">)</span>
</span><span class='line'>            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
</span><span class='line'>                <span class="n">rules_from_conseq</span><span class="p">(</span><span class="n">freqSet</span><span class="p">,</span> <span class="n">H1</span><span class="p">,</span> <span class="n">support_data</span><span class="p">,</span> <span class="n">rules</span><span class="p">,</span> <span class="n">min_confidence</span><span class="p">)</span>
</span><span class='line'>            <span class="k">else</span><span class="p">:</span>
</span><span class='line'>                <span class="n">calc_confidence</span><span class="p">(</span><span class="n">freqSet</span><span class="p">,</span> <span class="n">H1</span><span class="p">,</span> <span class="n">support_data</span><span class="p">,</span> <span class="n">rules</span><span class="p">,</span> <span class="n">min_confidence</span><span class="p">)</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">rules</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">calc_confidence</span><span class="p">(</span><span class="n">freqSet</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">support_data</span><span class="p">,</span> <span class="n">rules</span><span class="p">,</span> <span class="n">min_confidence</span><span class="o">=</span><span class="mf">0.7</span><span class="p">):</span>
</span><span class='line'>    <span class="s">&quot;Evaluate the rule generated&quot;</span>
</span><span class='line'>    <span class="n">pruned_H</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">conseq</span> <span class="ow">in</span> <span class="n">H</span><span class="p">:</span>
</span><span class='line'>        <span class="n">conf</span> <span class="o">=</span> <span class="n">support_data</span><span class="p">[</span><span class="n">freqSet</span><span class="p">]</span> <span class="o">/</span> <span class="n">support_data</span><span class="p">[</span><span class="n">freqSet</span> <span class="o">-</span> <span class="n">conseq</span><span class="p">]</span>
</span><span class='line'>        <span class="k">if</span> <span class="n">conf</span> <span class="o">&gt;=</span> <span class="n">min_confidence</span><span class="p">:</span>
</span><span class='line'>            <span class="k">print</span><span class="p">(</span><span class="n">freqSet</span> <span class="o">-</span> <span class="n">conseq</span><span class="p">,</span> <span class="s">&#39;---&gt;&#39;</span><span class="p">,</span> <span class="n">conseq</span><span class="p">,</span> <span class="s">&#39;conf:&#39;</span><span class="p">,</span> <span class="n">conf</span><span class="p">)</span>
</span><span class='line'>            <span class="n">rules</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">freqSet</span> <span class="o">-</span> <span class="n">conseq</span><span class="p">,</span> <span class="n">conseq</span><span class="p">,</span> <span class="n">conf</span><span class="p">))</span>
</span><span class='line'>            <span class="n">pruned_H</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">conseq</span><span class="p">)</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">pruned_H</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">rules_from_conseq</span><span class="p">(</span><span class="n">freqSet</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">support_data</span><span class="p">,</span> <span class="n">rules</span><span class="p">,</span> <span class="n">min_confidence</span><span class="o">=</span><span class="mf">0.7</span><span class="p">):</span>
</span><span class='line'>    <span class="s">&quot;Generate a set of candidate rules&quot;</span>
</span><span class='line'>    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">H</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span class='line'>    <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">freqSet</span><span class="p">)</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
</span><span class='line'>        <span class="n">Hmp1</span> <span class="o">=</span> <span class="n">aprioriGen</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span><span class='line'>        <span class="n">Hmp1</span> <span class="o">=</span> <span class="n">calc_confidence</span><span class="p">(</span><span class="n">freqSet</span><span class="p">,</span> <span class="n">Hmp1</span><span class="p">,</span>  <span class="n">support_data</span><span class="p">,</span> <span class="n">rules</span><span class="p">,</span> <span class="n">min_confidence</span><span class="p">)</span>
</span><span class='line'>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">Hmp1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span class='line'>            <span class="n">rules_from_conseq</span><span class="p">(</span><span class="n">freqSet</span><span class="p">,</span> <span class="n">Hmp1</span><span class="p">,</span> <span class="n">support_data</span><span class="p">,</span> <span class="n">rules</span><span class="p">,</span> <span class="n">min_confidence</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>




<blockquote>
  <p>运行测试</p>
</blockquote>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">()</span>
</span><span class='line'><span class="n">L</span><span class="p">,</span><span class="n">support_data</span> <span class="o">=</span> <span class="n">apriori</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</span><span class='line'><span class="n">generateRules</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">support_data</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>




<blockquote>
  <p>输出结果 <br>
  freqSet frozenset({3, 5}) H1 [frozenset({3}), frozenset({5})] <br>
  freqSet frozenset({1, 3}) H1 [frozenset({1}), frozenset({3})] <br>
  frozenset({1}) —&gt; frozenset({3}) conf: 1.0 <br>
  freqSet frozenset({2, 5}) H1 [frozenset({2}), frozenset({5})] <br>
  frozenset({5}) —&gt; frozenset({2}) conf: 1.0 <br>
  frozenset({2}) —&gt; frozenset({5}) conf: 1.0 <br>
  freqSet frozenset({2, 3}) H1 [frozenset({2}), frozenset({3})] <br>
  freqSet frozenset({2, 3, 5}) H1 [frozenset({2}), frozenset({3}), frozenset({5})] <br>
  Out[6]: <br>
  [(frozenset({1}), frozenset({3}), 1.0), <br>
   (frozenset({5}), frozenset({2}), 1.0), <br>
   (frozenset({2}), frozenset({5}), 1.0)]</p>
</blockquote>




<p>从输出结果可以看出，其中{1} —&gt; {3}，{5} —&gt; {2}，{2} —&gt; {5}这三条关联规则的可信度居然达到了1.0，而且{2}，{5}还存在一个相互关联的关系。除此之外，我们还可以发现：虽然{1} —&gt; {3}这条关联的可信度达到了1.0，但是{3} —&gt; {1}却不足0.7（因为我们的最低可信度标准为0.7，没有输出显示的规则，则意味着它们的可信度低于0.7)。为了可以看见输出更多的规则，我们可以降低下最低可信度的标准，再来执行一遍代码。</p>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">()</span>
</span><span class='line'><span class="n">L</span><span class="p">,</span><span class="n">support_data</span> <span class="o">=</span> <span class="n">apriori</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</span><span class='line'><span class="n">generateRules</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">support_data</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>




<blockquote>
  <p>输出结果 <br>
  freqSet frozenset({3, 5}) H1 [frozenset({3}), frozenset({5})] <br>
  frozenset({5}) —&gt; frozenset({3}) conf: 0.6666666666666666 <br>
  frozenset({3}) —&gt; frozenset({5}) conf: 0.6666666666666666 <br>
  freqSet frozenset({1, 3}) H1 [frozenset({1}), frozenset({3})] <br>
  frozenset({3}) —&gt; frozenset({1}) conf: 0.6666666666666666 <br>
  frozenset({1}) —&gt; frozenset({3}) conf: 1.0 <br>
  freqSet frozenset({2, 5}) H1 [frozenset({2}), frozenset({5})] <br>
  frozenset({5}) —&gt; frozenset({2}) conf: 1.0 <br>
  frozenset({2}) —&gt; frozenset({5}) conf: 1.0 <br>
  freqSet frozenset({2, 3}) H1 [frozenset({2}), frozenset({3})] <br>
  frozenset({3}) —&gt; frozenset({2}) conf: 0.6666666666666666 <br>
  frozenset({2}) —&gt; frozenset({3}) conf: 0.6666666666666666 <br>
  freqSet frozenset({2, 3, 5}) H1 [frozenset({2}), frozenset({3}), frozenset({5})] <br>
  frozenset({5}) —&gt; frozenset({2, 3}) conf: 0.6666666666666666 <br>
  frozenset({3}) —&gt; frozenset({2, 5}) conf: 0.6666666666666666 <br>
  frozenset({2}) —&gt; frozenset({3, 5}) conf: 0.6666666666666666 <br>
  Out[7]: <br>
  [(frozenset({5}), frozenset({3}), 0.6666666666666666), <br>
   (frozenset({3}), frozenset({5}), 0.6666666666666666), <br>
   (frozenset({3}), frozenset({1}), 0.6666666666666666), <br>
   (frozenset({1}), frozenset({3}), 1.0), <br>
   (frozenset({5}), frozenset({2}), 1.0), <br>
   (frozenset({2}), frozenset({5}), 1.0), <br>
   (frozenset({3}), frozenset({2}), 0.6666666666666666), <br>
   (frozenset({2}), frozenset({3}), 0.6666666666666666), <br>
   (frozenset({5}), frozenset({2, 3}), 0.6666666666666666), <br>
   (frozenset({3}), frozenset({2, 5}), 0.6666666666666666), <br>
   (frozenset({2}), frozenset({3, 5}), 0.6666666666666666)]</p>
</blockquote>




<p>这次我们就可以在输出中看见{3} —&gt; {1}了，它的可信度为0.6666666666666666。</p>




<p>到此，有关关联分析的apriori算法就告一段落了。后续还会有有关关联分析FP-growth算法的文章讲解。</p>

</div>
  
  


      <footer>
      
      - <a href="/blog/2017/09/19/aprioriyu-guan-lian-fen-xi/">Apriori与关联分析</a>
      <time datetime="2017-09-19T13:33:11+08:00" pubdate><span class='month'>Sep</span> <span class='day'>19</span> <span class='year'>2017</span></time>
      
      <span class="categories">posted in <a class='category' href='/blog/categories/ji-qi-xue-xi/'>机器学习</a></span>
      
      </footer>
    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/9">&larr; Older</a>
    
    <a href="/archives">Blog Archives</a>
    
    <a class="next" href="/posts/7">Newer &rarr;</a>
    
  </div><!-- /div.pagination -->
</div><!-- /div.blog-index -->

      </div><!-- /div#content -->
    </div><!-- /div#main -->
  </div><!-- /div.container -->
  <footer><div id="footer-widgets-wrapper">
  <div id="footer-first" class="footer-widget">
    <h3>About Me</h3>
    <section class="about-me">
      
        <img class="icon-image" src="https://avatars0.githubusercontent.com/u/13914416?s=240" alt="icon_image">
      
      <div>
        <ul>
          
            <li>GitHub: <a href="https://github.com/EdmondFrank">@EdmondFrank</a></li>
          
          
            <li>Twitter: <a href="https://twitter.com/EdmondFrank4">@EdmondFrank4</a></li>
          
            <li>Blog: <a href="https://edmondfrank.github.io">https://edmondfrank.github.io</a></li>
        </ul>
        <p>
          この町、冗談と気まぐれと偶然でてきっているらしい。
        </p>
      </div>
    </section>
  </div><!-- /div#footer-second -->

  <div id="footer-second" class="footer-widget">
    <h3>Recent Posts</h3>
    <section id="hatena-popular" class="hatena-bookmark">
      <script language="javascript" type="text/javascript" src="https://b.hatena.ne.jp/js/widget.js" charset="utf-8"></script>
      <script language="javascript" type="text/javascript">
        Hatena.BookmarkWidget.url   = "https://edmondfrank.github.io";
        Hatena.BookmarkWidget.title = "Recent Posts";
        Hatena.BookmarkWidget.sort  = "hot";
        Hatena.BookmarkWidget.width = 0;
        Hatena.BookmarkWidget.num   = 10;
        Hatena.BookmarkWidget.theme = "notheme";
        Hatena.BookmarkWidget.load();
      </script>
    </section>
  </div><!-- /div#footer-second -->

  <div id="footer-third" class="footer-widget">
    <h3>Popular Posts</h3>
    <section id="hatena-popular" class="hatena-bookmark">
      <script language="javascript" type="text/javascript" src="https://b.hatena.ne.jp/js/widget.js" charset="utf-8"></script>
      <script language="javascript" type="text/javascript">
        Hatena.BookmarkWidget.url   = "https://edmondfrank.github.io";
        Hatena.BookmarkWidget.title = "Popular Posts";
        Hatena.BookmarkWidget.sort  = "count";
        Hatena.BookmarkWidget.width = 0;
        Hatena.BookmarkWidget.num   = 10;
        Hatena.BookmarkWidget.theme = "notheme";
        Hatena.BookmarkWidget.load();
      </script>
    </section>
  </div><!-- /div#footer-third -->
</div><!-- /div#footer-widgets-wrapper -->

<div id="credit" role="contentinfo">
  <p>
    Copyright &copy; 2022 - <a href="https://github.com/EdmondFrank/">EdmondFrank</a> -
    <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  </p>
</div>

</footer>
  










  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'https://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
