<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 神经网络与深度学习 | EdmondFrank's 时光足迹]]></title>
  <link href="https://edmondfrank.github.io/blog/categories/shen-jing-wang-luo-yu-shen-du-xue-xi/atom.xml" rel="self"/>
  <link href="https://edmondfrank.github.io/"/>
  <updated>2019-11-30T00:18:19+08:00</updated>
  <id>https://edmondfrank.github.io/</id>
  <author>
    <name><![CDATA[EdmondFrank]]></name>
    <email><![CDATA[EdmomdFrank@yahoo.co.jp]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Python Keras + LSTM 进行单变量时间序列预测]]></title>
    <link href="https://edmondfrank.github.io/blog/2018/02/22/python-keras-plus-lstm-jin-xing-dan-bian-liang-shi-jian-xu-lie-yu-ce/"/>
    <updated>2018-02-22T16:06:57+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2018/02/22/python-keras-plus-lstm-jin-xing-dan-bian-liang-shi-jian-xu-lie-yu-ce</id>
    <content type="html"><![CDATA[<h1>Python Keras + LSTM 进行单变量时间序列预测</h1>

<p>首先，时间序列预测问题是一个复杂的预测模型问题，它不像一般的回归预测模型。时间序列预测的输入变量是一组按时间顺序的数字序列。它既具有延续性又具有随机性，所以在建模难度上相对回归预测更大。</p>

<p>但同时，正好有一种强大的神经网络适合处理这种存在依赖关系的序列问题：RNN（Recurrent neural networks）。在过去几年中，应用 RNN 在语音识别，语言建模，翻译，图片描述等问题上已经取得一定成功，并且应用领域还在扩展。</p>

<h2>LSTM网络</h2>

<p><strong>Long Short-Term Memory 网络</strong>亦称<strong>LSTM 网络</strong>，是一种在深度学习中应用的循环神经网络。可以学习长期依赖信息。LSTM 由Hochreiter &amp; Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力。</p>

<h2>具体应用</h2>

<p>下面以一个洗发水销售的例子，来实现LSTM。
首先，你可以在这里<a href="https://datamarket.com/data/set/22r0/sales-of-shampoo-over-a-three-year-period#!ds=22r0&amp;display=line">下载</a>到本文需要用的数据集。这是一个描述了3年内洗发水的月度销售数量的数据集。</p>

<h3>数据读取</h3>

<pre><code class="python">from pandas import read_csv
from pandas import datetime
from matplotlib import pyplot as plt
def parser(x):
    return datetime.strptime('190'+x,"%Y-%m")
series = read_csv('sales-of-shampoo-over-a-three-ye.csv',
                 header=0,parse_dates=[0],index_col=0, 
                  squeeze=True, date_parser=parser)
print(series.head())
series.plot()
plt.show()
</code></pre>

<h3>数据划分</h3>

<p>首先我们把数据集划分成两个部分即：训练集和测试集。
那么我们该如何划分呢？因为我们今天研究的是时间序列分析，所以在数据集的划分上我们也应该按照时间来划分。我们可以将前两年的数据作为我们的训练集而将最后一年的数据作为测试集。</p>

<pre><code class="python"># split data into train and test
X = series.values
train, test = X[0:-12], X[-12:]
</code></pre>

<p>这里我们假设一个滚动预测的情景，又称<strong>前向模型验证（walk-forward model validation）</strong>。其原理很简单，举例来说就像当公司的预测期长达一年时，预测会将已过去的月份排除，而将预测期末的月份补上。好比一月份过去后，我们将其从预测中移除，同时次年的一月份就会作为收尾被添加到预测中以便预测总能保持12个月的完整性。</p>

<p>这样通过使用每月新的洗发水销售量来进行下个月的预测，我们就像模拟了一个更接近于真实世界的场景。</p>

<p>最后，我们将所有在测试集上的预测结果收集起来并计算出他们与真实值的均方根误差（RMSE）以此来作为评估我们模型的基准。</p>

<h3>持续模型预测(Persistence Model Forecast)</h3>

<p>持续性预测的基本思路就是从先前的（t-1）时间序列的结果用于预测当前时间（t）的取值。
那么根据以上的思路，我们可以通过滚动预测的原理从训练集的历史数据中获取最后一次观察值并使用它来预测当前时间的可能取值。</p>

<pre><code class="python">from pandas import read_csv
from pandas import datetime
from sklearn.metrics import mean_squared_error
from math import sqrt
from matplotlib import pyplot
# load dataset
def parser(x):
    return datetime.strptime('190'+x, '%Y-%m')
series = read_csv('sales-of-shampoo-over-a-three-ye.csv', 
                  header=0, parse_dates=[0], index_col=0, 
                  squeeze=True, date_parser=parser)
# split data into train and test
X = series.values
train, test = X[0:-12], X[-12:]
# walk-forward validation
history = [x for x in train]
predictions = list()
for i in range(len(test)):
    # make prediction
    predictions.append(history[-1])
    # observation
    history.append(test[i])
# report performance
rmse = sqrt(mean_squared_error(test, predictions))
print('RMSE: %.3f' % rmse)
# line plot of observed vs predicted
pyplot.plot(test)
pyplot.plot(predictions)
pyplot.show()
</code></pre>

<p><img src="https://i.loli.net/2017/09/07/59b1503007a96.png" alt="persistence_rmse.png" /></p>

<p>通过持续模型的预测，我们得到了一个最基础的预测模型以及RMSE（baseline）为了提升我们预测模型的效果，下面让我们进入正题来构建LSTM模型来对数据集进行时间序列预测。</p>

<h3>数据处理</h3>

<p>为了能够构建一个LSTM模型对训练集进行训练，我们首先要对数据进行一下处理：</p>

<ol>
<li>将时间序列问题转化成监督学习问题</li>
<li>平稳时间序列</li>
<li>数据标准化</li>
</ol>


<h4>将时间序列转换成监督学习</h4>

<p>对于一个时间序列问题，我们可以通过使用从最后一个（t-1）时刻的观测值作为输入的特征X和当前时刻（t）的观测值作为输出Y来实现转换。</p>

<p>因为，需要转换的是一组时间序列数据，所以无法组合成像真正的监督学习那样有明确一对一映射的输入输出关系。尤其是在数据集的最开始或最后时，两个位置总有一个位置无法在训练集中找到对应关系。为了解决这样的问题，我们通常的做法是，在最开始时将输入特征置为0，而它对应的输出就是时间序列的第一个元素。</p>

<pre><code class="python">from pandas import read_csv
from pandas import datetime
from pandas import DataFrame
from pandas import concat

# frame a sequence as a supervised learning problem
def timeseries_to_supervised(data, lag=1):
    df = DataFrame(data)
    columns = [df.shift(i) for i in range(1, lag+1)]
    columns.append(df)
    df = concat(columns, axis=1)
    df.fillna(0, inplace=True)
    return df

# load dataset
def parser(x):
    return datetime.strptime('190'+x, '%Y-%m')
series = read_csv('sales-of-shampoo-over-a-three-ye.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)
# transform to supervised learning
X = series.values
supervised = timeseries_to_supervised(X, 1)
print(supervised)
</code></pre>

<blockquote><p>输出结果：
        0      0
0     0.0  266.0
1   266.0  145.9
2   145.9  183.1
3   183.1  119.3
4   119.3  180.3
5   180.3  168.5
6   168.5  231.8
7   231.8  224.5
8   224.5  192.8
9   192.8  122.9
10  122.9  336.5
11  336.5  185.9
12  185.9  194.3
13  194.3  149.5
14  149.5  210.1
15  210.1  273.3
16  273.3  191.4
17  191.4  287.0
18  287.0  226.0
19  226.0  303.6
20  303.6  289.9
21  289.9  421.6
22  421.6  264.5
23  264.5  342.3
24  342.3  339.7
25  339.7  440.4
26  440.4  315.9
27  315.9  439.3
28  439.3  401.3
29  401.3  437.4
30  437.4  575.5
31  575.5  407.6
32  407.6  682.0
33  682.0  475.3
34  475.3  581.3
35  581.3  646.9</p></blockquote>

<h4>平稳时间序列</h4>

<p>虽然不明显，但我们仍可以看出这个洗发水销售数据集在时间上呈上升趋势。因此我们说这个时间序列数据是非平稳的。那么，不平稳怎么办？</p>

<p>答案就是：差分。（有关差分的介绍点击<a href="https://zh.wikipedia.org/wiki/%E5%B7%AE%E5%88%86">此处</a>）</p>

<pre><code class="python">from pandas import read_csv
from pandas import datetime
from pandas import Series

# create a differenced series
def difference(dataset, interval=1):
    diff = list()
    for i in range(interval, len(dataset)):
        value = dataset[i] - dataset[i - interval]
        diff.append(value)
    return Series(diff)

# invert differenced value
def inverse_difference(history, yhat, interval=1):
    return yhat + history[-interval]

# load dataset
def parser(x):
    return datetime.strptime('190'+x, '%Y-%m')
series = read_csv('sales-of-shampoo-over-a-three-ye.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)
print(series.head())
# transform to be stationary
differenced = difference(series, 1)
print(differenced.head())
# invert transform
inverted = list()
for i in range(len(differenced)):
    value = inverse_difference(series, differenced[i], len(series)-i)
    inverted.append(value)
inverted = Series(inverted)
print(inverted.head())
differenced.plot()
plt.show()
</code></pre>

<p><img src="https://i.loli.net/2017/09/07/59b162fb84ac0.png" alt="diff.png" /></p>

<p>经过一阶差分处理后，从图上看还是挺平稳的。</p>

<h4>标准化数据</h4>

<p>在数据输入前进行标准化可以非常有效的提升收敛速度和效果。尤其如果我们的激活函数是sigmoid或者tanh，其梯度最大的区间是0附近，当输入值很大或者很小的时候，sigmoid或者tanh的变化就基本平坦了（sigmoid的导数sig（1-sig）会趋于0），也就是进行梯度下降进行优化的时候，梯度会趋于0，而倒是优化速度很慢。</p>

<p>如果输入不进行归一化，由于我们初始化的时候一般都是0均值的的正太分布或者小范围的均匀分布（Xavier），如果输入中存在着尺度相差很大的特征，例如（10000，0.001）这样的，很容易导致激活函数的输入w1<em>x1+w2</em>x2+b变的很大或者很小，从而引起梯度趋于0。</p>

<p>而LSTM的默认激活函数就是tanh函数，它的输出范围在-1 到 1 之间，同时这是时间序列数据的首选范围。因此我们可以使用MinMaxScaler类将数据集转换到范围[-1,1]。像其他scikit用于转换数据的方法类一样，它需要以行和列的矩阵格式提供的数据。因此，在转换之前，我们必须重塑NumPy数组。</p>

<pre><code class="python">from pandas import read_csv
from pandas import datetime
from pandas import Series
from sklearn.preprocessing import MinMaxScaler
# load dataset
def parser(x):
    return datetime.strptime('190'+x, '%Y-%m')
series = read_csv('sales-of-shampoo-over-a-three-ye.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)
print(series.head())
# transform scale
X = series.values
X = X.reshape(len(X), 1)
scaler = MinMaxScaler(feature_range=(-1, 1))
scaler = scaler.fit(X)
scaled_X = scaler.transform(X)
scaled_series = Series(scaled_X[:, 0])
print(scaled_series.head())
# invert transform
inverted_X = scaler.inverse_transform(scaled_X)
inverted_series = Series(inverted_X[:, 0])
print(inverted_series.head())
</code></pre>

<blockquote><p>输出结果：
Month
1901-01-01    266.0
1901-02-01    145.9
1901-03-01    183.1
1901-04-01    119.3
1901-05-01    180.3
Name: Sales of shampoo over a three year period, dtype: float64
0   -0.478585
1   -0.905456
2   -0.773236
3   -1.000000
4   -0.783188
dtype: float64
0    266.0
1    145.9
2    183.1
3    119.3
4    180.3
dtype: float64</p></blockquote>

<h3>构建LSTM模型</h3>

<p>长短期记忆网络（LSTM）是一种递归神经网络（RNN）。
这类网络的的优点是它能学习并记住较长序列，并不依赖预先指定的窗口滞后观察值作为输入。
在Keras中，这被称为stateful，在定义LSTM网络层时将“stateful”语句设定为“True”。</p>

<p>LSTM层要求输入矩阵格式为：[样本，时间步长，特征]</p>

<p>鉴于训练数据集的形式定义为X输入和y输出，必须先将其转化为样本/时间步长/特征的形式。</p>

<h4>完整代码</h4>

<pre><code class="python">from pandas import DataFrame
from pandas import Series
from pandas import concat
from pandas import read_csv
from pandas import datetime
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from math import sqrt
from matplotlib import pyplot
import numpy

# date-time parsing function for loading the dataset
def parser(x):
    return datetime.strptime('190'+x, '%Y-%m')

# frame a sequence as a supervised learning problem
def timeseries_to_supervised(data, lag=1):
    df = DataFrame(data)
    columns = [df.shift(i) for i in range(1, lag+1)]
    columns.append(df)
    df = concat(columns, axis=1)
    df.fillna(0, inplace=True)
    return df

# create a differenced series
def difference(dataset, interval=1):
    diff = list()
    for i in range(interval, len(dataset)):
        value = dataset[i] - dataset[i - interval]
        diff.append(value)
    return Series(diff)

# invert differenced value
def inverse_difference(history, yhat, interval=1):
    return yhat + history[-interval]

# scale train and test data to [-1, 1]
def scale(train, test):
    # fit scaler
    scaler = MinMaxScaler(feature_range=(-1, 1))
    scaler = scaler.fit(train)
    # transform train
    train = train.reshape(train.shape[0], train.shape[1])
    train_scaled = scaler.transform(train)
    # transform test
    test = test.reshape(test.shape[0], test.shape[1])
    test_scaled = scaler.transform(test)
    return scaler, train_scaled, test_scaled

# inverse scaling for a forecasted value
def invert_scale(scaler, X, value):
    new_row = [x for x in X] + [value]
    array = numpy.array(new_row)
    array = array.reshape(1, len(array))
    inverted = scaler.inverse_transform(array)
    return inverted[0, -1]

# fit an LSTM network to training data
def fit_lstm(train, batch_size, nb_epoch, neurons):
    X, y = train[:, 0:-1], train[:, -1]
    X = X.reshape(X.shape[0], 1, X.shape[1])
    model = Sequential()
    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')
    for i in range(nb_epoch):
        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)
        model.reset_states()
    return model

# make a one-step forecast
def forecast_lstm(model, batch_size, X):
    X = X.reshape(1, 1, len(X))
    yhat = model.predict(X, batch_size=batch_size)
    return yhat[0,0]

# load dataset
series = read_csv('sales-of-shampoo-over-a-three-ye.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)

# transform data to be stationary
raw_values = series.values
diff_values = difference(raw_values, 1)

# transform data to be supervised learning
supervised = timeseries_to_supervised(diff_values, 1)
supervised_values = supervised.values

# split data into train and test-sets
train, test = supervised_values[0:-12], supervised_values[-12:]

# transform the scale of the data
scaler, train_scaled, test_scaled = scale(train, test)

# fit the model
lstm_model = fit_lstm(train_scaled, 1, 3000, 4)
# forecast the entire training dataset to build up state for forecasting
train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)
lstm_model.predict(train_reshaped, batch_size=1)

# walk-forward validation on the test data
predictions = list()
for i in range(len(test_scaled)):
    # make one-step forecast
    X, y = test_scaled[i, 0:-1], test_scaled[i, -1]
    yhat = forecast_lstm(lstm_model, 1, X)
    # invert scaling
    yhat = invert_scale(scaler, X, yhat)
    # invert differencing
    yhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)
    # store forecast
    predictions.append(yhat)
    expected = raw_values[len(train) + i + 1]
    print('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))

#report performance
rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))
print('Test RMSE: %.3f' % rmse)
# line plot of observed vs predicted
pyplot.plot(raw_values[-12:])
pyplot.plot(predictions)
pyplot.show()
</code></pre>

<p><img src="https://i.loli.net/2017/09/07/59b16bc803033.png" alt="lstm_pred.png" /></p>

<p>最后运行结果打印出测试数据集12个月份中每个月份的预期和预测销量。示例还打印了所有预测值得均方根误差。该模型显示洗发水月度销量的均方根误差为111.925，好于持续性模型得出的对应结果136.761。</p>

<p><strong>另外</strong>，神经网络的一个难题是初始条件不同，它们给出结果就不同。一种解决办法是修改Keras使用的随机数种子值以确保结果可复制。另一种办法是使用不同的实验设置控制随机初始条件。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[如何免费使用谷歌GPU训练神经网络]]></title>
    <link href="https://edmondfrank.github.io/blog/2018/02/18/ru-he-mian-fei-shi-yong-gu-ge-gpuxun-lian-shen-jing-wang-luo/"/>
    <updated>2018-02-18T21:21:44+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2018/02/18/ru-he-mian-fei-shi-yong-gu-ge-gpuxun-lian-shen-jing-wang-luo</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="完全云端运行免费使用谷歌gpu训练神经网络">完全云端运行：免费使用谷歌GPU训练神经网络</h1></p>

<h2 id="背景">背景</h2>




<p>对，你没有听错，高大上的ＧＰＵ，现在不花钱也能用上了。这是Google的一项免费云端机器学习服务，全名Colaboratory。</p>




<p><strong>Colaboratory</strong> 是一个 Google 研究项目，旨在帮助传播机器学习培训和研究成果。它是一个 Jupyter 笔记本环境，不需要进行任何设置就可以使用，并且完全在云端运行。Colaboratory 笔记本存储在 Google 云端硬盘中，并且可以共享，就如同您使用 Google 文档或表格一样。Colaboratory 可免费使用，而且最重要的还提供免费的英伟达Tesla K80 GPU。还有这等好事？事不宜迟，本文马上介绍如何使用 Google CoLaboratory 训练神经网络。</p>




<h2 id="准备工作">准备工作</h2>




<p><strong>在Google Drive上创建文件夹</strong></p>




<p><img src="https://ws1.sinaimg.cn/large/a3d23450gy1fokw4gt73cj20au0cdmxt.jpg" alt="" title=""></p>




<p>Colab用的数据都存储在Google Drive云端硬盘上，所以，我们需要先指定要在Google Drive上用的文件夹。</p>




<p>比如说，可以在Google Drive上创建一个“app”文件夹，或者其他什么名字，也可以选择Colab笔记本默认的文件夹。</p>




<h2 id="新建colab笔记本">新建Colab笔记本</h2>




<p>在刚刚创建的app文件夹里点击右键，选择<strong>“更多”</strong>，然后从菜单里选择<strong>“Colaboratory”</strong>，这样就新建出了一个Colab笔记本。</p>




<p>若是更多选项中没有<strong>“Colaboratory”</strong>选项，可以点击<strong>“关联更多应用”</strong>选项，然后在打开的页面中，搜索<strong>“Colaboratory”</strong>，然后再点<strong>关联应用</strong>，再次点击右键就可以在<strong>“更多”</strong>选项中看到<strong>“Colaboratory”</strong>选项了。</p>




<p><img src="https://ws1.sinaimg.cn/large/a3d23450gy1fokw7hbuf6j20je0hpwfn.jpg" alt="" title=""></p>




<h2 id="设置免费gpu">设置免费GPU</h2>




<p>新建Colaboratory成功后，在笔记本里点Edit&gt;Notebook settings（编辑&gt;笔记本设置），或者Runtime&gt;Change runtime type（运行时&gt;改变运行时类型），然后在Hardware accelerator（硬件加速器）一栏选择GPU。</p>




<p><img src="https://ws1.sinaimg.cn/large/a3d23450gy1fokwett0eij209b091jrn.jpg" alt="" title=""></p>




<p>然后，Google Colab就可以用了。</p>




<h2 id="关联google-drive">关联Google Drive</h2>




<p>为了能让Colaboratory使用到你的Google Drive的文件，我们需要先运行下面这些代码，来安装必要的库、执行授权。</p>




<pre class="prettyprint"><code class="language-sh hljs lasso"><span class="hljs-subst">!</span>apt<span class="hljs-attribute">-get</span> install <span class="hljs-attribute">-y</span> <span class="hljs-attribute">-qq</span> software<span class="hljs-attribute">-properties</span><span class="hljs-attribute">-common</span> python<span class="hljs-attribute">-software</span><span class="hljs-attribute">-properties</span> module<span class="hljs-attribute">-init</span><span class="hljs-attribute">-tools</span>
<span class="hljs-subst">!</span>add<span class="hljs-attribute">-apt</span><span class="hljs-attribute">-repository</span> <span class="hljs-attribute">-y</span> ppa:alessandro<span class="hljs-attribute">-strada</span>/ppa <span class="hljs-number">2</span><span class="hljs-subst">&gt;&amp;</span><span class="hljs-number">1</span> <span class="hljs-subst">&gt;</span> /dev/<span class="hljs-built_in">null</span>
<span class="hljs-subst">!</span>apt<span class="hljs-attribute">-get</span> update <span class="hljs-attribute">-qq</span> <span class="hljs-number">2</span><span class="hljs-subst">&gt;&amp;</span><span class="hljs-number">1</span> <span class="hljs-subst">&gt;</span> /dev/<span class="hljs-built_in">null</span>
<span class="hljs-subst">!</span>apt<span class="hljs-attribute">-get</span> <span class="hljs-attribute">-y</span> install <span class="hljs-attribute">-qq</span> google<span class="hljs-attribute">-drive</span><span class="hljs-attribute">-ocamlfuse</span> fuse
from google<span class="hljs-built_in">.</span>colab <span class="hljs-keyword">import</span> auth
auth<span class="hljs-built_in">.</span>authenticate_user()
from oauth2client<span class="hljs-built_in">.</span>client <span class="hljs-keyword">import</span> GoogleCredentials
creds <span class="hljs-subst">=</span> GoogleCredentials<span class="hljs-built_in">.</span>get_application_default()
<span class="hljs-keyword">import</span> getpass
<span class="hljs-subst">!</span>google<span class="hljs-attribute">-drive</span><span class="hljs-attribute">-ocamlfuse</span> <span class="hljs-attribute">-headless</span> <span class="hljs-attribute">-id</span><span class="hljs-subst">=</span>{creds<span class="hljs-built_in">.</span>client_id} <span class="hljs-attribute">-secret</span><span class="hljs-subst">=</span>{creds<span class="hljs-built_in">.</span>client_secret} <span class="hljs-subst">&lt;</span> /dev/<span class="hljs-built_in">null</span> <span class="hljs-number">2</span><span class="hljs-subst">&gt;&amp;</span><span class="hljs-number">1</span> <span class="hljs-subst">|</span> grep URL
vcode <span class="hljs-subst">=</span> getpass<span class="hljs-built_in">.</span>getpass()
<span class="hljs-subst">!</span>echo {vcode} <span class="hljs-subst">|</span> google<span class="hljs-attribute">-drive</span><span class="hljs-attribute">-ocamlfuse</span> <span class="hljs-attribute">-headless</span> <span class="hljs-attribute">-id</span><span class="hljs-subst">=</span>{creds<span class="hljs-built_in">.</span>client_id} <span class="hljs-attribute">-secret</span><span class="hljs-subst">=</span>{creds<span class="hljs-built_in">.</span>client_secret}
</code></pre>




<p>运行的时候应该会看到下图所示的结果： <br>
<img src="https://ws1.sinaimg.cn/large/a3d23450gy1fokwi71w3uj20jv07q41s.jpg" alt="" title=""></p>




<p>看见那个链接之后，点击它，复制验证码并粘贴到文本框里。（这里其实是调用了Google Drive的SDK来访问你的Google Drive，而这个验证码就相当于access_key了）</p>




<p>授权完成后，就可以挂载Google Drive了：</p>




<pre class="prettyprint"><code class="language-sh hljs lasso"><span class="hljs-subst">!</span>mkdir <span class="hljs-attribute">-p</span> drive
<span class="hljs-subst">!</span>google<span class="hljs-attribute">-drive</span><span class="hljs-attribute">-ocamlfuse</span> drive</code></pre>




<h2 id="测试gpu">测试GPU</h2>




<p>这时，我们在本地电脑上创建一个.py文件来测试一下，挂载是否成功以及GPU是否在工作吧。</p>




<pre class="prettyprint"><code class="language-sh hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">"import tensorflow as tf\nprint(tf.test.gpu_device_name())"</span> &gt; test.py</code></pre>




<p>然后将test.py上传到我们开始时创建的app的文件夹里。</p>




<p>然后在Colaboratory笔记本中运行一下代码：</p>




<pre class="prettyprint"><code class="language-sh hljs diff"><span class="hljs-change">!python3 drive/app/test.py</span></code></pre>




<p>不出意外的话，就会输出类似以下的结果：</p>




<pre class="prettyprint"><code class="language-sh hljs applescript">/usr/<span class="hljs-keyword">local</span>/lib/python3<span class="hljs-number">.6</span>/dist-packages/h5py/__init__.py:<span class="hljs-number">36</span>: FutureWarning: Conversion <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> <span class="hljs-keyword">second</span> argument <span class="hljs-keyword">of</span> issubdtype <span class="hljs-keyword">from</span> `float` <span class="hljs-keyword">to</span> `np.floating` <span class="hljs-keyword">is</span> deprecated. In future, <span class="hljs-keyword">it</span> will be treated <span class="hljs-keyword">as</span> `np.float64 == np.dtype(float).type`.
  <span class="hljs-keyword">from</span> ._conv import register_converters <span class="hljs-keyword">as</span> _register_converters
<span class="hljs-number">2018</span>-<span class="hljs-number">02</span>-<span class="hljs-number">18</span> <span class="hljs-number">12</span>:<span class="hljs-number">37</span>:<span class="hljs-number">05.172726</span>: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:<span class="hljs-number">898</span>] successful NUMA node <span class="hljs-command">read</span> <span class="hljs-keyword">from</span> SysFS had negative value (-<span class="hljs-number">1</span>), <span class="hljs-keyword">but</span> there must be <span class="hljs-keyword">at</span> least one NUMA node, so <span class="hljs-keyword">returning</span> NUMA node zero
<span class="hljs-number">2018</span>-<span class="hljs-number">02</span>-<span class="hljs-number">18</span> <span class="hljs-number">12</span>:<span class="hljs-number">37</span>:<span class="hljs-number">05.172988</span>: I tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="hljs-number">1208</span>] Found device <span class="hljs-number">0</span> <span class="hljs-keyword">with</span> properties: 
<span class="hljs-property">name</span>: Tesla K80 major: <span class="hljs-number">3</span> minor: <span class="hljs-number">7</span> memoryClockRate(GHz): <span class="hljs-number">0.8235</span>
pciBusID: <span class="hljs-number">0000</span>:<span class="hljs-number">00</span>:<span class="hljs-number">04.0</span>
totalMemory: <span class="hljs-number">11.17</span>GiB freeMemory: <span class="hljs-number">503.62</span>MiB
<span class="hljs-number">2018</span>-<span class="hljs-number">02</span>-<span class="hljs-number">18</span> <span class="hljs-number">12</span>:<span class="hljs-number">37</span>:<span class="hljs-number">05.173016</span>: I tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="hljs-number">1308</span>] Adding visible gpu devices: <span class="hljs-number">0</span>
<span class="hljs-number">2018</span>-<span class="hljs-number">02</span>-<span class="hljs-number">18</span> <span class="hljs-number">12</span>:<span class="hljs-number">37</span>:<span class="hljs-number">05.457665</span>: I tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="hljs-number">989</span>] Creating TensorFlow device (/device:GPU:<span class="hljs-number">0</span> <span class="hljs-keyword">with</span> <span class="hljs-number">243</span> MB memory) -&gt; physical GPU (device: <span class="hljs-number">0</span>, <span class="hljs-property">name</span>: Tesla K80, pci bus <span class="hljs-property">id</span>: <span class="hljs-number">0000</span>:<span class="hljs-number">00</span>:<span class="hljs-number">04.0</span>, compute capability: <span class="hljs-number">3.7</span>)
/device:GPU:<span class="hljs-number">0</span></code></pre>




<p>到这里的话，那么恭喜你，你的GPU环境基本可以用了，只要把你的项目文件夹上传到你的app文件夹下，搭建好深度学习的库环境，就可以通过类似上面的操作进行神经网络训练了。</p>




<h2 id="tips">Tips</h2>




<h3 id="如何安装库">如何安装库？</h3>




<p>安装Keras：</p>




<pre class="prettyprint"><code class=" hljs lasso"><span class="hljs-subst">!</span>pip install <span class="hljs-attribute">-q</span> keras
<span class="hljs-keyword">import</span> keras</code></pre>




<p>安装PyTorch：</p>




<pre class="prettyprint"><code class=" hljs avrasm">!pip install -q http://download<span class="hljs-preprocessor">.pytorch</span><span class="hljs-preprocessor">.org</span>/whl/cu75/torch-<span class="hljs-number">0.2</span><span class="hljs-number">.0</span><span class="hljs-preprocessor">.post</span>3-cp27-cp27mu-manylinux1_x86_64<span class="hljs-preprocessor">.whl</span> torchvision
import torch</code></pre>




<p>安装OpenCV：</p>




<pre class="prettyprint"><code class=" hljs lasso"><span class="hljs-subst">!</span>apt<span class="hljs-attribute">-get</span> <span class="hljs-attribute">-qq</span> install <span class="hljs-attribute">-y</span> libsm6 libxext6 <span class="hljs-subst">&amp;&amp;</span> pip install <span class="hljs-attribute">-q</span> <span class="hljs-attribute">-U</span> opencv<span class="hljs-attribute">-python</span>
<span class="hljs-keyword">import</span> cv2</code></pre>




<p>安装XGBoost：</p>




<pre class="prettyprint"><code class=" hljs lasso"><span class="hljs-subst">!</span>pip install <span class="hljs-attribute">-q</span> xgboost<span class="hljs-subst">==</span><span class="hljs-number">0.4</span>a30
<span class="hljs-keyword">import</span> xgboost</code></pre>




<p>安装GraphViz：</p>




<pre class="prettyprint"><code class=" hljs lasso"><span class="hljs-subst">!</span>apt<span class="hljs-attribute">-get</span> <span class="hljs-attribute">-qq</span> install <span class="hljs-attribute">-y</span> graphviz <span class="hljs-subst">&amp;&amp;</span> pip install <span class="hljs-attribute">-q</span> pydot
<span class="hljs-keyword">import</span> pydot</code></pre>




<p>安装7zip Reader：</p>




<pre class="prettyprint"><code class=" hljs lasso"><span class="hljs-subst">!</span>apt<span class="hljs-attribute">-get</span> <span class="hljs-attribute">-qq</span> install <span class="hljs-attribute">-y</span> libarchive<span class="hljs-attribute">-dev</span> <span class="hljs-subst">&amp;&amp;</span> pip install <span class="hljs-attribute">-q</span> <span class="hljs-attribute">-U</span> libarchive
<span class="hljs-keyword">import</span> libarchive</code></pre>




<p>安装其他库：</p>




<pre class="prettyprint"><code class=" hljs cmake">用!pip <span class="hljs-keyword">install</span>或者!apt-get <span class="hljs-keyword">install</span>命令。</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[传统机器学习走向神经网络]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/12/14/chuan-tong-ji-qi-xue-xi-zou-xiang-shen-jing-wang-luo/"/>
    <updated>2017-12-14T23:43:13+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/12/14/chuan-tong-ji-qi-xue-xi-zou-xiang-shen-jing-wang-luo</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="传统机器学习走向神经网络">传统机器学习走向神经网络</h1></p>

<h2 id="神经网络的基本结构">神经网络的基本结构</h2>




<p>首先我们先来看一下最基础的神经网络结构： <br>
<img src="https://ws1.sinaimg.cn/large/a3d23450gy1fmgm7cftunj20kf0ckaep.jpg" alt="" title=""></p>




<p>由上图的结构可以看出，这个神经网络具有三层，其中输入层不计。而中间的橙色层则为两层隐藏层，最右的蓝色层为输出层。输入从最左边的输入层进行输入，然后经过两次隐藏层和激活函数之后进行输出，这样我们可以把这个神经网络简单地表示成一下的式子： <br>
<script type="math/tex; mode=display" id="MathJax-Element-82">Y_{out} = W_iX_{in}+B</script> <br>
W为X的权重，而B为函数的偏置。 <br>
其中，偏置值B的存在有利于打破数据对称的局面，使得神经网络可以应用在非对称的数据之上。</p>




<h2 id="神经网络的基本算法">神经网络的基本算法</h2>




<p>前向传导：前向传导的思想比较简单，下面的一张图足以概括它的主要思想。 <br>
<img src="https://ws1.sinaimg.cn/large/a3d23450gy1fmgp39x2ltj20oh0gydi9.jpg" alt="" title=""></p>




<p>反向传播：反向传播的方法其实也比较简单，其主要思想是涉及求偏导，以及链式求导法则。 <br>
<img src="https://ws1.sinaimg.cn/large/a3d23450gy1fmgp6qj2wlj20mj0i441b.jpg" alt="" title=""></p>




<p>梯度下降：梯度下降法是一个最优化算法，通常也称为最速下降法。最速下降法是求解无约束优化问题最简单和最古老的方法之一，虽然现已不具有实用性，但是许多有效算法都是以它为基础进行改进和修正而得到的。最速下降法是用负梯度方向为搜索方向的，最速下降法越接近目标值，步长越小，前进越慢。</p>




<h2 id="朴素贝叶斯和神经网络">朴素贝叶斯和神经网络</h2>




<p>首先朴素贝叶斯算法的原始形式可以表达成以下的形式： <br>
<script type="math/tex" id="MathJax-Element-1467">G(x)=arg\  max\ p(y)  
\prod\limits^n_{i=1}p(x_i|y)^{x_i}</script></p>




<p>除此之外，该算法还有一下特点： <br>
<script type="math/tex" id="MathJax-Element-1468">x_i只有0，1两种取值</script> <br>
<script type="math/tex" id="MathJax-Element-1469">x_i取1意味着x_i对应了的特征“出现了”</script>  <br>
<script type="math/tex" id="MathJax-Element-1470">x_i取0意味着x_i对应了的特征“没出现”</script></p>




<p>这样转换成矩阵的形式时，我们可以采用独热编码亦称One-hot Encode。 <br>
独热编码：</p>




<p>解决了分类标签的问题，那么我们又该怎样用神经网络的线性模型形式来表达贝叶斯公式中概率相乘的情况呢？</p>




<p>没错，就是使用对数函数。根据对数函数的性质<script type="math/tex" id="MathJax-Element-1471">log_2X+log_2Y=log_2XY</script>,我们就可以通过对数变换，将乘法转换成加法的形式，我们可以把上面的朴素贝叶斯公式改写成： <br>
<script type="math/tex" id="MathJax-Element-1472">G(x)=arg\ max\ log(y)+\sum\limits^n_{i=1}x_ilog\ p(x_i|y)</script></p>




<p>那么我们就可以用退化成线性模型的神经网络来实现朴素贝叶斯模型。</p>




<h3 id="核心实现">核心实现</h3>


<pre><code class="python"># 独热化处理部分
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
x_train = enc.fit_transform(x_train).toarray()
x_test = enc.transform(x_test).toarray()

## .....篇幅有限,此处省略其余代码

# NaiveBayes -&gt; NN 权值转换部分
class NB2NN(TransformationBase):
    def __init__(self, *args, **kwargs):
        super(NB2NN, self).__init__(*args, **kwargs)
        self._name_appendix = "NaiveBayes"
        self.model_param_settings.setdefault("activations", None)

    def _transform(self):
        self.hidden_units = []
        x, y, x_test, y_test = self._get_all_data()
        nb = MultinomialNB()
        nb.fit(x, y)
        self._print_model_performance(nb, "Naive Bayes", x, y, x_test, y_test)
        self._transform_ws = [nb.feature_log_prob_.T]
        self._transform_bs = [nb.class_log_prior_]
</code></pre>

<h2 id="决策树贝叶斯和神经网络">决策树贝叶斯和神经网络</h2>




<p>首先，决策树的原理主要就是通过数据信息熵的变化来选择当前的最优分类点，然后从根开始一步一步扩展成树。而实质上，最后成功构建出来的决策树，其从根节点开始到每个分类叶子节点的路径对应的都是一组高维空间上的超平面组合。决策树的分类也就是用一组超平面去划分数据空间，使得最后剩下一个唯一确定的标识。</p>




<p>知道决策树的本质之后，我们就可以用这样的方法来将决策树算法迁移到神经网络上： <br>
* 第一个隐藏层表达决策树的中间节点所对应的超平面 <br>
* 第二个隐藏层表达各个决策的路径 <br>
* 第二个隐藏层和输出层之间的权值矩阵表达各个叶节点</p>




<h3 id="核心实现-1">核心实现</h3>


<pre><code class="python">## 因为决策树到神经网络的转换较为复杂,此处仅贴出核心代码
class DT2NN(TransformationBase):
    def __init__(self, *args, **kwargs):
        super(DT2NN, self).__init__(*args, **kwargs)
        self._name_appendix = "DTree"
        self.model_param_settings.setdefault("activations", ["sign", "one_hot"])

    def _transform(self):
        x, y, x_test, y_test = self._get_all_data()
        tree = DecisionTreeClassifier()
        tree.fit(x, y)
        self._print_model_performance(tree, "Decision Tree", x, y, x_test, y_test)

        tree_structure = export_structure(tree)
        n_leafs = sum([1 if pair[1] == -1 else 0 for pair in tree_structure])
        n_internals = n_leafs - 1

        print("Internals : {} ; Leafs : {}".format(n_internals, n_leafs))

        b = np.zeros(n_internals, dtype=np.float32)
        w1 = np.zeros([x.shape[1], n_internals], dtype=np.float32)
        w2 = np.zeros([n_internals, n_leafs], dtype=np.float32)
        w3 = np.zeros([n_leafs, self.n_class], dtype=np.float32)
        node_list = []
        node_sign_list = []
        node_id_cursor = leaf_id_cursor = 0
        max_route_length = 0
        self.hidden_units = [n_internals, n_leafs]

        for depth, feat_dim, rs in tree_structure:
            if feat_dim != -1:
                if depth == len(node_list):
                    node_sign_list.append(-1)
                    node_list.append([node_id_cursor, feat_dim, rs])
                    w1[feat_dim, node_id_cursor] = 1
                    b[node_id_cursor] = -rs
                    node_id_cursor += 1
                else:
                    node_list = node_list[:depth + 1]
                    node_sign_list = node_sign_list[:depth] + [1]
            else:
                valid_nodes = set()
                local_sign_list = node_sign_list[:]
                for i, ((node_id, node_dim, node_threshold), node_sign) in enumerate(
                    zip(node_list, node_sign_list)
                ):
                    valid_nodes.add((node_id, node_sign))
                    if i &gt;= 1:
                        for j, ((local_id, local_dim, local_threshold), local_sign) in enumerate(zip(
                            node_list[:i], local_sign_list[:i]
                        )):
                            if node_sign == local_sign and node_dim == local_dim:
                                if (
                                    (node_sign == -1 and node_threshold &lt; local_threshold) or
                                    (node_sign == 1 and node_threshold &gt; local_threshold)
                                ):
                                    local_sign_list[j] = 0
                                    valid_nodes.remove((local_id, local_sign))
                                    break
                for node_id, node_sign in valid_nodes:
                    w2[node_id, leaf_id_cursor] = node_sign / len(valid_nodes)
                max_route_length = max(max_route_length, len(valid_nodes))
                w3[leaf_id_cursor] = rs / np.sum(rs)
                leaf_id_cursor += 1

        w2 *= max_route_length
        self._transform_ws = [w1, w2, w3]
        self._transform_bs = [b]

#................ 篇幅有限,省略其余代码

# DTree -&gt; NN
def export_structure(tree):
    tree = tree.tree_

    def recurse(node, depth):
        feature_dim = tree.feature[node]
        if feature_dim == _tree.TREE_UNDEFINED:
            yield depth, -1, tree.value[node]
        else:
            threshold = tree.threshold[node]
            yield depth, feature_dim, threshold
            yield from recurse(tree.children_left[node], depth + 1)
            yield depth, feature_dim, threshold
            yield from recurse(tree.children_right[node], depth + 1)

    return list(recurse(0, 0))
</code></pre>

<h2 id="模型改进">模型改进</h2>




<h3 id="对于朴素贝叶斯">对于朴素贝叶斯</h3>




<p>根据上述的原理和理论，我们可以将朴素贝叶斯和决策树转换成神经网络模型，但是转换之后是否存在意义呢？</p>




<p><strong>首先</strong>本身可以通过简单log对数转换成线性模型的朴素贝叶斯算法来说，其转换的步骤并不复杂，但却能够对朴素贝叶斯的独立假设进行一定的微调修正。</p>




<h3 id="对于决策树">对于决策树</h3>




<p>那么对于决策树来说，神经网络的介入可以对决策树的硬边界作一定的修正和“软化”作用。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度学习入门简介（二）]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/12/08/shen-du-xue-xi-ru-men-jian-jie-(er-)/"/>
    <updated>2017-12-08T12:52:08+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/12/08/shen-du-xue-xi-ru-men-jian-jie-(er-)</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="深度学习入门简介二">深度学习入门简介（二）</h1></p>

<h2 id="深度学习的三部曲">深度学习的三部曲</h2>




<h3 id="训练前的准备">训练前的准备</h3>




<p>1）训练数据 <br>
在训练一个深度学习的模型之前，我们首先需要准备的就是训练数据，若是图片的话其中就包括：图片的内容以及他的标签。 <br>
<strong>注：学习的分类目标也是包括在训练数据里面的</strong></p>




<p>2）学习目标 <br>
学习的目标往往就是一个二分类或者多分类问题。而对于最后的效果，我们需要达到当我们输入一个待预测或分类的值时，正确的结果应该对应那个最大概率的输出项。</p>




<p>3）损失函数 <br>
简单来说，深度学习的分类和回归的本质就是，找到一个使得在所有样本项上取得的误差值最小的函数。而预测值与真实值的误差我们可以通过他们之间的距离计算得出。</p>




<h3 id="最小化误差">最小化误差</h3>




<p>为了达到一个分类或预测准确的效果，我们就要找到一个网络中的对应的超参数<script type="math/tex" id="MathJax-Element-471">\theta</script>使得网络的预测与真实值的误差是最小的。其中一个简单而粗暴的方法就是：枚举法。但是这样做的效率显然非常的低效。为了能够更加优化地找到或者说是接近使得网络取得最小误差的超参数<script type="math/tex" id="MathJax-Element-472">\theta</script>我们可以采用<strong>梯度下降法</strong>，其根据预设的学习率不断更新权重的梯度来接近局部最优解。</p>




<p>其具体过程图如下所示： <br>
<img src="https://ws1.sinaimg.cn/large/a3d23450gy1fm92vbwnztj20ps0em0ub.jpg" alt="" title=""></p>




<p><strong>梯度下降的缺点：</strong> <br>
由于梯度下降每次计算时都是随机选取一个开始点，再根据学习率来慢慢减小全局误差。这样一来，学习率的设定就十分重要了，过大的学习率容易越过最低点，而过小的学习率又使得误差降低的速度过慢，且过小的学习率也会造成学习过程中陷入局部最低点后无法跳出。但实际上由于精度误差的问题梯度下降永远无法到达真正意义上的全局最低点，即无法取得全局最有解。但在多次的迭代运算后一般可以达到一个可接受的损失误差的局部最优解。</p>




<p>具体图示如下： <br>
<img src="https://ws1.sinaimg.cn/large/a3d23450gy1fm937sjcc5j20q40hcdn6.jpg" alt="" title=""></p>




<h3 id="反向传播">反向传播</h3>




<p><strong>反向传播算法</strong>：这是一种高效的计算权值梯度的方式。</p>




<p>有关算法的详细介绍可以参考：</p>




<p><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/DNN%20backprop.ecm.mp4/index.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/DNN%20backprop.ecm.mp4/index.html</a></p>




<p>通常我们在使用流行框架来构建神经网络时，不用亲自考虑如何去计算和处理梯度值，框架的作者在实现框架时已经做好了相关处理。</p>




<h3 id="神经网络的理论">神经网络的理论</h3>




<p>根据 <a href="http://neuralnetworksanddeeplearning.com/chap4.html">A visual proof that neural nets can compute any function</a> 文章的描述任何的连续函数 f 都可以用一个隐藏层内有足够多的神经元的神经网络来近似。</p>




<p><strong>既然这样，为什么今天流行的是深度网络而不是广度网络呢？</strong></p>




<p>根据 <a href="https://www.microsoft.com/en-us/research/publication/conversational-speech-transcription-using-context-dependent-deep-neural-networks/">Seide, Frank, Gang Li, and Dong Yu. “Conversational Speech Transcription <br>
Using Context-Dependent Deep Neural Networks.” Interspeech. 2011. <br>
</a> 论文的研究，广度和深度网络对降低全局误差时的参数如下表所示：</p>




<p><img src="https://ws1.sinaimg.cn/large/a3d23450gy1fm98wlbz8yj20np0g2gmz.jpg" alt="fat-vs-deep" title=""></p>




<p>根据上图的研究结果，我们可以发现使用多层的神经元能够更加容易近似一些函数，这其实就跟我们的电子电路中的逻辑电路类似，即便在电子电路中两层的逻辑门电路就可以实现任意的逻辑操作，但是使用多层的逻辑门电路可以更容易的构建一些逻辑操作。</p>




<h3 id="模块化">模块化</h3>




<p>深度学习中还有一个特点就是<strong>模块化</strong>，在一层层的网络层的堆叠中，每一层都会作为一个模块来学习数据。简单来说，深度学习的过程其实就是一个自动提取特征的过程。对于传统的机器学习而言，数据科学家通过特征工程，提取出数据的特征，再利用特征对数据进行建模以此达到分类预测的效果。深度学习通过各个神经元的加权组合以及反向传播的权值调整，使得整个网络的每一层都渐渐趋向稳定，且其稳定值能够在那个维度上进行部分数据的划分，简单来说就是一个区域性的能够对数据有所区分的特性。那随着各个神经层的共同作用使得深度学习在分类预测应用上效果显著。</p>




<p>最后，深度学习在图像分类的本质大概可以用以下这张图片概括： <br>
<img src="https://ws1.sinaimg.cn/large/a3d23450gy1fm99ftf2vwj20ow0dsn0p.jpg" alt="" title=""></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度学习入门简介]]></title>
    <link href="https://edmondfrank.github.io/blog/2017/12/02/shen-du-xue-xi-ru-men-jian-jie/"/>
    <updated>2017-12-02T17:17:32+08:00</updated>
    <id>https://edmondfrank.github.io/blog/2017/12/02/shen-du-xue-xi-ru-men-jian-jie</id>
    <content type="html"><![CDATA[<p>﻿<h1 id="深度学习入门简介">深度学习入门简介</h1></p>

<h2 id="背景">背景</h2>




<p><strong>深度学习</strong>的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。</p>




<h2 id="概念">概念</h2>




<p><strong>深度学习</strong>的概念由Hinton等人于2006年提出。基于深度置信网络(DBN)提出非监督贪心逐层训练算法，为解决深层结构相关的优化难题带来希望，随后提出多层自动编码器深层结构。此外Lecun等人提出的卷积神经网络是第一个真正多层结构学习算法，它利用空间相对关系减少参数数目以提高训练性能。</p>




<h2 id="原理">原理</h2>




<p><strong>深度学习</strong>是机器学习中一种基于对数据进行表征学习的方法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别）。深度学习的好处是用非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征。</p>




<p>（以上内容摘取自百度百科）</p>




<p><strong>个人理解：</strong>如果说机器学习是为了找出一个能够代表输入变量和输出变量的关系的函数的话；那么深度学习就是先根据输入和输出变量之间的关系，列出一系列能够代表他们之间关系的函数，然后再从这个函数集之中提取一个最优的函数。</p>




<h2 id="结构">结构</h2>




<h3 id="神经元">神经元</h3>




<p>随着神经网络的应用和深度学习在人工智能领域的大放异彩，很多人都说神经网络的是最成功的仿真模型。那么他的结构究竟是怎样子的呢？</p>




<p><img src="https://i.loli.net/2017/12/01/5a217a29cf015.png" alt="nn.png" title=""></p>




<p>一个简单的神经网咯函数（一般称作：神经元），就如上图所示。</p>




<p>他的主要执行过程：</p>




<blockquote>
  <p>多个输入a X 各自的权重w + 偏置值b =&gt; 激活函数 =&gt; 输出</p>
</blockquote>




<p>其中，在这个流程之中，我们可能比较迷惑的是那个激活函数Activation function。</p>




<p><strong>Activation Function：</strong>即激活函数，目前的常用的激活函数由挺多的，例如，Simmoid Function，tanh，relu等等。虽然形式上不同，但是他们大体的目的都是较为一致的，就是用来加入非线性因素的，因为线性模型的表达能力不够。</p>




<p><script type="math/tex; mode=display" id="MathJax-Element-1"> 如下所示的Sigmoid Function \\\sigma(z)=\frac{1}{1+e^{-z}}</script></p>




<p>同时，激活函数可以将非常大或非常小的数据映射到“逻辑空间”[-1,1]之间，这样映射过后的数据更适合在反向传播算法中进行梯度下降。</p>




<h3 id="连接方式">连接方式</h3>




<p>上面我们提及的仅仅是神经网络中的一个神经元，他是神经网络之中最基本的组成单位。但是如果要构建一个强大智能的神经网络，仅仅靠一个神经元是不行的。于是，我们便可以将多个神经元分层连接起来，这样才构成了我们所知道的神经网络。</p>




<p>既然，神经网络的构成本质就是神经元的连接，那么不同的连接方式就会形成不同的神经网络结构如全连接前馈网络，多层感知器，卷积神经网络，循环神经网络等等。</p>




<h2 id="全连接前馈网络">全连接前馈网络</h2>




<p>在众多的连接之间，全连接的前馈网络不仅较为简单，也是很多深层网络的基础。他的基本连接方式如下图片所示：</p>




<p><img src="https://i.loli.net/2017/12/02/5a2180e6120e2.png" alt="feedforward.png" title=""></p>




<p>其中，一般来说神经网络的第一层通常都是输入层，而最后一层便是输出层以及中间的都统一称作隐藏层。深度神经网络中的“深”便代表了这个网络中间有非常多的隐藏层。</p>




<h2 id="输出层">输出层</h2>




<p>通常，输出层一般为Softmax 层，并且其可以为任意值。在应用中，输出的结果通常用概率的形式表达，其具体形式如下图所示： <br>
<img src="https://i.loli.net/2017/12/02/5a218350cc19b.png" alt="output.png" title=""></p>




<p>那么，我们知道了神经网络的组成之后，我们要是想自己构建一个神经网络，我们又该如何确定神经网络的层数和每层的神经元的个数呢？</p>




<p><strong>就目前来说，</strong>并没有相当的严谨的理论来指导神经网络的构建。我们往往需要依靠直觉和训练测试结果的误差反馈来一步一步选择我们的层数和神经元数以达到要求的效果。</p>

]]></content>
  </entry>
  
</feed>
