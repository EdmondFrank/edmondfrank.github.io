
<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>K-近邻算法-Part1 | EdmondFrank's 时光足迹</title>

	<meta name="author" content="EdmondFrank"> 
	
	<meta name="description" content="K-近邻算法-Part1 概述 K-近邻算法，即K-近邻分类算法，简称KNN其通过采用测量不同的特征值之间的距离方法进行分类。 有关K-近邻算法的问题 优点：精度高，对异常值不敏感，无数据输入假定 缺点：计算复杂度较高，空间复杂度较高 适用数据范围：数值型和标称型 工作原理 存在一个样本数据集合， &hellip;"> <meta name="keywords" content="">

	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="/atom.xml" rel="alternate" title="EdmondFrank's 时光足迹" type="application/atom+xml">
	<link rel="canonical" href="">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="/stylesheets/font-awesome.min.css" rel="stylesheet" type="text/css">
	
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	
</head>



<body>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
	<header id="header">
<div class="inner">
  <ul class="navigation">
    <li><a href="/"><h1>EdmondFrank's 时光足迹</h1></a></li>
    <li><a href="/" class="is-selected">Blog</a></li>
    <li><a href="http://github.com/EdmondFrank" target="blank">Github</a></li>
    <li><a href="https://twitter.com/EdmondFrank4" target="blank">Twitter</a></li>
  </ul>
</div></header>

	<div id="content" class="inner"><article class="post">
	<h2 class="title">K-近邻算法-Part1</h2>
	<div class="meta">
		<div class="date">








  



<time datetime="2017-08-14T23:48:56+08:00" pubdate data-updated="true">Aug 14th, 2017</time></div>
	</div>
	<div class="entry-content"><h1>K-近邻算法-Part1</h1>

<h2>概述</h2>

<p>K-近邻算法，即K-近邻分类算法，简称KNN其通过采用测量不同的特征值之间的距离方法进行分类。</p>

<h3>有关K-近邻算法的问题</h3>

<p>优点：精度高，对异常值不敏感，无数据输入假定</p>

<p>缺点：计算复杂度较高，空间复杂度较高</p>

<p>适用数据范围：数值型和标称型</p>

<h3>工作原理</h3>

<p>存在一个样本数据集合，即训练样本集，并且训练样本中的每一个数据都存在标签，我们可以清楚地知道每一个数据条目其与对应分类的所属关系。</p>

<p>然后在接受没有标记的新数据输入时，将新数据的特征提取出来将其一一与训练样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似的（即所谓的最近邻）的分类标签来作为新数据的标签。</p>

<p>其中为了避免偶尔性和离群值造成的误差因此有了以样本数据集中前k个最相似的数据作为判别的参考的标准这种做法。<strong>通常k是不大于20的整数而且一般选取奇数作为k值</strong>（奇数可以在投票分类时避免出现等票的情况），最终的分类结果由k个样本的分类标签投票形成，出现最多的分类标签作为新数据的分类。</p>

<h3>一般算法流程</h3>

<ol>
<li>收集数据</li>
<li>准备数据：对数据进行清洗和结构化处理，使得数据可以进行距离计算</li>
<li>分析数据：提取相关特征</li>
<li>训练分类：knn算法并不需要训练</li>
<li>测试算法：计算错误率</li>
<li>使用算法：将新数据输入进行对应结构化之后，运行算法进行判定分类情况，并对后续的分类结果进行进一步处理应用</li>
</ol>


<h2>用KNN 制作简单的分类器</h2>

<p>使用数据：<a href="https://archive.ics.uci.edu/ml/datasets/Iris">UCI的鸢尾花数据集</a>
点击进入目标链接后:</p>

<blockquote><p>按照 Download Data Folder > iris.data 路径来下载指定数据集</p></blockquote>

<h4>准备数据</h4>

<p>在这里我们使用Python 的 Pandas 包以没有标题栏的csv文件的形式读入数据</p>

<p><strong>关键函数：read_csv</strong></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># loading libraries</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'>
</span><span class='line'><span class="c"># define column names</span>
</span><span class='line'><span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;sepal_length&#39;</span><span class="p">,</span> <span class="s">&#39;sepal_width&#39;</span><span class="p">,</span> <span class="s">&#39;petal_length&#39;</span><span class="p">,</span> <span class="s">&#39;petal_width&#39;</span><span class="p">,</span> <span class="s">&#39;class&#39;</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'><span class="c"># loading training data</span>
</span><span class='line'><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;iris.data.txt&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
</span><span class='line'><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>


<p><strong>输出</strong>：</p>

<table>
<thead>
<tr>
<th> index </th>
<th> sepal_length </th>
<th> sepal_width </th>
<th> petal_length </th>
<th> petal_width </th>
<th> class       </th>
</tr>
</thead>
<tbody>
<tr>
<td> 0     </td>
<td> 5.1          </td>
<td> 3.5         </td>
<td> 1.4          </td>
<td> 0.2         </td>
<td> Iris-setosa </td>
</tr>
<tr>
<td> 1     </td>
<td> 4.9          </td>
<td> 3.0         </td>
<td> 1.4          </td>
<td> 0.2         </td>
<td> Iris-setosa </td>
</tr>
<tr>
<td> 2     </td>
<td> 4.7          </td>
<td> 3.2         </td>
<td> 1.3          </td>
<td> 0.2         </td>
<td> Iris-setosa </td>
</tr>
<tr>
<td> 3     </td>
<td> 4.6          </td>
<td> 3.1         </td>
<td> 1.5          </td>
<td> 0.2         </td>
<td> Iris-setosa </td>
</tr>
<tr>
<td> 4     </td>
<td> 5.0          </td>
<td> 3.6         </td>
<td> 1.4          </td>
<td> 0.2         </td>
<td> Iris-setosa </td>
</tr>
</tbody>
</table>


<h4>构建算法</h4>

<p><strong>距离函数</strong></p>

<p>我a们在上面的例子中把一个很重要的概念隐藏了起来，在选择一个数量k还只是小问题，更重要的是距离的计算方法。毕竟，当我们说“最近的k个点”时，这个“近”是怎么衡量的？</p>

<p>在数学中，一个空间上距离的严格定义如下：
设 M 为一个空间，M上的一个距离函数是一个函数<math xmlns='http://www.w3.org/1998/Math/MathML'
    mathematica:form='TraditionalForm'
    xmlns:mathematica='http://www.wolfram.com/XML/'>
 <mrow>
  <mi>d</mi>
  <mo>:</mo>
  <mrow>
   <mrow>
    <mrow>
     <mi>M</mi>
     <mo>&#8290;</mo>
     <mi>x</mi>
     <mo>&#8290;</mo>
     <mi>M</mi>
    </mrow>
    <mo>-</mo>
   </mrow>
   <mo>&gt;</mo>
   <mi>R</mi>
  </mrow>
 </mrow>
</math>，满足</p>

<ul>
<li>d(x,y)≥0  ∀x,y∈M</li>
<li>d(x,y)=0⟺x=y</li>
<li>d(x,y)=d(y,x) ∀x,y∈M</li>
<li>d(x,z)≤d(x,y)+d(y,z) ∀x,y,z∈M
两个点 x,y 之间的距离就是<math xmlns='http://www.w3.org/1998/Math/MathML'
  mathematica:form='TraditionalForm'
  xmlns:mathematica='http://www.wolfram.com/XML/'>
<mrow>
<mi>d</mi>
<mo>(</mo>
<mrow>
 <mi>x</mi>
 <mo>,</mo>
 <mi>y</mi>
</mrow>
<mo>)</mo>
</mrow>
</math>。</li>
</ul>


<p>我们一般最常用的距离函数是欧氏距离，也称作<math xmlns='http://www.w3.org/1998/Math/MathML'
    mathematica:form='TraditionalForm'
    xmlns:mathematica='http://www.wolfram.com/XML/'>
 <msub>
  <mi>L</mi>
  <mn>2</mn>
 </msub>
</math>距离。</p>

<p>如果
x=(x1,x2,…,xn) 和 y=(y1,y2,…,yn)是 n 维欧式空间 Rn 上的两个点，那它们之间的<math xmlns='http://www.w3.org/1998/Math/MathML'
    mathematica:form='TraditionalForm'
    xmlns:mathematica='http://www.wolfram.com/XML/'>
 <msub>
  <mi>L</mi>
  <mn>2</mn>
 </msub>
</math>距离是</p>

<p><math xmlns='http://www.w3.org/1998/Math/MathML'
    mathematica:form='TraditionalForm'
    xmlns:mathematica='http://www.wolfram.com/XML/'>
 <mrow>
  <mrow>
   <msub>
    <mi>d</mi>
    <mn>2</mn>
   </msub>
   <mo>(</mo>
   <mrow>
    <mi>X</mi>
    <mo>,</mo>
    <mi>Y</mi>
   </mrow>
   <mo>)</mo>
  </mrow>
  <mo>&#63449;</mo>
  <msqrt>
   <mrow>
    <munderover>
     <mo movablelimits='true'>&#8721;</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>n</mi>
    </munderover>
    <mrow>
     <mrow>
      <mo>(</mo>
      <mrow>
       <msub>
        <mi>X</mi>
        <mi>i</mi>
       </msub>
       <mo>-</mo>
       <msub>
        <mi>Y</mi>
        <mi>i</mi>
       </msub>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mo>&#8290;</mo>
     <msup>
      <mo>&#8202;</mo>
      <mn>2</mn>
     </msup>
    </mrow>
   </mrow>
  </msqrt>
 </mrow>
</math></p>

<p>由于Python 的scikit-learn包已经实现了KNN算法，因此我们在这里可以直接调用。</p>

<p>在<a href="http://scikit-learn.org/stable/index.html">scikit—learn</a>中，需要以matrix的形式和目标向量的形式来导入和训练数据。</p>

<p>因此在使用scikit-learn之前需要做额外的数据结构处理，同时还应该把原数据划分成训练数据和测试数据这样更加有利于我们下面的算法正确率评估。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># loading libraries</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span><span class='line'>
</span><span class='line'><span class="c"># create design matrix X and target vector y</span>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">])</span>   <span class="c"># end index is exclusive</span>
</span><span class='line'><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;class&#39;</span><span class="p">])</span>   <span class="c"># another way of indexing a pandas df</span>
</span><span class='line'>
</span><span class='line'><span class="c"># split into train and test</span>
</span><span class='line'><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>最后，我们根据划分好的数据集来构建真正的分类器，并用构建出来的分类器来进行数据拟合以及评估他的正确率。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># loading library</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
</span><span class='line'><span class="c"># instantiate learning model (k = 3)</span>
</span><span class='line'><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># fitting the model</span>
</span><span class='line'><span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># predict the response</span>
</span><span class='line'><span class="n">pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># evaluate accuracy</span>
</span><span class='line'><span class="c"># print(y_test,pred)</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<p><strong>输出</strong></p>

<blockquote><p>0.98</p></blockquote>

<p>由此可见，在适用的场合之下，KNN分类器的准确率也是可以达到一个较为理想的水平的。</p>
</div>


<div class="sharing">
  
  <div class="sns-tw">
    <a href="https://twitter.com/share" class="twitter-share-button">Tweet</a>
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js  ';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
  
  
</div>
</article>
</div>
	<footer id="footer" class="inner"><div class="aboutme">
<hr>
  <div class="profile">

    <h3>EdmondFrank</h3>
    <p><a href="http://twitter.com/EdmondFrank4">@EdmondFrank4</a></p>

    <div class="codes">
      <h4>Codes</h4>
      <div class="github"><a href="https://github.com/EdmondFrank">github.com/EdmondFrank</a></div>
    </div>
  </div>

  <div class="float_clear"></div>

</div>

Copyright &copy; 2017
 EdmondFrank 
<br>
Powered by Octopress.



</footer>
	

</body>
</html>
