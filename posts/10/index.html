
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="ja"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>EdmondFrank's 时光足迹</title>
  <meta name="author" content="EdmondFrank">

  
  <meta name="description" content="Python实现Fisher 判别分析 Fisher原理 费歇（Fisher）判别思想是投影，使多维问题化为一维问题来处理。选择一个适当的投影轴，使所有的样本点都投影在这个轴上得到一个投影值。对这个投影轴的方向的要求是：使每一类内的投影值所形成的类内距离差尽可能小， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="https://edmondfrank.github.io/posts/10/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="EdmondFrank's 时光足迹" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.cat.net/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
   <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_SVG"></script>
  
  

</head>

<body >
  <div id="container">
    <header role="banner"><hgroup>
  <h1><a href="/">EdmondFrank's 时光足迹</a></h1>
  
    <h2>この先は暗い夜道だけかもしれない　それでも信じて進むんだ。星がその道を少しでも照らしてくれるのを。<br>或许前路永夜，即便如此我也要前进，因为星光即使微弱也会我为照亮前途。<br>——《四月は君の嘘》</h2>
  
</hgroup>

</header>
    <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="site:edmondfrank.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/archives">Archives</a></li>
</ul>

</nav>
    <div id="main">
      <div id="content">
        <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/08/30/pythonshi-xian-fisherpan-bie-fen-xi/">Python实现Fisher判别分析</a></h1>
    
    
      <div class="post-meta">
        <p class="meta">
          <span class="timestamp">- 








  



<time datetime="2017-08-30T09:06:21+08:00" pubdate data-updated="true"></time> -</span>
          
        </p>
      </div>
    
  </header>


  <div class="entry-content"><h1 id="python实现fisher-判别分析">Python实现Fisher 判别分析</h1>




<h2 id="fisher原理">Fisher原理</h2>




<p>费歇（Fisher）判别思想是投影，使多维问题化为一维问题来处理。选择一个适当的投影轴，使所有的样本点都投影在这个轴上得到一个投影值。对这个投影轴的方向的要求是：使每一类内的投影值所形成的类内距离差尽可能小，而不同类间的投影值所形成的类间距离差尽可能大。 <br>
<img src="https://i.loli.net/2017/09/01/59a90dda8790e.png" alt="" title=""></p>




<p><img src="https://i.loli.net/2017/09/01/59a9136f647d1.png" alt="enter image description here" title=""></p>




<p>这样如果我们想要同类样列的投影点尽可能接近，可以让同类样列投影点的协方差尽可能小，即<script type="math/tex" id="MathJax-Element-1">w^T \left(\sum _0 w\right)+w^T \left(\sum _1 w\right)</script>尽可能小;而欲使异类样列的投影点尽可能远离，可以让类中心之间的距离尽可能大，即<script type="math/tex" id="MathJax-Element-2">\left[\left[u_0 w^T-u_1 w^T\right]\right]</script>尽可能大。同时结合两者我们可以得到欲最大化的目标： <br>
<img src="https://i.loli.net/2017/09/01/59a9136f56af9.png" alt="enter image description here" title=""></p>




<p>(本文图片截取自<a href="https://book.douban.com/subject/26708119/">《机器学习》</a>周志华)</p>




<p><img src="https://i.loli.net/2017/09/01/59a9136f635c6.png" alt="enter image description here" title=""></p>




<p>有了上面的推理之后我们接下来就以DNA分类为例来实现一下Fisher线性判别。</p>




<h2 id="数据准备">数据准备</h2>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span><span class='line'><span class="n">dna_list</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">&#39;dna2&#39;</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span class='line'>    <span class="n">dna_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="o">.</span><span class="n">strip</span><span class="p">,</span><span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()))</span>
</span><span class='line'>    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dna_list</span><span class="p">))</span>
</span><span class='line'><span class="k">def</span> <span class="nf">generate_feature</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">:</span>
</span><span class='line'>        <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</span><span class='line'>        <span class="k">yield</span> <span class="p">[</span>
</span><span class='line'>        <span class="n">chary</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="s">&#39;a&#39;</span><span class="p">)</span><span class="o">/</span><span class="n">size</span><span class="p">,</span>
</span><span class='line'>        <span class="n">chary</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="s">&#39;t&#39;</span><span class="p">)</span><span class="o">/</span><span class="n">size</span><span class="p">,</span>
</span><span class='line'>        <span class="n">chary</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="s">&#39;c&#39;</span><span class="p">)</span><span class="o">/</span><span class="n">size</span><span class="p">,</span>
</span><span class='line'>        <span class="n">chary</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="s">&#39;g&#39;</span><span class="p">)</span><span class="o">/</span><span class="n">size</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">generate_feature</span><span class="p">(</span><span class="n">dna_list</span><span class="p">)),</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
</span><span class='line'><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
</span><span class='line'><span class="n">y</span><span class="p">[</span><span class="mi">10</span><span class="p">:]</span><span class="o">=</span><span class="mi">2</span>
</span><span class='line'><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">20</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="s">&#39;</span><span class="se">\n</span><span class="s">&#39;</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<blockquote>
  <p>输出结果： <br>
  40 <br>
  [[ 0.2972973   0.13513514  0.17117117  0.3963964 ] <br>
   [ 0.35454545  0.5         0.04545455  0.1       ] <br>
   [ 0.42342342  0.28828829  0.10810811  0.18018018] <br>
   [ 0.35135135  0.12612613  0.12612613  0.3963964 ] <br>
   [ 0.27927928  0.18918919  0.16216216  0.36936937] <br>
   [ 0.21818182  0.56363636  0.14545455  0.07272727] <br>
   [ 0.20720721  0.15315315  0.20720721  0.43243243] <br>
   [ 0.3         0.5         0.08181818  0.11818182] <br>
   [ 0.2         0.56363636  0.17272727  0.06363636] <br>
   [ 0.27027027  0.06306306  0.21621622  0.45045045] <br>
   [ 0.32727273  0.5         0.02727273  0.14545455] <br>
   [ 0.23423423  0.10810811  0.23423423  0.42342342] <br>
   [ 0.29090909  0.64545455  0.          0.06363636] <br>
   [ 0.18181818  0.13636364  0.27272727  0.40909091] <br>
   [ 0.29090909  0.5         0.11818182  0.09090909] <br>
   [ 0.25454545  0.51818182  0.1         0.12727273] <br>
   [ 0.27433628  0.36283186  0.19469027  0.16814159] <br>
   [ 0.27027027  0.15315315  0.16216216  0.41441441]]  <br>
   [ 1.  2.  1.  1.  1.  2.  1.  2.  2.  1.  2.  1.  2.  1.  2.  2.  2.  1.]</p>
</blockquote>




<h2 id="fisher算法实现">Fisher算法实现</h2>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">cal_cov_and_avg</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
</span><span class='line'>    <span class="sd">&quot;&quot;&quot;</span>
</span><span class='line'><span class="sd">    给定一个类别的数据，计算协方差矩阵和平均向量</span>
</span><span class='line'><span class="sd">    :param samples: </span>
</span><span class='line'><span class="sd">    :return: </span>
</span><span class='line'><span class="sd">    &quot;&quot;&quot;</span>
</span><span class='line'>    <span class="n">u1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class='line'>    <span class="n">cov_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
</span><span class='line'>        <span class="n">t</span> <span class="o">=</span> <span class="n">s</span> <span class="o">-</span> <span class="n">u1</span>
</span><span class='line'>        <span class="n">cov_m</span> <span class="o">+=</span> <span class="n">t</span> <span class="o">*</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">cov_m</span><span class="p">,</span> <span class="n">u1</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">fisher</span><span class="p">(</span><span class="n">c_1</span><span class="p">,</span> <span class="n">c_2</span><span class="p">):</span>
</span><span class='line'>    <span class="sd">&quot;&quot;&quot;</span>
</span><span class='line'><span class="sd">    fisher算法实现(请参考上面推导出来的公式，那个才是精华部分)</span>
</span><span class='line'><span class="sd">    :param c_1: </span>
</span><span class='line'><span class="sd">    :param c_2: </span>
</span><span class='line'><span class="sd">    :return: </span>
</span><span class='line'><span class="sd">    &quot;&quot;&quot;</span>
</span><span class='line'>    <span class="n">cov_1</span><span class="p">,</span> <span class="n">u1</span> <span class="o">=</span> <span class="n">cal_cov_and_avg</span><span class="p">(</span><span class="n">c_1</span><span class="p">)</span>
</span><span class='line'>    <span class="n">cov_2</span><span class="p">,</span> <span class="n">u2</span> <span class="o">=</span> <span class="n">cal_cov_and_avg</span><span class="p">(</span><span class="n">c_2</span><span class="p">)</span>
</span><span class='line'>    <span class="n">s_w</span> <span class="o">=</span> <span class="n">cov_1</span> <span class="o">+</span> <span class="n">cov_2</span>
</span><span class='line'>    <span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">s_w</span><span class="p">)</span>  <span class="c"># 奇异值分解</span>
</span><span class='line'>    <span class="n">s_w_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">))),</span> <span class="n">u</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s_w_inv</span><span class="p">,</span> <span class="n">u1</span> <span class="o">-</span> <span class="n">u2</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>




<h2 id="判别类型">判别类型</h2>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">judge</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">c_1</span><span class="p">,</span> <span class="n">c_2</span><span class="p">):</span>
</span><span class='line'>    <span class="sd">&quot;&quot;&quot;</span>
</span><span class='line'><span class="sd">    true 属于1</span>
</span><span class='line'><span class="sd">    false 属于2</span>
</span><span class='line'><span class="sd">    :param sample:</span>
</span><span class='line'><span class="sd">    :param w:</span>
</span><span class='line'><span class="sd">    :param center_1:</span>
</span><span class='line'><span class="sd">    :param center_2:</span>
</span><span class='line'><span class="sd">    :return:</span>
</span><span class='line'><span class="sd">    &quot;&quot;&quot;</span>
</span><span class='line'>    <span class="n">u1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">c_1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class='line'>    <span class="n">u2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">c_2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class='line'>    <span class="n">center_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">u1</span><span class="p">)</span>
</span><span class='line'>    <span class="n">center_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">u2</span><span class="p">)</span>
</span><span class='line'>    <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sample</span><span class="p">)</span>
</span><span class='line'>    <span class="k">return</span> <span class="nb">abs</span><span class="p">(</span><span class="n">pos</span> <span class="o">-</span> <span class="n">center_1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">abs</span><span class="p">(</span><span class="n">pos</span> <span class="o">-</span> <span class="n">center_2</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="n">w</span> <span class="o">=</span> <span class="n">fisher</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">10</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>  <span class="c"># 调用函数，得到参数w</span>
</span><span class='line'><span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
</span><span class='line'>    <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">judge</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">w</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">10</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span> <span class="k">else</span> <span class="mi">2</span><span class="p">)</span>   <span class="c"># 判断所属的类别</span>
</span><span class='line'><span class="c"># evaluate accuracy</span>
</span><span class='line'><span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">pred</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
</span><span class='line'><span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">):</span>
</span><span class='line'>    <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">judge</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">w</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">10</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span> <span class="k">else</span> <span class="mi">2</span><span class="p">)</span>   <span class="c"># 判断所属的类别</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<blockquote>
  <p>输出结果： <br>
  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2. <br>
    2.  2.] [1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1] <br>
  0.95 <br>
  [1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1]</p>
</blockquote>




<p>在这我们可以看出我们的Fisher算法在测试集中的误差率还算理想，误判率仅有5%。但是，我们可以看出其预测分类并不如其他KNN，SVM，等算法的预测效果。</p>




<p>最后，有关Fisher算法的介绍也就到此结束了！</p>

</div>
  
  


      <footer>
      
      - <a href="/blog/2017/08/30/pythonshi-xian-fisherpan-bie-fen-xi/">Python实现Fisher判别分析</a>
      <time datetime="2017-08-30T09:06:21+08:00" pubdate><span class='month'>Aug</span> <span class='day'>30</span> <span class='year'>2017</span></time>
      
      <span class="categories">posted in <a class='category' href='/blog/categories/ji-qi-xue-xi/'>机器学习</a></span>
      
      </footer>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/08/23/k-jin-lin-suan-fa-part2/">K-近邻算法-Part2</a></h1>
    
    
      <div class="post-meta">
        <p class="meta">
          <span class="timestamp">- 








  



<time datetime="2017-08-23T12:19:17+08:00" pubdate data-updated="true"></time> -</span>
          
        </p>
      </div>
    
  </header>


  <div class="entry-content"><h1>K-近邻算法-Part2</h1>

<h2>使用交叉验证来调整k值</h2>

<p>通常来说,一个最优的KNN模型其k参数所对应的预估错误率应该是最低的。因此，在选定模型k值的时候应该反复尝试不同的k值在预估上的效果，对比其错误率。初学者在这里为了降低模型的误差通常会将全部样本数据也作为训练集一起代入模型进行训练。虽然这种做法在训练时确实能够有效降低误差，对现有数据进行更好的拟合。但是，同时带来的后果是：我们会将数据的各种无法避免的真实误差，如测量误差，抽样误差等也训练进了我们的模型之中，使得训练出来的模型在新数据或未知数据上的预估效果特别差，这种现象也被称为<strong>过拟合（overfitting）</strong>。</p>

<p>为了降低预估的错误率以及避免过拟合现象的发生，我们可以在某种意义下将<strong>原始数据(dataset)</strong>进行分组,一部分做为<strong>训练集(train set)</strong>,另一部分做为<strong>验证集(validation set or test set)</strong>，首先用训练集对分类器进行训练,再利用验证集来测试训练得到的<strong>模型(model)</strong>,以此来做为评价分类器的性能指标。这种方法也就是所谓的<strong>交叉验证（Cross Validation）</strong>。</p>

<p>而在交叉验证中，<strong>K折交叉验证（k-fold cross validation）</strong>是比较常用的。其主要思想是：初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。</p>

<p><img src="https://kevinzakka.github.io/assets/k_fold_cv.jpg" alt="enter image description here" /></p>

<p>为了更好的理解k折交叉验证，我们继续沿用part1部分的训练数据，使用10折交叉验证的方法来调整我们的k值。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
</span><span class='line'><span class="c"># creating odd list of K for KNN</span>
</span><span class='line'><span class="n">myList</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">50</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="c"># subsetting just the odd ones</span>
</span><span class='line'><span class="n">neighbors</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">myList</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="c"># empty list that will hold cv scores</span>
</span><span class='line'><span class="n">cv_scores</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'>
</span><span class='line'><span class="c"># perform 10-fold cross validation</span>
</span><span class='line'><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">neighbors</span><span class="p">:</span>
</span><span class='line'>    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
</span><span class='line'>    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">&#39;accuracy&#39;</span><span class="p">)</span>
</span><span class='line'>    <span class="n">cv_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</span><span class='line'>
</span><span class='line'><span class="c"># changing to misclassification error</span>
</span><span class='line'><span class="n">MSE</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">cv_scores</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'><span class="c"># determining best k</span>
</span><span class='line'><span class="n">optimal_k</span> <span class="o">=</span> <span class="n">neighbors</span><span class="p">[</span><span class="n">MSE</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">MSE</span><span class="p">))]</span>
</span><span class='line'><span class="k">print</span> <span class="p">(</span><span class="s">&quot;The optimal number of neighbors is </span><span class="si">%d</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">optimal_k</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># plot misclassification error vs k</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">MSE</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;Number of Neighbors K&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;Misclassification Error&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>


<p>在上面的程序中，我们在1-50的奇数中选取k值，并根据k值训练模型进行预估验证。最后根据不同k值训练出去的模型的<strong>均方误差（Mean Squared Error, MSE）</strong>作出折线图，并求出使得MSE最小的最优k值。</p>

<blockquote><p>输出结果:</p>

<p>The optimal number of neighbors is 7
<img src="https://kevinzakka.github.io/assets/cv_knn.png" alt="enter image description here" /></p></blockquote>

<p>由此我们可以得出：在这个模型中，10折交叉验证告诉我们最优的k值是7。</p>

<h2>尝试自己实现KNN算法</h2>

<p>到目前为止，我们都是调用sklearn库中的KNN来完成分类任务。那么，下面我们来尝试自己实现一个简单的KNN算法并用它来分类我们之前的数据。</p>

<p>经过上一篇文章的介绍，我们可以知道KNN算法的关键是计算出新的<strong>待分类数据</strong>与现有的样本数据的<strong>“距离”</strong>，而其中较为常用的还是<strong>欧式距离（euclidean distance ）</strong>。然后提取出k个最相邻的点，并根据他们大多数点的分类属性来给待分类点进行归类。</p>

<p>因此核心计算代码我们可以这样写：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
</span><span class='line'>  <span class="c"># create list for distances and targets</span>
</span><span class='line'>  <span class="n">distances</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'>  <span class="n">targets</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)):</span>
</span><span class='line'>      <span class="c"># first we compute the euclidean distance</span>
</span><span class='line'>      <span class="n">distance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x_test</span> <span class="o">-</span> <span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])))</span>
</span><span class='line'>      <span class="c"># add it to list of distances</span>
</span><span class='line'>      <span class="n">distances</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">distance</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>
</span><span class='line'>
</span><span class='line'>  <span class="c"># sort the list</span>
</span><span class='line'>  <span class="n">distances</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="c"># make a list of the k neighbors&#39; targets</span>
</span><span class='line'>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
</span><span class='line'>      <span class="n">index</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
</span><span class='line'>      <span class="n">targets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
</span><span class='line'>
</span><span class='line'>  <span class="c"># return most common target</span>
</span><span class='line'>  <span class="k">return</span> <span class="n">Counter</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>在上面的代码中，我们首先创建一个保存距离的数组，并在存储完计算出的待测点与各样本点的距离后对数组进行升序排列，然后取出前k个最接近待测点的样本点，返回其出现最多的分类标签。</p>

<p>然后接下来让我继续完成整个KNN算法。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">kNearestNeighbor</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
</span><span class='line'>
</span><span class='line'>  <span class="c"># loop over all observations</span>
</span><span class='line'>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)):</span>
</span><span class='line'>      <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">k</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>使用我们上面得出最优的 k = 7作为参数生成模型并进行预估。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># making our predictions </span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</span><span class='line'><span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'>
</span><span class='line'><span class="n">kNearestNeighbor</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># transform the list into an array</span>
</span><span class='line'><span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</span><span class='line'><span class="c">#print(y_test,predictions)</span>
</span><span class='line'><span class="c"># evaluating accuracy</span>
</span><span class='line'><span class="c">#for i in range(predictions.size):</span>
</span><span class='line'><span class="c">#    print(predictions.tolist()[i],list(y_test)[i])</span>
</span><span class='line'><span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\n</span><span class="s">The accuracy of our classifier is </span><span class="si">%d%%</span><span class="s">&#39;</span> <span class="o">%</span> <span class="nb">int</span><span class="p">(</span><span class="n">accuracy</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<blockquote><p>输出结果：
The accuracy of our classifier is 98%</p></blockquote>

<p>到此，我们基本已经完成了一个类似于sklearn库中的KNN算法了，并且还有不错的准确率。</p>

<h2>小结</h2>

<p>最后，KNN算法介绍性文章到这就结束了。我们来总结一下KNN算法的优点与不足。</p>

<h4>优点</h4>

<ul>
<li>易于理解</li>
<li>无需训练</li>
<li>容易迁移至多分类情况</li>
</ul>


<h4>不足</h4>

<ul>
<li>计算量大，时间复杂度随数据规模增大而增大</li>
<li>分类情况容易受高频分类影响</li>
</ul>

</div>
  
  


      <footer>
      
      - <a href="/blog/2017/08/23/k-jin-lin-suan-fa-part2/">K-近邻算法-Part2</a>
      <time datetime="2017-08-23T12:19:17+08:00" pubdate><span class='month'>Aug</span> <span class='day'>23</span> <span class='year'>2017</span></time>
      
      <span class="categories">posted in <a class='category' href='/blog/categories/ji-qi-xue-xi/'>机器学习</a></span>
      
      </footer>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/08/14/k-jin-lin-suan-fa-part1/">K-近邻算法-Part1</a></h1>
    
    
      <div class="post-meta">
        <p class="meta">
          <span class="timestamp">- 








  



<time datetime="2017-08-14T23:48:56+08:00" pubdate data-updated="true"></time> -</span>
          
        </p>
      </div>
    
  </header>


  <div class="entry-content"><h1 id="k-近邻算法-part1">K-近邻算法-Part1</h1>




<h2 id="概述">概述</h2>




<p>K-近邻算法，即K-近邻分类算法，简称KNN其通过采用测量不同的特征值之间的距离方法进行分类。</p>




<h3 id="有关k-近邻算法的问题">有关K-近邻算法的问题</h3>




<p>优点：精度高，对异常值不敏感，无数据输入假定</p>




<p>缺点：计算复杂度较高，空间复杂度较高</p>




<p>适用数据范围：数值型和标称型</p>




<h3 id="工作原理">工作原理</h3>




<p>存在一个样本数据集合，即训练样本集，并且训练样本中的每一个数据都存在标签，我们可以清楚地知道每一个数据条目其与对应分类的所属关系。</p>




<p>然后在接受没有标记的新数据输入时，将新数据的特征提取出来将其一一与训练样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似的（即所谓的最近邻）的分类标签来作为新数据的标签。</p>




<p>其中为了避免偶尔性和离群值造成的误差因此有了以样本数据集中前k个最相似的数据作为判别的参考的标准这种做法。<strong>通常k是不大于20的整数而且一般选取奇数作为k值</strong>（奇数可以在投票分类时避免出现等票的情况），最终的分类结果由k个样本的分类标签投票形成，出现最多的分类标签作为新数据的分类。</p>




<h3 id="一般算法流程">一般算法流程</h3>




<ol>
<li>收集数据</li>
<li>准备数据：对数据进行清洗和结构化处理，使得数据可以进行距离计算</li>
<li>分析数据：提取相关特征</li>
<li>训练分类：knn算法并不需要训练</li>
<li>测试算法：计算错误率</li>
<li>使用算法：将新数据输入进行对应结构化之后，运行算法进行判定分类情况，并对后续的分类结果进行进一步处理应用</li>
</ol>




<h2 id="用knn-制作简单的分类器">用KNN 制作简单的分类器</h2>




<p>使用数据：<a href="https://archive.ics.uci.edu/ml/datasets/Iris">UCI的鸢尾花数据集</a> <br>
点击进入目标链接后: </p>




<blockquote>
  <p>按照 Download Data Folder &gt; iris.data 路径来下载指定数据集</p>
</blockquote>




<h4 id="准备数据">准备数据</h4>




<p>在这里我们使用Python 的 Pandas 包以没有标题栏的csv文件的形式读入数据</p>




<p><strong>关键函数：read_csv</strong></p>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># loading libraries</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'>
</span><span class='line'><span class="c"># define column names</span>
</span><span class='line'><span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;sepal_length&#39;</span><span class="p">,</span> <span class="s">&#39;sepal_width&#39;</span><span class="p">,</span> <span class="s">&#39;petal_length&#39;</span><span class="p">,</span> <span class="s">&#39;petal_width&#39;</span><span class="p">,</span> <span class="s">&#39;class&#39;</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'><span class="c"># loading training data</span>
</span><span class='line'><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;iris.data.txt&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
</span><span class='line'><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>




<p><strong>输出</strong>：</p>




<table>
<thead>
<tr>
  <th>index</th>
  <th>sepal_length</th>
  <th>sepal_width</th>
  <th>petal_length</th>
  <th>petal_width</th>
  <th>class</th>
</tr>
</thead>
<tbody><tr>
  <td>0</td>
  <td>5.1</td>
  <td>3.5</td>
  <td>1.4</td>
  <td>0.2</td>
  <td>Iris-setosa</td>
</tr>
<tr>
  <td>1</td>
  <td>4.9</td>
  <td>3.0</td>
  <td>1.4</td>
  <td>0.2</td>
  <td>Iris-setosa</td>
</tr>
<tr>
  <td>2</td>
  <td>4.7</td>
  <td>3.2</td>
  <td>1.3</td>
  <td>0.2</td>
  <td>Iris-setosa</td>
</tr>
<tr>
  <td>3</td>
  <td>4.6</td>
  <td>3.1</td>
  <td>1.5</td>
  <td>0.2</td>
  <td>Iris-setosa</td>
</tr>
<tr>
  <td>4</td>
  <td>5.0</td>
  <td>3.6</td>
  <td>1.4</td>
  <td>0.2</td>
  <td>Iris-setosa</td>
</tr>
</tbody></table>




<h4 id="构建算法">构建算法</h4>




<p><strong>距离函数</strong></p>




<p>我a们在上面的例子中把一个很重要的概念隐藏了起来，在选择一个数量k还只是小问题，更重要的是距离的计算方法。毕竟，当我们说“最近的k个点”时，这个“近”是怎么衡量的？</p>




<p>在数学中，一个空间上距离的严格定义如下： <br>
设 M 为一个空间，M上的一个距离函数是一个函数<script type="math/tex" id="MathJax-Element-383">d:M\times M \rightarrow R</script>，满足：</p>




<ul>
<li><script type="math/tex" id="MathJax-Element-384">d(x,y)≥0  ∀x,y∈M</script></li>
<li><script type="math/tex" id="MathJax-Element-385">d(x,y)=0⟺x=y</script></li>
<li><script type="math/tex" id="MathJax-Element-386">d(x,y)=d(y,x) ∀x,y∈M</script></li>
<li><script type="math/tex" id="MathJax-Element-387">d(x,z)≤d(x,y)+d(y,z) ∀x,y,z∈M</script></li>
</ul>




<p>两个点 x,y 之间的距离就是<script type="math/tex" id="MathJax-Element-388">d(x,y)</script>。</p>




<p>我们一般最常用的距离函数是欧氏距离，也称作<script type="math/tex" id="MathJax-Element-389">L_2</script>距离。</p>




<p>如果 <br>
<script type="math/tex" id="MathJax-Element-390">x=(x1,x2,…,xn)</script> 和 <script type="math/tex" id="MathJax-Element-391">y=(y1,y2,…,yn)</script>是 n 维欧式空间 Rn 上的两个点，那它们之间的<script type="math/tex" id="MathJax-Element-392">L_2</script>距离是</p>




<p><script type="math/tex" id="MathJax-Element-393">d_2(X,Y)=\sqrt{\sum _{i=1}^n \left(X_i-Y_i\right){}^2}</script></p>




<p>由于Python 的scikit-learn包已经实现了KNN算法，因此我们在这里可以直接调用。</p>




<p>在<a href="http://scikit-learn.org/stable/index.html">scikit-learn</a>中，需要以matrix的形式和目标向量的形式来导入和训练数据。</p>




<p>因此在使用scikit-learn之前需要做额外的数据结构处理，同时还应该把原数据划分成训练数据和测试数据这样更加有利于我们下面的算法正确率评估。</p>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># loading libraries</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span><span class='line'>
</span><span class='line'><span class="c"># create design matrix X and target vector y</span>
</span><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">])</span>   <span class="c"># end index is exclusive</span>
</span><span class='line'><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;class&#39;</span><span class="p">])</span>   <span class="c"># another way of indexing a pandas df</span>
</span><span class='line'>
</span><span class='line'><span class="c"># split into train and test</span>
</span><span class='line'><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>




<p>最后，我们根据划分好的数据集来构建真正的分类器，并用构建出来的分类器来进行数据拟合以及评估他的正确率。</p>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># loading library</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
</span><span class='line'><span class="c"># instantiate learning model (k = 3)</span>
</span><span class='line'><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># fitting the model</span>
</span><span class='line'><span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># predict the response</span>
</span><span class='line'><span class="n">pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># evaluate accuracy</span>
</span><span class='line'><span class="c"># print(y_test,pred)</span>
</span><span class='line'><span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>




<p><strong>输出</strong></p>




<blockquote>
  <p>0.98</p>
</blockquote>




<p>由此可见，在适用的场合之下，KNN分类器的准确率也是可以达到一个较为理想的水平的。</p>

</div>
  
  


      <footer>
      
      - <a href="/blog/2017/08/14/k-jin-lin-suan-fa-part1/">K-近邻算法-Part1</a>
      <time datetime="2017-08-14T23:48:56+08:00" pubdate><span class='month'>Aug</span> <span class='day'>14</span> <span class='year'>2017</span></time>
      
      <span class="categories">posted in <a class='category' href='/blog/categories/ji-qi-xue-xi/'>机器学习</a></span>
      
      </footer>
    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/08/06/ji-qi-xue-xi-gai-shu-ke-pu-xiang/">机器学习概述-科普向</a></h1>
    
    
      <div class="post-meta">
        <p class="meta">
          <span class="timestamp">- 








  



<time datetime="2017-08-06T23:02:41+08:00" pubdate data-updated="true"></time> -</span>
          
        </p>
      </div>
    
  </header>


  <div class="entry-content"><h1>机器学习概论-科普篇</h1>

<h2>什么是机器学习？</h2>

<p>机器学习是一门多领域交叉学科。专门研究计算机或其他软硬件设备怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有知识结构使不断改善自身的性能。</p>

<h2>机器学习的应用领域</h2>

<p>机器学习是人工智能研究的核心内容。它的应用已遍及人工智能的各个分支。如：专家系统，自动推理，自然语言处理，模式识别，计算机视觉，智能机器人等领域。</p>

<h2>机器学习与数据挖掘的区别</h2>

<p>机器学习在数据挖掘中被大量使用，其技术内涵几乎通用，可以看作同一座山峰在不同角度下的侧影。</p>

<h2>机器学习与统计学的关系</h2>

<p>机器学习和统计学是非常接近的两个领域。根据 Michael I. Jordan在机器学习领域的理念，从方法论原则到理论工具，在统计学领域是有一段很长的史前史。他也建议数据科学这一术语作为全部领域的前置。 Leo Breiman区别两个统计学的模型：数据模型和算法模型，在算法模型中意味着或多或少包含着机器学习的算法，比如随机森林（Random forest）。 一些统计学家已经采纳了机器学习中的一些做法，引申出了一个联结领域&mdash;&ndash;统计学习。</p>

<h2>机器学习方法</h2>

<p><strong>决策树学习：</strong>决策树学习使用了一个决策树作为预测性模型，映射一个对象的观察结果给其目标价值一个推论。</p>

<p><strong>关联规则学习:</strong>是一种用来在大型数据库中发现变量之间的有趣联系的方法,例如频繁模式挖掘。</p>

<p><strong>人工神经网络：</strong>一个人工神经网络学习（ANN）算法，通常被称为神经网络（NN），是一个由生物的神经网络所激发出的一个算法。计算结构是由联结的人工神经元组所构成，通过联结式的方法来传递信息和计算。现代神经网络是非线性的统计学数据模型工具。它们通常被用来在输入和输出之间模拟复杂关系，找到数据中的关系，或者在观测变量中从不知道的节点捕获统计学结构。</p>

<p><strong>深度学习：</strong>个人不能承受硬件的价格和GPU的发展推动了这些年深度学习的进步，深度学习是由人工神经网络中的多个隐藏层组成的。这条道路试图去模拟人脑的过程，光、声进入视觉和听觉。一些成功的应用有计算机视觉和演讲识别。</p>

<p><strong>归纳逻辑编程：</strong>归纳逻辑编程（ILP）是一门用逻辑编程控制规则的学科，它使用统一的表示法来处理输入样例，背景知识和假说。给定已知的背景知识的编码和一组被表示为事实的逻辑数据库的示例，ILP系统将派生出一个假设的逻辑程序，该程序包含所有积极的和没有负面的示例。归纳编程是一个相关的领域，它考虑任何一种表示假设(而不仅仅是逻辑编程)的编程语言，例如函数式编程。</p>

<p><strong>支持向量机：</strong>支持向量机是一系列关于监督学习在分类和回归上的应用。给出训练样本的数据集，每一个标记属于两类中的一类，一个SVM训练算法构成了一个模型，可以用来预测一个新的样本是否进入一个类别或者是另一个。</p>

<p><strong>集群：</strong>集群分析是将一组观察结果分配到子集(称为集群)，这样，同一集群中的观察与一些预先确定的标准或标准相似，而来自不同集群的观察则不同。不同的聚类技术对数据的结构作出不同的假设，通常由一些相似性度量定义，并通过内部紧度(相同集群的成员之间的相似性)和不同的集群之间的分离来评估。其他方法基于估计的密度和图连通性。摘要聚类是一种非引导性学习的方法，是一种统计数据分析的常用技术。</p>

<p><strong>贝叶斯网络：</strong>一个贝叶斯网络，信任网络或者有向无环图模型是一个概率性图的模型，它通过有向无环图代表了一系列的随机变量和他们的条件独立性。举例，一个贝叶斯网络代表着疾病和症状可能的关系。给出症状，网络可以被用来计算疾病出现的可能性。有效的算法存在于执行推理和学习的过程中。</p>

<p><strong>增强学习：</strong>增强学习关心代理人如何在一个环境中采取行动，从而最大化一些长期受益的概念。增强学习算法尝试去寻找一些策略，映射当前世界的状态给代理在这些状态中应该采取的行动。</p>

<p><strong>相似度量学习：</strong>在这个问题中，学习机被给予了很多对相似或者不相似的例子。它需要去学习一个相似的函数，以用来预测一个新的对象是否相似。它有时被用到推荐系统中。</p>

<p><strong>遗传算法：</strong>遗传算法是一种启发式搜索，它模仿自然选择的过程，并且使用一些突变和变向来生成新的基因型，以找到好的情况解决问题。在机器学习中，遗传算法在20世纪80年代和90年代使用过。反之，机器学习技术被用来提高遗传和进化算法的表现。</p>

<p><strong>基于规则的机器学习：</strong>基于规则的机器学习是任何机器学习方法的通用术语，它可以识别、学习或发展规则来存储、操作或应用知识。基于规则的机器学习者的定义特征是一组关系规则的标识和利用，这些规则集合了系统所捕获的知识。这与其他机器学习者形成鲜明对比，他们通常会识别出一种特殊的模型，这种模型可以普遍应用于任何实例，以便做出预测。基于规则的机器学习方法包括学习分类器系统、关联规则学习和人工免疫系统。</p>

<h2>机器学习应用场景</h2>

<h3>活跃的领域：</h3>

<ul>
<li>数据分析</li>
<li>数据挖掘。</li>
<li>图像和语音识别</li>
<li>智能机器，机器人，人机对话，电脑博弈。</li>
</ul>


<h3>推荐系统：</h3>

<ul>
<li>基于物品的协同过滤</li>
<li>频繁模式挖掘</li>
</ul>


<h3>贝叶斯分类器：</h3>

<ul>
<li>垃圾邮件过滤</li>
<li>网页自动分类：自动化门户系统</li>
<li>评论自动分析</li>
</ul>


<h3>决策树</h3>

<ul>
<li>量化交易</li>
<li>智能博弈</li>
<li>局面标准化</li>
<li>局面评估函数</li>
<li>棋谱学习</li>
</ul>


<h3>神经网络和深度学习</h3>

<ul>
<li>语音识别,图像识别</li>
<li>图形识别：</li>
<li>车牌识别</li>
<li>指纹，虹膜纹识别</li>
<li>脸像识别</li>
<li>动态图像识别</li>
<li>小波分析</li>
</ul>


<h2>机器学习常用软件</h2>

<p>常用软件列表：</p>

<ul>
<li>R（及其扩展包）</li>
<li>Weka（Waikato Environment for Knowledge Analysis）</li>
<li>Matlab</li>
<li>Python,numpy,matplotlib,sklearn,tensorflow</li>
</ul>


<h2>代表性算法</h2>

<h3>回归预测及降维技术：</h3>

<ul>
<li>线性回归</li>
<li>Logistic回归</li>
<li>主成分分析</li>
<li>因子分析</li>
<li>岭回归</li>
<li>LASSO</li>
</ul>


<h3>分类器:</h3>

<ul>
<li>决策树</li>
<li>朴素贝叶斯</li>
<li>贝叶斯信念网络</li>
<li>支持向量机(SVM)</li>
<li>提升分类器准确率的Adaboost和随机森林算法</li>
</ul>


<h3>聚类和孤立点判别</h3>

<ul>
<li>Kmeans聚类</li>
</ul>


<h3>人工神经网路及深度学习</h3>

<ul>
<li>CNN</li>
<li>RNN</li>
</ul>


<p>&hellip;</p>
</div>
  
  


      <footer>
      
      - <a href="/blog/2017/08/06/ji-qi-xue-xi-gai-shu-ke-pu-xiang/">机器学习概述-科普向</a>
      <time datetime="2017-08-06T23:02:41+08:00" pubdate><span class='month'>Aug</span> <span class='day'>06</span> <span class='year'>2017</span></time>
      
      <span class="categories">posted in <a class='category' href='/blog/categories/ji-qi-xue-xi/'>机器学习</a></span>
      
      </footer>
    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/11">&larr; Older</a>
    
    <a href="/archives">Blog Archives</a>
    
    <a class="next" href="/posts/9">Newer &rarr;</a>
    
  </div><!-- /div.pagination -->
</div><!-- /div.blog-index -->

      </div><!-- /div#content -->
    </div><!-- /div#main -->
  </div><!-- /div.container -->
  <footer><div id="footer-widgets-wrapper">
  <div id="footer-first" class="footer-widget">
    <h3>About Me</h3>
    <section class="about-me">
      
        <img class="icon-image" src="https://avatars0.githubusercontent.com/u/13914416?s=240" alt="icon_image">
      
      <div>
        <ul>
          
            <li>GitHub: <a href="https://github.com/EdmondFrank">@EdmondFrank</a></li>
          
          
            <li>Twitter: <a href="https://twitter.com/EdmondFrank4">@EdmondFrank4</a></li>
          
            <li>Blog: <a href="https://edmondfrank.github.io">https://edmondfrank.github.io</a></li>
        </ul>
        <p>
          この町、冗談と気まぐれと偶然でてきっているらしい。
        </p>
      </div>
    </section>
  </div><!-- /div#footer-second -->

  <div id="footer-second" class="footer-widget">
    <h3>Recent Posts</h3>
    <section id="hatena-popular" class="hatena-bookmark">
      <script language="javascript" type="text/javascript" src="https://b.hatena.ne.jp/js/widget.js" charset="utf-8"></script>
      <script language="javascript" type="text/javascript">
        Hatena.BookmarkWidget.url   = "https://edmondfrank.github.io";
        Hatena.BookmarkWidget.title = "Recent Posts";
        Hatena.BookmarkWidget.sort  = "hot";
        Hatena.BookmarkWidget.width = 0;
        Hatena.BookmarkWidget.num   = 10;
        Hatena.BookmarkWidget.theme = "notheme";
        Hatena.BookmarkWidget.load();
      </script>
    </section>
  </div><!-- /div#footer-second -->

  <div id="footer-third" class="footer-widget">
    <h3>Popular Posts</h3>
    <section id="hatena-popular" class="hatena-bookmark">
      <script language="javascript" type="text/javascript" src="https://b.hatena.ne.jp/js/widget.js" charset="utf-8"></script>
      <script language="javascript" type="text/javascript">
        Hatena.BookmarkWidget.url   = "https://edmondfrank.github.io";
        Hatena.BookmarkWidget.title = "Popular Posts";
        Hatena.BookmarkWidget.sort  = "count";
        Hatena.BookmarkWidget.width = 0;
        Hatena.BookmarkWidget.num   = 10;
        Hatena.BookmarkWidget.theme = "notheme";
        Hatena.BookmarkWidget.load();
      </script>
    </section>
  </div><!-- /div#footer-third -->
</div><!-- /div#footer-widgets-wrapper -->

<div id="credit" role="contentinfo">
  <p>
    Copyright &copy; 2022 - <a href="https://github.com/EdmondFrank/">EdmondFrank</a> -
    <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
  </p>
</div>

</footer>
  










  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'https://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
